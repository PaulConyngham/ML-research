{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q Learning with OpenAi gym\n",
    "\n",
    "The purpose of this workbook is to work through a simple tabular q problem (cartpole) using OpenAi's gym\n",
    "\n",
    "https://gym.openai.com/envs/CartPole-v0/\n",
    "\n",
    "decription below:\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-02 15:28:14,277] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# put this up here incase the rest crashes..\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01818737 -0.02927275  0.0101939  -0.01820194]\n",
      "[-0.01877282 -0.2245394   0.00982986  0.27767977]\n",
      "[-0.02326361 -0.02955906  0.01538346 -0.01188669]\n",
      "[-0.02385479  0.16533894  0.01514573 -0.29967653]\n",
      "[-0.02054801 -0.02999559  0.0091522  -0.00225567]\n",
      "[-0.02114792  0.16499392  0.00910708 -0.29203697]\n",
      "[-0.01784805  0.35998484  0.00326634 -0.58183379]\n",
      "[-0.01064835  0.55506088 -0.00837033 -0.87348597]\n",
      "[  4.52868469e-04   7.50295634e-01  -2.58400520e-02  -1.16878870e+00]\n",
      "[ 0.01545878  0.94574402 -0.04921583 -1.46945954]\n",
      "[ 0.03437366  1.14143237 -0.07860502 -1.77710025]\n",
      "[ 0.05720231  1.33734666 -0.11414702 -2.09315036]\n",
      "[ 0.08394924  1.14354468 -0.15601003 -1.83782559]\n",
      "[ 0.10682014  0.9504531  -0.19276654 -1.59738696]\n",
      "Episode finished after 14 timesteps\n",
      "[  2.22093016e-02   9.82554764e-05   1.13610411e-02   3.16652472e-02]\n",
      "[ 0.02221127 -0.19518476  0.01199435  0.32791094]\n",
      "[ 0.01830757 -0.3904754   0.01855256  0.62435208]\n",
      "[ 0.01049806 -0.58585138  0.03103961  0.92281976]\n",
      "[ -1.21896408e-03  -7.81378638e-01   4.94960018e-02   1.22509355e+00]\n",
      "[-0.01684654 -0.97710177  0.07399787  1.53286459]\n",
      "[-0.03638857 -0.78294536  0.10465516  1.26416249]\n",
      "[-0.05204748 -0.97923768  0.12993841  1.58770247]\n",
      "[-0.07163223 -0.78587721  0.16169246  1.33820132]\n",
      "[-0.08734978 -0.59311824  0.18845649  1.10016504]\n",
      "Episode finished after 10 timesteps\n",
      "[ 0.04456873 -0.04234328  0.03418741  0.04212187]\n",
      "[ 0.04372187 -0.23793836  0.03502985  0.34539221]\n",
      "[ 0.0389631  -0.04333177  0.0419377   0.06395807]\n",
      "[ 0.03809647  0.15116459  0.04321686 -0.21520389]\n",
      "[ 0.04111976  0.34564291  0.03891278 -0.49394719]\n",
      "[ 0.04803262  0.54019507  0.02903384 -0.77411705]\n",
      "[ 0.05883652  0.344686    0.01355149 -0.47244228]\n",
      "[ 0.06573024  0.53961396  0.00410265 -0.76082335]\n",
      "[ 0.07652252  0.34443572 -0.01111382 -0.46685229]\n",
      "[ 0.08341123  0.53971292 -0.02045086 -0.7630174 ]\n",
      "[ 0.09420549  0.34487853 -0.03571121 -0.47683906]\n",
      "[ 0.10110306  0.54048603 -0.04524799 -0.7805601 ]\n",
      "[ 0.11191278  0.73619983 -0.0608592  -1.0871287 ]\n",
      "[ 0.12663678  0.541931   -0.08260177 -0.8141465 ]\n",
      "[ 0.1374754   0.73808114 -0.0988847  -1.13162583]\n",
      "[ 0.15223702  0.9343487  -0.12151722 -1.45361375]\n",
      "[ 0.17092399  0.74091047 -0.15058949 -1.20123426]\n",
      "[ 0.1857422   0.54802215 -0.17461418 -0.95928237]\n",
      "[ 0.19670265  0.74500674 -0.19379982 -1.30134222]\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.00519889  0.04960694 -0.040387   -0.01903733]\n",
      "[ 0.00619103  0.24528412 -0.04076775 -0.32418429]\n",
      "[ 0.01109671  0.44096213 -0.04725143 -0.62943983]\n",
      "[ 0.01991595  0.63671051 -0.05984023 -0.93662143]\n",
      "[ 0.03265016  0.83258617 -0.07857266 -1.24749155]\n",
      "[ 0.04930189  1.02862279 -0.10352249 -1.56371585]\n",
      "[ 0.06987434  0.83487978 -0.13479681 -1.30503847]\n",
      "[ 0.08657194  1.03142855 -0.16089758 -1.63669754]\n",
      "[ 0.10720051  0.83851808 -0.19363153 -1.39816936]\n",
      "Episode finished after 9 timesteps\n",
      "[ 0.02975343 -0.02247632 -0.02661519 -0.00985346]\n",
      "[ 0.0293039   0.17301702 -0.02681225 -0.31081355]\n",
      "[ 0.03276424  0.36851051 -0.03302853 -0.61183024]\n",
      "[ 0.04013445  0.56407813 -0.04526513 -0.9147303 ]\n",
      "[ 0.05141601  0.36959666 -0.06355974 -0.63661031]\n",
      "[ 0.05880795  0.56554474 -0.07629194 -0.94861291]\n",
      "[ 0.07011884  0.37152824 -0.0952642  -0.68084278]\n",
      "[ 0.07754941  0.17784947 -0.10888106 -0.41960723]\n",
      "[ 0.0811064   0.37433219 -0.1172732  -0.74453386]\n",
      "[ 0.08859304  0.57086049 -0.13216388 -1.07170084]\n",
      "[ 0.10001025  0.37770974 -0.15359789 -0.82324455]\n",
      "[ 0.10756445  0.57456171 -0.17006279 -1.16002664]\n",
      "[ 0.11905568  0.38201332 -0.19326332 -0.92512837]\n",
      "Episode finished after 13 timesteps\n",
      "[-0.02314821  0.01662431  0.02212701  0.02985633]\n",
      "[-0.02281572 -0.17880785  0.02272413  0.32943763]\n",
      "[-0.02639188 -0.3742458   0.02931289  0.62919918]\n",
      "[-0.03387679 -0.56976429  0.04189687  0.93096768]\n",
      "[-0.04527208 -0.76542587  0.06051622  1.23651657]\n",
      "[-0.0605806  -0.96127096  0.08524656  1.54752737]\n",
      "[-0.07980602 -0.76726963  0.1161971   1.28261423]\n",
      "[-0.09515141 -0.57380336  0.14184939  1.02845684]\n",
      "[-0.10662748 -0.77049892  0.16241852  1.36210084]\n",
      "[-0.12203746 -0.96724031  0.18966054  1.70087036]\n",
      "Episode finished after 10 timesteps\n",
      "[ 0.01763613 -0.03358749  0.0457885  -0.00151762]\n",
      "[ 0.01696438  0.16084889  0.04575815 -0.2794094 ]\n",
      "[ 0.02018136  0.35528924  0.04016996 -0.55731643]\n",
      "[ 0.02728715  0.15962707  0.02902363 -0.25225315]\n",
      "[ 0.03047969  0.35432282  0.02397857 -0.53564202]\n",
      "[ 0.03756614  0.15887204  0.01326573 -0.23550112]\n",
      "[ 0.04074359 -0.03643691  0.0085557   0.06133653]\n",
      "[ 0.04001485  0.15856134  0.00978244 -0.22863478]\n",
      "[ 0.04318607 -0.03669903  0.00520974  0.06711775]\n",
      "[ 0.04245209  0.15834784  0.00655209 -0.22391693]\n",
      "[ 0.04561905  0.35337553  0.00207376 -0.51452589]\n",
      "[ 0.05268656  0.54846822 -0.00821676 -0.80655461]\n",
      "[ 0.06365593  0.74370183 -0.02434785 -1.10181083]\n",
      "[ 0.07852996  0.93913551 -0.04638407 -1.40203204]\n",
      "[ 0.09731267  1.13480202 -0.07442471 -1.70884816]\n",
      "[ 0.12000871  0.94061022 -0.10860167 -1.44022747]\n",
      "[ 0.13882092  1.13688939 -0.13740622 -1.76477787]\n",
      "[ 0.1615587   1.33327145 -0.17270178 -2.09684492]\n",
      "Episode finished after 18 timesteps\n",
      "[-0.02613227  0.04457439 -0.03480377 -0.0256119 ]\n",
      "[-0.02524079 -0.15003161 -0.03531601  0.25589008]\n",
      "[-0.02824142  0.0455763  -0.03019821 -0.04771959]\n",
      "[-0.02732989 -0.14909991 -0.0311526   0.23528466]\n",
      "[-0.03031189 -0.34376324 -0.02644691  0.5179806 ]\n",
      "[-0.03718716 -0.1482791  -0.0160873   0.21708245]\n",
      "[-0.04015274  0.04706909 -0.01174565 -0.08063138]\n",
      "[-0.03921136 -0.14788253 -0.01335828  0.20832274]\n",
      "[-0.04216901  0.04742786 -0.00919182 -0.08854392]\n",
      "[-0.04122045 -0.14756114 -0.0109627   0.20122487]\n",
      "[-0.04417167 -0.3425246  -0.0069382   0.49042952]\n",
      "[-0.05102216 -0.14730547  0.00287039  0.19556805]\n",
      "[-0.05396827 -0.34246836  0.00678175  0.48915507]\n",
      "[-0.06081764 -0.53768533  0.01656485  0.78396761]\n",
      "[-0.07157135 -0.73303095  0.0322442   1.08181568]\n",
      "[-0.08623197 -0.5383491   0.05388052  0.79942286]\n",
      "[-0.09699895 -0.34400604  0.06986897  0.52416453]\n",
      "[-0.10387907 -0.54003814  0.08035226  0.83802001]\n",
      "[-0.11467983 -0.34610003  0.09711267  0.57164962]\n",
      "[-0.12160183 -0.54244001  0.10854566  0.8932782 ]\n",
      "[-0.13245063 -0.73885346  0.12641122  1.21801425]\n",
      "[-0.1472277  -0.93535807  0.15077151  1.5474852 ]\n",
      "[-0.16593486 -1.13193427  0.18172121  1.88316349]\n",
      "Episode finished after 23 timesteps\n",
      "[ 0.04269677  0.0221104   0.04388155 -0.030026  ]\n",
      "[ 0.04313898  0.21657649  0.04328103 -0.3085473 ]\n",
      "[ 0.04747051  0.02086543  0.03711008 -0.00253497]\n",
      "[ 0.04788782  0.21543606  0.03705938 -0.28328212]\n",
      "[ 0.05219654  0.01980566  0.03139374  0.02085496]\n",
      "[ 0.05259265  0.21446365  0.03181084 -0.26175992]\n",
      "[ 0.05688192  0.40911741  0.02657564 -0.54424203]\n",
      "[ 0.06506427  0.60385603  0.0156908  -0.82843449]\n",
      "[  7.71413924e-02   7.98759983e-01  -8.77892242e-04  -1.11614153e+00]\n",
      "[ 0.09311659  0.99389345 -0.02320072 -1.40909971]\n",
      "[ 0.11299446  0.79906684 -0.05138272 -1.12375885]\n",
      "[ 0.1289758   0.99482334 -0.07385789 -1.43210593]\n",
      "[ 0.14887226  1.19077495 -0.10250001 -1.74692651]\n",
      "[ 0.17268776  1.38690174 -0.13743854 -2.0696549 ]\n",
      "[ 0.2004258   1.58312798 -0.17883164 -2.40149852]\n",
      "Episode finished after 15 timesteps\n",
      "[  5.72714664e-05  -4.06169202e-02   4.60443689e-02  -3.58186354e-02]\n",
      "[-0.00075507  0.15381554  0.045328   -0.31362595]\n",
      "[ 0.00232124  0.34826344  0.03905548 -0.59167636]\n",
      "[ 0.00928651  0.15261711  0.02722195 -0.28695126]\n",
      "[ 0.01233885 -0.04288227  0.02148292  0.01419142]\n",
      "[ 0.01148121  0.15192511  0.02176675 -0.27163672]\n",
      "[ 0.01451971 -0.04350057  0.01633402  0.02783128]\n",
      "[ 0.0136497  -0.23885291  0.01689064  0.32562269]\n",
      "[ 0.00887264 -0.43421122  0.0234031   0.62358397]\n",
      "[  1.88417748e-04  -6.29651978e-01   3.58747777e-02   9.23544710e-01]\n",
      "[-0.01240462 -0.43503256  0.05434567  0.64234833]\n",
      "[-0.02110527 -0.24070856  0.06719264  0.36726244]\n",
      "[-0.02591944 -0.43671776  0.07453789  0.68035255]\n",
      "[-0.0346538  -0.63279155  0.08814494  0.99554044]\n",
      "[-0.04730963 -0.43895198  0.10805575  0.73179095]\n",
      "[-0.05608867 -0.63538806  0.12269157  1.05643315]\n",
      "[-0.06879643 -0.44208667  0.14382023  0.80464158]\n",
      "[-0.07763816 -0.24919838  0.15991306  0.56043384]\n",
      "[-0.08262213 -0.05663937  0.17112174  0.32209489]\n",
      "[-0.08375492  0.13568524  0.17756364  0.08788613]\n",
      "[-0.08104121 -0.06147862  0.17932136  0.43091171]\n",
      "[-0.08227079 -0.2586263   0.18793959  0.77432971]\n",
      "[-0.08744331 -0.45576772  0.20342619  1.11976428]\n",
      "Episode finished after 23 timesteps\n",
      "[-0.00632228  0.03510538  0.0313861   0.04928358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00562017 -0.16045224  0.03237177  0.35170153]\n",
      "[-0.00882922  0.03419477  0.0394058   0.06939963]\n",
      "[-0.00814532  0.22873026  0.0407938  -0.21059477]\n",
      "[-0.00357072  0.42324591  0.0365819  -0.49013545]\n",
      "[ 0.0048942   0.22762751  0.02677919 -0.18615148]\n",
      "[ 0.00944675  0.42235628  0.02305616 -0.47026777]\n",
      "[ 0.01789388  0.22691636  0.01365081 -0.1704079 ]\n",
      "[ 0.0224322   0.03160171  0.01024265  0.12654999]\n",
      "[ 0.02306424  0.22657544  0.01277365 -0.16288398]\n",
      "[ 0.02759575  0.03127298  0.00951597  0.13380117]\n",
      "[ 0.02822121  0.22625734  0.01219199 -0.15586448]\n",
      "[ 0.03274635  0.42120262  0.0090747  -0.44467632]\n",
      "[  4.11704054e-02   2.25953463e-01   1.81175898e-04  -1.49146712e-01]\n",
      "[ 0.04568947  0.03082892 -0.00280176  0.14359337]\n",
      "[  4.63060530e-02   2.25990881e-01   7.01090060e-05  -1.49972146e-01]\n",
      "[ 0.05082587  0.03086793 -0.00292933  0.1427329 ]\n",
      "[  5.14432292e-02   2.26031708e-01  -7.46759412e-05  -1.50872740e-01]\n",
      "[ 0.05596386  0.03091083 -0.00309213  0.14178663]\n",
      "[ 0.05658208  0.22607693 -0.0002564  -0.15187021]\n",
      "[ 0.06110362  0.03095865 -0.0032938   0.14073182]\n",
      "[ 0.06172279  0.22612762 -0.00047917 -0.15298842]\n",
      "[ 0.06624534  0.03101253 -0.00353893  0.1395433 ]\n",
      "[ 0.06686559  0.22618499 -0.00074807 -0.154254  ]\n",
      "[ 0.07138929  0.03107376 -0.00383315  0.13819283]\n",
      "[ 0.07201077 -0.16399308 -0.00106929  0.429664  ]\n",
      "[ 0.06873091 -0.35909988  0.00752399  0.72200965]\n",
      "[ 0.06154891 -0.16408281  0.02196418  0.43170436]\n",
      "[ 0.05826725 -0.35950877  0.03059827  0.73122954]\n",
      "[ 0.05107708 -0.16482274  0.04522286  0.44833168]\n",
      "[ 0.04778062 -0.36055423  0.05418949  0.75491975]\n",
      "[ 0.04056954 -0.16621952  0.06928789  0.47976948]\n",
      "[ 0.03724515 -0.36224768  0.07888328  0.79346054]\n",
      "[ 0.03000019 -0.55835874  0.09475249  1.10988   ]\n",
      "[ 0.01883302 -0.75458924  0.11695009  1.43072098]\n",
      "[ 0.00374124 -0.95094454  0.14556451  1.75754701]\n",
      "[-0.01527766 -1.14738518  0.18071545  2.09173669]\n",
      "Episode finished after 37 timesteps\n",
      "[ 0.03864908 -0.03390865 -0.01559417  0.02335832]\n",
      "[ 0.03797091 -0.22880354 -0.015127    0.31108061]\n",
      "[ 0.03339484 -0.42370675 -0.00890539  0.5989548 ]\n",
      "[ 0.0249207  -0.22846133  0.0030737   0.30348014]\n",
      "[ 0.02035147 -0.42362696  0.00914331  0.59713086]\n",
      "[ 0.01187894 -0.61887565  0.02108592  0.89267976]\n",
      "[ -4.98577387e-04  -8.14277169e-01   3.89395197e-02   1.19191568e+00]\n",
      "[-0.01678412 -0.61968073  0.06277783  0.91168761]\n",
      "[-0.02917774 -0.81559334  0.08101159  1.22342221]\n",
      "[-0.0454896  -1.01165993  0.10548003  1.54034891]\n",
      "[-0.0657228  -0.81795292  0.13628701  1.28235695]\n",
      "[-0.08208186 -1.01452184  0.16193415  1.61442069]\n",
      "[-0.1023723  -0.82163877  0.19422256  1.37628208]\n",
      "Episode finished after 13 timesteps\n",
      "[ 0.00343559 -0.04506892 -0.00325471  0.03520025]\n",
      "[ 0.00253422 -0.24014405 -0.00255071  0.32685451]\n",
      "[-0.00226867 -0.04498588  0.00398638  0.0333683 ]\n",
      "[-0.00316838  0.15007868  0.00465375 -0.25805422]\n",
      "[-0.00016681 -0.0451094  -0.00050733  0.03609292]\n",
      "[ -1.06899748e-03  -2.40224070e-01   2.14524429e-04   3.28615739e-01]\n",
      "[-0.00587348 -0.43534907  0.00678684  0.62136631]\n",
      "[-0.01458046 -0.24032255  0.01921417  0.33082858]\n",
      "[-0.01938691 -0.04547931  0.02583074  0.04426627]\n",
      "[-0.0202965  -0.24096195  0.02671606  0.34498582]\n",
      "[-0.02511574 -0.43645356  0.03361578  0.64597217]\n",
      "[-0.03384481 -0.6320274   0.04653522  0.94904838]\n",
      "[-0.04648536 -0.8277439   0.06551619  1.25598174]\n",
      "[-0.06304023 -1.02364057  0.09063582  1.5684443 ]\n",
      "[-0.08351305 -0.82971038  0.12200471  1.30535349]\n",
      "[-0.10010725 -1.02614954  0.14811178  1.63360239]\n",
      "[-0.12063024 -0.83304429  0.18078383  1.39050167]\n",
      "[-0.13729113 -1.02989801  0.20859386  1.73383148]\n",
      "Episode finished after 18 timesteps\n",
      "[ 0.02652215  0.03013877 -0.03948088 -0.02188772]\n",
      "[ 0.02712492 -0.1643954  -0.03991864  0.25808177]\n",
      "[ 0.02383701  0.03127304 -0.034757   -0.04692003]\n",
      "[ 0.02446247  0.2268757  -0.0356954  -0.35036331]\n",
      "[ 0.02899999  0.42248664 -0.04270267 -0.65408503]\n",
      "[ 0.03744972  0.22798448 -0.05578437 -0.37514853]\n",
      "[ 0.04200941  0.03369744 -0.06328734 -0.1005634 ]\n",
      "[ 0.04268336  0.22966662 -0.06529861 -0.41252274]\n",
      "[ 0.04727669  0.4256505  -0.07354906 -0.72505619]\n",
      "[ 0.0557897   0.6217083  -0.08805019 -1.03995176]\n",
      "[ 0.06822387  0.81788272 -0.10884922 -1.35892676]\n",
      "[ 0.08458152  0.62428098 -0.13602776 -1.10218134]\n",
      "[ 0.09706714  0.431185   -0.15807138 -0.8550819 ]\n",
      "[ 0.10569084  0.23852947 -0.17517302 -0.61598114]\n",
      "[ 0.11046143  0.43561007 -0.18749264 -0.95831586]\n",
      "[ 0.11917363  0.63269034 -0.20665896 -1.30355587]\n",
      "Episode finished after 16 timesteps\n",
      "[-0.03596454 -0.00774459 -0.01473961  0.0005427 ]\n",
      "[-0.03611943  0.18758561 -0.01472875 -0.29675406]\n",
      "[-0.03236772 -0.00732331 -0.02066384 -0.00875249]\n",
      "[-0.03251418 -0.20214291 -0.02083889  0.27733979]\n",
      "[-0.03655704 -0.00672996 -0.01529209 -0.02184219]\n",
      "[-0.03669164 -0.20162931 -0.01572893  0.26597695]\n",
      "[-0.04072423 -0.00628644 -0.01040939 -0.03162522]\n",
      "[-0.04084995  0.18898323 -0.0110419  -0.32757416]\n",
      "[-0.03707029  0.38426063 -0.01759338 -0.62371865]\n",
      "[-0.02938508  0.57962374 -0.03006776 -0.92189016]\n",
      "[-0.0177926   0.38492069 -0.04850556 -0.63880619]\n",
      "[-0.01009419  0.19050743 -0.06128168 -0.36178433]\n",
      "[-0.00628404 -0.00369237 -0.06851737 -0.08903697]\n",
      "[-0.00635789  0.19234138 -0.07029811 -0.40252599]\n",
      "[-0.00251106  0.38838632 -0.07834863 -0.71651771]\n",
      "[ 0.00525667  0.5845002  -0.09267898 -1.03279704]\n",
      "[ 0.01694667  0.39072493 -0.11333492 -0.77059019]\n",
      "[ 0.02476117  0.58720899 -0.12874673 -1.09667464]\n",
      "[ 0.03650535  0.78376929 -0.15068022 -1.42682191]\n",
      "[ 0.05218073  0.98039732 -0.17921666 -1.76255456]\n",
      "Episode finished after 20 timesteps\n",
      "[ 0.04639728 -0.03819241  0.01143243  0.0035373 ]\n",
      "[ 0.04563343 -0.23347644  0.01150317  0.29980526]\n",
      "[ 0.0409639  -0.42876045  0.01749928  0.59609376]\n",
      "[ 0.0323887  -0.23388771  0.02942115  0.30897393]\n",
      "[ 0.02771094 -0.03919704  0.03560063  0.02571289]\n",
      "[ 0.026927   -0.23481097  0.03611489  0.32941224]\n",
      "[ 0.02223078 -0.43042793  0.04270314  0.63326181]\n",
      "[ 0.01362222 -0.62611877  0.05536837  0.93908137]\n",
      "[ 0.00109985 -0.43178519  0.07415     0.6642977 ]\n",
      "[-0.00753586 -0.23776878  0.08743595  0.39585291]\n",
      "[-0.01229123 -0.43401546  0.09535301  0.71477184]\n",
      "[-0.02097154 -0.24033375  0.10964845  0.45356033]\n",
      "[-0.02577822 -0.04691925  0.11871965  0.19735296]\n",
      "[-0.0267166   0.14632219  0.12266671 -0.05564724]\n",
      "[-0.02379016  0.33949127  0.12155377 -0.30724997]\n",
      "[-0.01700033  0.5326904   0.11540877 -0.55926151]\n",
      "[-0.00634652  0.33615381  0.10422354 -0.23256281]\n",
      "[ 0.00037655  0.139709    0.09957228  0.0910917 ]\n",
      "[ 0.00317073 -0.05668873  0.10139412  0.413455  ]\n",
      "[ 0.00203696  0.13686081  0.10966322  0.15438061]\n",
      "[ 0.00477417 -0.05964652  0.11275083  0.47954739]\n",
      "[ 0.00358124  0.13371822  0.12234178  0.22442052]\n",
      "[ 0.00625561 -0.06292055  0.12683019  0.55305286]\n",
      "[ 0.0049972   0.13021355  0.13789124  0.30286682]\n",
      "[ 0.00760147  0.32312851  0.14394858  0.05665276]\n",
      "[ 0.01406404  0.1262674   0.14508164  0.39106519]\n",
      "[ 0.01658939 -0.07058353  0.15290294  0.72574397]\n",
      "[ 0.01517772 -0.26745189  0.16741782  1.06237975]\n",
      "[ 0.00982868 -0.07489409  0.18866541  0.82657269]\n",
      "[ 0.0083308  -0.27202565  0.20519687  1.17216217]\n",
      "Episode finished after 30 timesteps\n",
      "[-0.0489772   0.03620146  0.04776192 -0.00161663]\n",
      "[-0.04825317  0.23060706  0.04772959 -0.2788558 ]\n",
      "[-0.04364103  0.42501677  0.04215247 -0.5561111 ]\n",
      "[-0.03514069  0.61952235  0.03103025 -0.83522115]\n",
      "[-0.02275025  0.81420697  0.01432583 -1.11798607]\n",
      "[-0.00646611  0.6189     -0.00803389 -0.82084402]\n",
      "[ 0.00591189  0.81413096 -0.02445078 -1.11604295]\n",
      "[ 0.02219451  0.61933834 -0.04677163 -0.83112917]\n",
      "[ 0.03458128  0.42488578 -0.06339422 -0.55351515]\n",
      "[ 0.043079    0.23070864 -0.07446452 -0.28145995]\n",
      "[ 0.04769317  0.03672349 -0.08009372 -0.01316092]\n",
      "[ 0.04842764  0.2328973  -0.08035694 -0.33000068]\n",
      "[ 0.05308558  0.42906572 -0.08695695 -0.64690331]\n",
      "[ 0.0616669   0.62528473 -0.09989502 -0.96565289]\n",
      "[ 0.07417259  0.43163611 -0.11920808 -0.70594838]\n",
      "[ 0.08280532  0.62819029 -0.13332704 -1.0336515 ]\n",
      "[ 0.09536912  0.43506901 -0.15400007 -0.78562259]\n",
      "[ 0.1040705   0.2423603  -0.16971253 -0.54507775]\n",
      "[ 0.10891771  0.43940945 -0.18061408 -0.88606138]\n",
      "[ 0.1177059   0.63646285 -0.19833531 -1.22964575]\n",
      "Episode finished after 20 timesteps\n",
      "[-0.00046501  0.0063972   0.03512355  0.0274358 ]\n",
      "[-0.00033706  0.20099831  0.03567227 -0.25396162]\n",
      "[ 0.0036829   0.39559323  0.03059304 -0.53518289]\n",
      "[ 0.01159477  0.59027193  0.01988938 -0.81807134]\n",
      "[ 0.02340021  0.39488345  0.00352795 -0.51919949]\n",
      "[ 0.03129787  0.19971201 -0.00685604 -0.22540694]\n",
      "[ 0.03529211  0.39493127 -0.01136417 -0.52024461]\n",
      "[ 0.04319074  0.59021134 -0.02176907 -0.81648679]\n",
      "[ 0.05499497  0.78562446 -0.0380988  -1.11593662]\n",
      "[ 0.07070746  0.98122523 -0.06041754 -1.42032321]\n",
      "[ 0.09033196  1.17704048 -0.088824   -1.73126211]\n",
      "[ 0.11387277  1.37305735 -0.12344924 -2.05020822]\n",
      "[ 0.14133392  1.56920923 -0.16445341 -2.37839888]\n",
      "Episode finished after 13 timesteps\n",
      "[-0.01399979 -0.00390821  0.01881881  0.04485116]\n",
      "[-0.01407796  0.1909389   0.01971583 -0.24183536]\n",
      "[-0.01025918  0.38577376  0.01487913 -0.52823476]\n",
      "[-0.0025437   0.19044566  0.00431443 -0.23090075]\n",
      "[ 0.00126521 -0.00473767 -0.00030358  0.06313997]\n",
      "[ 0.00117046 -0.19985527  0.00095922  0.3557271 ]\n",
      "[-0.00282665 -0.39499084  0.00807376  0.64871234]\n",
      "[-0.01072647 -0.19998229  0.021048    0.3585827 ]\n",
      "[-0.01472611 -0.00516579  0.02821966  0.07261032]\n",
      "[-0.01482943  0.18954047  0.02967186 -0.2110373 ]\n",
      "[-0.01103862 -0.00599289  0.02545112  0.09085588]\n",
      "[-0.01115848  0.18875519  0.02726824 -0.19368981]\n",
      "[-0.00738337  0.38347668  0.02339444 -0.47764759]\n",
      "[  2.86161742e-04   5.78260649e-01   1.38414877e-02  -7.62866256e-01]\n",
      "[ 0.01185137  0.77318924 -0.00141584 -1.05116187]\n",
      "[ 0.02731516  0.96832995 -0.02243907 -1.34428889]\n",
      "[ 0.04668176  1.16372684 -0.04932485 -1.64390698]\n",
      "[ 0.0699563   1.35939024 -0.08220299 -1.95154075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0971441   1.16523259 -0.12123381 -1.68542818]\n",
      "[ 0.12044875  0.97170427 -0.15494237 -1.43282189]\n",
      "[ 0.13988284  0.77879609 -0.18359881 -1.19229707]\n",
      "[ 0.15545876  0.58646431 -0.20744475 -0.96232348]\n",
      "Episode finished after 22 timesteps\n",
      "[-0.02976394 -0.02721362 -0.04033029 -0.030753  ]\n",
      "[-0.03030821  0.16846279 -0.04094535 -0.33588283]\n",
      "[-0.02693895 -0.02605326 -0.04766301 -0.05638787]\n",
      "[-0.02746002  0.16971855 -0.04879076 -0.36371971]\n",
      "[-0.02406565 -0.02467724 -0.05606516 -0.08681182]\n",
      "[-0.02455919 -0.2189526  -0.0578014   0.18766881]\n",
      "[-0.02893824 -0.02305336 -0.05404802 -0.12267346]\n",
      "[-0.02939931 -0.21736099 -0.05650149  0.15248   ]\n",
      "[-0.03374653 -0.02147744 -0.05345189 -0.15747905]\n",
      "[-0.03417608 -0.21579498 -0.05660147  0.1178736 ]\n",
      "[-0.03849198 -0.01990965 -0.054244   -0.1921158 ]\n",
      "[-0.03889017 -0.21421535 -0.05808631  0.08297414]\n",
      "[-0.04317448 -0.40845862 -0.05642683  0.35677988]\n",
      "[-0.05134365 -0.60273483 -0.04929123  0.63114971]\n",
      "[-0.06339835 -0.79713564 -0.03666824  0.90791099]\n",
      "[-0.07934106 -0.601537   -0.01851002  0.60393221]\n",
      "[-0.0913718  -0.40616113 -0.00643138  0.30547697]\n",
      "[-0.09949502 -0.21094812 -0.00032184  0.01077271]\n",
      "[ -1.03713986e-01  -4.06065455e-01  -1.06381430e-04   3.03354073e-01]\n",
      "[-0.1118353  -0.21094199  0.0059607   0.0106376 ]\n",
      "[-0.11605413 -0.40614891  0.00617345  0.30519522]\n",
      "[-0.12417711 -0.21111548  0.01227736  0.01446563]\n",
      "[-0.12839942 -0.40641133  0.01256667  0.31099678]\n",
      "[-0.13652765 -0.60171005  0.0187866   0.60761622]\n",
      "[-0.14856185 -0.79708955  0.03093893  0.9061567 ]\n",
      "[-0.16450364 -0.99261646  0.04906206  1.20840129]\n",
      "[-0.18435597 -0.79816135  0.07323009  0.93148808]\n",
      "[-0.2003192  -0.60409996  0.09185985  0.66268656]\n",
      "[-0.2124012  -0.41036799  0.10511358  0.40028264]\n",
      "[-0.22060856 -0.60681171  0.11311923  0.72416838]\n",
      "[-0.23274479 -0.4134208   0.1276026   0.46912217]\n",
      "[-0.24101321 -0.61009259  0.13698505  0.79914484]\n",
      "[-0.25321506 -0.80680114  0.15296794  1.13159134]\n",
      "[-0.26935108 -0.61397613  0.17559977  0.89052859]\n",
      "[-0.2816306  -0.42161549  0.19341034  0.65778625]\n",
      "[-0.29006291 -0.22963648  0.20656607  0.43169586]\n",
      "Episode finished after 36 timesteps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# first lets copy in the cartpole code from OpenAi's example \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        \n",
    "        # so we need to implement our tabular Q agents activities around here\n",
    "        \n",
    "        #action = 0 #applying a force to the left\n",
    "        action = env.action_space.sample() # sampling the \"action\" array which in this case only contains two elements\n",
    "        \n",
    "        # action contains 0 (apply force to the left) and 1 (apply force from the right)\n",
    "        # by sampling the action array above we are choosing one of these two possible actions at random\n",
    "        \n",
    "        # and here\n",
    "        \n",
    "        \n",
    "        observation, reward, done, info = env.step(action) \n",
    "        \n",
    "        # here we taking the next \"step\" in our environment by taking in our action variable randomly selected above\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to understand what the observation variables are (are they velocity, acceleration, angle from the centre line? etc etc) and also which variable is which."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found out what these were doing a quick google search, the answer is located here:\n",
    "\n",
    "https://github.com/openai/gym/issues/238\n",
    "\n",
    "turns out to be:\n",
    "\n",
    "[position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what I learned from my conversation with Will, is first we need to discretize the outputs (put then in \"buckets\" from our cartpole, I am going to just copy his method to discretize, line by line (so that I know what is going on) to do this\n",
    "\n",
    "we need to do this because we need to convert \"the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "[  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "[ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n"
     ]
    }
   ],
   "source": [
    "#so observation space high - prints out the highest value observed in the observation space over one training episode\n",
    "# observation space low - prints out the lowest value observed in the oberservation.....\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  6  1 11  8]\n"
     ]
    }
   ],
   "source": [
    "   #    self.bins = []\n",
    "#    self.bins.append(np.linspace(-2.4, 2.4, 5))\n",
    " #       self.bins.append(np.linspace(-0.5, 0.5, 5))\n",
    "        self.bins.append(np.linspace(-41.8, 41.8, 5))\n",
    "        self.bins.append(np.linspace(-math.radians(50), math.radians(50), 5))\n",
    "\n",
    "\n",
    "\n",
    "#next Will created a numpy array with values...\n",
    "\n",
    "#x is just an example of some random input for testing purposes....\n",
    "\n",
    "x = np.array([-2.4, 0, -4.0, 4, 2])\n",
    "\n",
    "# next he created the bins using the max and the minimum values from our printing of the observation space above...\n",
    "# with 12 'bins' for discretization purposes\n",
    "bins = np.linspace(-4.8, 4.8, 12) \n",
    "\n",
    "#the next line of code implements discretization on our test variables x\n",
    "out = np.digitize(x, bins)\n",
    "\n",
    "print out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that the output of the above is the POSITION of the discretized version of X in the new bin array!!! aka the index."
   ]
  },
  {
   "attachments": {
    "cartpole3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok time to implement our Q learner, I am going to do this using these two websites as referances:\n",
    "\n",
    "http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    "\n",
    "https://towardsdatascience.com/introduction-to-q-learning-88d1c4f2b49c\n",
    " \n",
    "and this\n",
    "\n",
    "https://dev.to/n1try/cartpole-with-q-learning---first-experiences-with-openai-gym\n",
    "\n",
    "and Will's workbook:\n",
    "\n",
    "https://github.com/whathelll/DeepRLBootCampLabs/blob/master/pytorch/4-CartPoleQLearning.ipynb\n",
    "\n",
    "I also would like to implement the Q agent uses Python Classes, to refresh my memory on how to write them\n",
    "\n",
    "finally the most important thing in all of the formula to update the Q table. It goes something like this:\n",
    "\n",
    "Q = Q(s,a) + alpha * (R + discount * max(Q(s',a')) - Q)\n",
    "\n",
    "or\n",
    "\n",
    "Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "\n",
    "or graphically:\n",
    "\n",
    "![cartpole3.png](attachment:cartpole3.png)\n",
    "\n",
    "https://res.cloudinary.com/practicaldev/image/fetch/s--k7Z0JGcN--/c_limit,f_auto,fl_progressive,q_auto,w_880/https://ferdinand-muetsch.de/images/cartpole3.png\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q learning algorithm:\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "    1. Initialise Q-matrix by all zeros. Set value for ‘γ’. Fill rewards matrix.\n",
    "    2. For each episode. Select a random starting state (here we will restrict our starting state to state-1).\n",
    "    3. Select one among all possible actions for the current state (S).\n",
    "    4. Travel to the next state (S’) as a result of that action (a).\n",
    "    5. for all possible actions from the state (S’) select the one with the highest Q value.\n",
    "    6. Update Q-table using eqn.1 .\n",
    "    7. Set the next state as the current state.\n",
    "    8. If goal state reached then end.\n",
    "    \n",
    "as implemented by the other dude:\n",
    "\n",
    "The Q-Learning algorithm goes as follows:\n",
    "\n",
    "    1. Set the gamma parameter, and environment rewards in matrix R.\n",
    "\n",
    "    2. Initialize matrix Q to zero.\n",
    "\n",
    "    3. For each episode:\n",
    "\n",
    "        Select a random initial state.\n",
    "\n",
    "        Do While the goal state hasn't been reached.\n",
    "\n",
    "            Select one among all possible actions for the current state.\n",
    "            Using this possible action, consider going to the next state.\n",
    "            Get maximum Q value for this next state based on all possible actions.\n",
    "            Compute: Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "            Set the next state as the current state.\n",
    "\n",
    "        End Do\n",
    "\n",
    "    End For\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "\n",
    "-  my Q table setup is incorrect. Need to understand it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-02 15:28:53,949] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensionality of action space is is :  2\n",
      "dimensionality of observation space is is :  4\n",
      "nth episode 0\n",
      "Old_Q is: 0\n",
      "Max Q is: 0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0\n",
      "Max Q is: 0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.6513215599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.6513215599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.68618940391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.794108867905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.794108867905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.5217031\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.5217031\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.6513215599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.6513215599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.68618940391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.68618940391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.745813417167\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.745813417167\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.745813417167\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.794108867905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.890581010868\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.890581010868\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.5217031\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.5217031\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.6513215599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.911370618803\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.911370618803\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.920233556923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.928210201231\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.5217031\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.1\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.890581010868\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.911370618803\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.68618940391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.68618940391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.745813417167\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.947665236697\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.947665236697\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.972187161056\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.972187161056\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.97496844495\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.97496844495\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.928210201231\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.928210201231\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.947665236697\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.947665236697\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.612579511\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.6513215599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.745813417167\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.794108867905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.794108867905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.972187161056\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.972187161056\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.972187161056\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.97496844495\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.717570463519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.745813417167\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.981751996369\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.794108867905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.6513215599\n",
      "Max Q is: 0.981751996369\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.68618940391\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.985219117059\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.985219117059\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.990302262702\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.68618940391\n",
      "Max Q is: 0.981751996369\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.717570463519\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.981751996369\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.985219117059\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.717570463519\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.745813417167\n",
      "Old_Q is: 0.745813417167\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.77123207545\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.990302262702\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.990302262702\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.993637314559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.993637314559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.890581010868\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.890581010868\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.890581010868\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.993637314559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.993637314559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.995361602313\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.996956747278\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.99726107255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.77123207545\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.997781468766\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.9982029897\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.99838269073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.998689979491\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.998820981542\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.890581010868\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.911370618803\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.996956747278\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.996956747278\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.996956747278\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.920233556923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.997781468766\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.928210201231\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.928210201231\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.9982029897\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.998689979491\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.998820981542\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.77123207545\n",
      "Max Q is: 0.794108867905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.794108867905\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.794108867905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "Old_Q is: 0.794108867905\n",
      "Max Q is: 0.814697981115\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.814697981115\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.833228183003\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.999373421252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999543224093\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.999630011515\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999700309327\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999730278394\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.999757250555\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.99980337295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.814697981115\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.833228183003\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.849905364703\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.833228183003\n",
      "Max Q is: 0.864914828233\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.849905364703\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.911370618803\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.99985665888\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.999895504324\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.999895504324\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.920233556923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.928210201231\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.849905364703\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.864914828233\n",
      "Old_Q is: 0.864914828233\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.878423345409\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.878423345409\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.999905953891\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.999905953891\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999915358502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 0.999915358502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.999938296348\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.999944466713\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.999959516234\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.999959516234\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.878423345409\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.890581010868\n",
      "Old_Q is: 0.890581010868\n",
      "Max Q is: 0.901522909782\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.901522909782\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.1\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.972187161056\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.972187161056\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.911370618803\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.901522909782\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.911370618803\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.999970487335\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 0.999976094741\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.999978485267\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.99998063674\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.99998431576\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.99998431576\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.97496844495\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.97496844495\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.97496844495\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999630011515\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.999630011515\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.999700309327\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999700309327\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999700309327\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999730278394\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.999985884184\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.999988566189\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.99998970957\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.99998970957\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.99998970957\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 0.911370618803\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.911370618803\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.911370618803\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.920233556923\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.920233556923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.928210201231\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 0.935389181108\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.999992498276\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999992498276\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999992498276\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.999993923604\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999994531244\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.920233556923\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.928210201231\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.999994531244\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995078119\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 0.999995078119\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 0.999995078119\n",
      "Max Q is: 0.999995078119\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995570307\n",
      "Old_Q is: 0.999995570307\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996013277\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.999996013277\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.941850262997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999996013277\n",
      "Max Q is: 0.999996013277\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996411949\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.999996411949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.999996411949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.957608841725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.999996411949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.999996411949\n",
      "Max Q is: 0.999996411949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996770754\n",
      "Old_Q is: 0.999996770754\n",
      "Max Q is: 0.999996770754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997093679\n",
      "Old_Q is: 0.999997093679\n",
      "Max Q is: 0.999997093679\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997384311\n",
      "Old_Q is: 0.999997384311\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999764588\n",
      "Old_Q is: 0.99999764588\n",
      "Max Q is: 0.99999764588\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997881292\n",
      "Old_Q is: 0.999997881292\n",
      "Max Q is: 0.999997881292\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998093163\n",
      "Old_Q is: 0.999998093163\n",
      "Max Q is: 0.999998093163\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998283846\n",
      "Old_Q is: 0.999998283846\n",
      "Max Q is: 0.999998283846\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998455462\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.952898713028\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.999998455462\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.999998455462\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.999998455462\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.999998455462\n",
      "Max Q is: 0.999998455462\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998609915\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.999998609915\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.999998609915\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.999998609915\n",
      "Max Q is: 0.999998609915\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998748924\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999998748924\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 0.999998748924\n",
      "Max Q is: 0.999998748924\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998874032\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.999998874032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 0.999998874032\n",
      "Max Q is: 0.999998874032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998986628\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.999998986628\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.999998986628\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.999998986628\n",
      "Max Q is: 0.999998986628\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999087966\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.928210201231\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.935389181108\n",
      "Old_Q is: 0.935389181108\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.941850262997\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.961847957552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.941850262997\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.947665236697\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.947665236697\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.952898713028\n",
      "Old_Q is: 0.952898713028\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.957608841725\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.999999087966\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999179169\n",
      "Old_Q is: 0.999999179169\n",
      "Max Q is: 0.999999179169\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999261252\n",
      "Old_Q is: 0.999993923604\n",
      "Max Q is: 0.999999261252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999994531244\n",
      "Old_Q is: 0.999994531244\n",
      "Max Q is: 0.999999261252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995078119\n",
      "Old_Q is: 0.999999261252\n",
      "Max Q is: 0.999999261252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999335127\n",
      "Old_Q is: 0.999995078119\n",
      "Max Q is: 0.999999335127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995570307\n",
      "Old_Q is: 0.999999335127\n",
      "Max Q is: 0.999999335127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999401614\n",
      "Old_Q is: 0.999999401614\n",
      "Max Q is: 0.999999401614\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999461453\n",
      "Old_Q is: 0.999999461453\n",
      "Max Q is: 0.999999461453\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999515307\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.99985665888\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.99985665888\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.965663161797\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.999895504324\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999905953891\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999905953891\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.999905953891\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999915358502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 0.999915358502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.985219117059\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.985219117059\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.985219117059\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999995570307\n",
      "Max Q is: 0.97496844495\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996013277\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.999996013277\n",
      "Max Q is: 0.977471600455\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996411949\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.999996411949\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996770754\n",
      "Old_Q is: 0.957608841725\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.961847957552\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.961847957552\n",
      "Max Q is: 0.981751996369\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.965663161797\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.981751996369\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.965663161797\n",
      "Max Q is: 0.969096845617\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.969096845617\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.999938296348\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.999999515307\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999563777\n",
      "Old_Q is: 0.999999563777\n",
      "Max Q is: 0.999999563777\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999607399\n",
      "Old_Q is: 0.999996770754\n",
      "Max Q is: 0.999999607399\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997093679\n",
      "Old_Q is: 0.999999607399\n",
      "Max Q is: 0.999999607399\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999646659\n",
      "Old_Q is: 0.999997093679\n",
      "Max Q is: 0.999999646659\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997384311\n",
      "Old_Q is: 0.999997384311\n",
      "Max Q is: 0.999999646659\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999764588\n",
      "Old_Q is: 0.999999646659\n",
      "Max Q is: 0.999999646659\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999681993\n",
      "Old_Q is: 0.999999681993\n",
      "Max Q is: 0.999999681993\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999713794\n",
      "Old_Q is: 0.999999713794\n",
      "Max Q is: 0.999999713794\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999742415\n",
      "Old_Q is: 0.99999764588\n",
      "Max Q is: 0.999999742415\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997881292\n",
      "Old_Q is: 0.999999742415\n",
      "Max Q is: 0.999999742415\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999768173\n",
      "Old_Q is: 0.999999768173\n",
      "Max Q is: 0.999999768173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999791356\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.999999791356\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.999997881292\n",
      "Max Q is: 0.999999791356\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998093163\n",
      "Old_Q is: 0.999999791356\n",
      "Max Q is: 0.999999791356\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999981222\n",
      "Old_Q is: 0.99999981222\n",
      "Max Q is: 0.99999981222\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999830998\n",
      "Old_Q is: 0.999999830998\n",
      "Max Q is: 0.999999830998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999847898\n",
      "Old_Q is: 0.999999847898\n",
      "Max Q is: 0.999999847898\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999863109\n",
      "Old_Q is: 0.999999863109\n",
      "Max Q is: 0.999999863109\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999876798\n",
      "Old_Q is: 0.999998093163\n",
      "Max Q is: 0.999999876798\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998283846\n",
      "Old_Q is: 0.999998283846\n",
      "Max Q is: 0.999999876798\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998455462\n",
      "Old_Q is: 0.999999876798\n",
      "Max Q is: 0.999999876798\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999889118\n",
      "Old_Q is: 0.999998455462\n",
      "Max Q is: 0.999999889118\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998609915\n",
      "Old_Q is: 0.999998609915\n",
      "Max Q is: 0.999999889118\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998748924\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.999959516234\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.999959516234\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.972187161056\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.97496844495\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.999970487335\n",
      "reward is: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.999976094741\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.999978485267\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999978485267\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.999978485267\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.99998063674\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.99998063674\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.99998063674\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.999999889118\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.999999889118\n",
      "Max Q is: 0.999999889118\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999900206\n",
      "Old_Q is: 0.999999900206\n",
      "Max Q is: 0.999999900206\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999910186\n",
      "Old_Q is: 0.999998748924\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998874032\n",
      "Old_Q is: 0.999998874032\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998986628\n",
      "Old_Q is: 0.999999910186\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999919167\n",
      "Old_Q is: 0.999999919167\n",
      "Max Q is: 0.999999919167\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999992725\n",
      "Old_Q is: 0.99999992725\n",
      "Max Q is: 0.99999992725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999934525\n",
      "Old_Q is: 0.999999934525\n",
      "Max Q is: 0.999999934525\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999941073\n",
      "Old_Q is: 0.999998986628\n",
      "Max Q is: 0.999999941073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999087966\n",
      "Old_Q is: 0.999999941073\n",
      "Max Q is: 0.999999941073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999946965\n",
      "Old_Q is: 0.999999087966\n",
      "Max Q is: 0.999999946965\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999179169\n",
      "Old_Q is: 0.999999179169\n",
      "Max Q is: 0.999999946965\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999261252\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.969096845617\n",
      "Max Q is: 0.977471600455\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.972187161056\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.977471600455\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.97972444041\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.981751996369\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.999999946965\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.999999261252\n",
      "Max Q is: 0.999999946965\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999335127\n",
      "Old_Q is: 0.999999946965\n",
      "Max Q is: 0.999999946965\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999952269\n",
      "Old_Q is: 0.999999952269\n",
      "Max Q is: 0.999999952269\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999957042\n",
      "Old_Q is: 0.999999335127\n",
      "Max Q is: 0.999999957042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999401614\n",
      "Old_Q is: 0.999999957042\n",
      "Max Q is: 0.999999957042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999961338\n",
      "Old_Q is: 0.999999401614\n",
      "Max Q is: 0.999999961338\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999461453\n",
      "Old_Q is: 0.999999461453\n",
      "Max Q is: 0.999999961338\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999515307\n",
      "Old_Q is: 0.999999515307\n",
      "Max Q is: 0.999999961338\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999563777\n",
      "Old_Q is: 0.999999563777\n",
      "Max Q is: 0.986697205353\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999607399\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.999999961338\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.999999607399\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999646659\n",
      "Old_Q is: 0.972187161056\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97496844495\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.999999961338\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.999999646659\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999681993\n",
      "Old_Q is: 0.97496844495\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.977471600455\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.990302262702\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.990302262702\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.1\n",
      "Max Q is: 0.19\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.19\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.993637314559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.977471600455\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.97972444041\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.99998431576\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.999999961338\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.999999681993\n",
      "Max Q is: 0.999999961338\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999713794\n",
      "Old_Q is: 0.999999961338\n",
      "Max Q is: 0.999999961338\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999965204\n",
      "Old_Q is: 0.999999965204\n",
      "Max Q is: 0.999999965204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999968684\n",
      "Old_Q is: 0.999999713794\n",
      "Max Q is: 0.999999968684\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999742415\n",
      "Old_Q is: 0.999999968684\n",
      "Max Q is: 0.999999968684\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999971815\n",
      "Old_Q is: 0.999999742415\n",
      "Max Q is: 0.999999971815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999768173\n",
      "Old_Q is: 0.999999971815\n",
      "Max Q is: 0.999999971815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999974634\n",
      "Old_Q is: 0.999999974634\n",
      "Max Q is: 0.999999974634\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999997717\n",
      "Old_Q is: 0.999999768173\n",
      "Max Q is: 0.99999997717\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999791356\n",
      "Old_Q is: 0.999999791356\n",
      "Max Q is: 0.99999997717\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999981222\n",
      "Old_Q is: 0.99999981222\n",
      "Max Q is: 0.99999997717\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999830998\n",
      "Old_Q is: 0.999999830998\n",
      "Max Q is: 0.99999997717\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999847898\n",
      "Old_Q is: 0.99999997717\n",
      "Max Q is: 0.99999997717\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999979453\n",
      "Old_Q is: 0.999999979453\n",
      "Max Q is: 0.999999979453\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999981508\n",
      "Old_Q is: 0.999999847898\n",
      "Max Q is: 0.999999981508\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999863109\n",
      "Old_Q is: 0.999999863109\n",
      "Max Q is: 0.999999981508\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999876798\n",
      "Old_Q is: 0.999999981508\n",
      "Max Q is: 0.999999981508\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999983357\n",
      "Old_Q is: 0.999999983357\n",
      "Max Q is: 0.999999983357\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999985021\n",
      "Old_Q is: 0.999999876798\n",
      "Max Q is: 0.999999985021\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999889118\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999889118\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999900206\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.999985884184\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.999985884184\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.99998970957\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.996956747278\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999900206\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999910186\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.999999985021\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.999999910186\n",
      "Max Q is: 0.993637314559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999919167\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.993637314559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999992498276\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.999993923604\n",
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999994531244\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 0.999994531244\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995078119\n",
      "Old_Q is: 0.999995078119\n",
      "Max Q is: 0.999995078119\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995570307\n",
      "Old_Q is: 0.999995570307\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996013277\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.999996013277\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.999996013277\n",
      "Max Q is: 0.999996013277\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996411949\n",
      "Old_Q is: 0.999996411949\n",
      "Max Q is: 0.997781468766\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996770754\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.997781468766\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.997781468766\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.997781468766\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.999996770754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.999996770754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.999996770754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old_Q is: 0.999996770754\n",
      "Max Q is: 0.999996770754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997093679\n",
      "Old_Q is: 0.999997093679\n",
      "Max Q is: 0.99838269073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997384311\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.97972444041\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.981751996369\n",
      "Old_Q is: 0.981751996369\n",
      "Max Q is: 0.999999985021\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.983576796732\n",
      "Old_Q is: 0.999999985021\n",
      "Max Q is: 0.999999985021\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999986519\n",
      "Old_Q is: 0.999999986519\n",
      "Max Q is: 0.999999986519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999987867\n",
      "Old_Q is: 0.999999987867\n",
      "Max Q is: 0.999999987867\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999989081\n",
      "Old_Q is: 0.999999989081\n",
      "Max Q is: 0.999999989081\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999990173\n",
      "Old_Q is: 0.999999990173\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999991155\n",
      "Old_Q is: 0.999999991155\n",
      "Max Q is: 0.999999991155\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999204\n",
      "Old_Q is: 0.999999919167\n",
      "Max Q is: 0.99999999204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999992725\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.99999999204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.99999999204\n",
      "Max Q is: 0.99999999204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999992836\n",
      "Old_Q is: 0.999999992836\n",
      "Max Q is: 0.999999992836\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999993552\n",
      "Old_Q is: 0.99999992725\n",
      "Max Q is: 0.999999993552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999934525\n",
      "Old_Q is: 0.999999934525\n",
      "Max Q is: 0.999999993552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999941073\n",
      "Old_Q is: 0.999999993552\n",
      "Max Q is: 0.999999993552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999994197\n",
      "Old_Q is: 0.999999941073\n",
      "Max Q is: 0.999999994197\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999946965\n",
      "Old_Q is: 0.999999994197\n",
      "Max Q is: 0.999999994197\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999994777\n",
      "Old_Q is: 0.999999994777\n",
      "Max Q is: 0.999999994777\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9999999953\n",
      "Old_Q is: 0.9999999953\n",
      "Max Q is: 0.9999999953\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999577\n",
      "Old_Q is: 0.99999999577\n",
      "Max Q is: 0.99999999577\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996193\n",
      "Old_Q is: 0.999999996193\n",
      "Max Q is: 0.999999996193\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996573\n",
      "Old_Q is: 0.999999946965\n",
      "Max Q is: 0.999999996573\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999952269\n",
      "Old_Q is: 0.999999952269\n",
      "Max Q is: 0.999999996573\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999957042\n",
      "Old_Q is: 0.999999957042\n",
      "Max Q is: 0.999999996573\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999961338\n",
      "Old_Q is: 0.999999961338\n",
      "Max Q is: 0.999999996573\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999965204\n",
      "Old_Q is: 0.999999996573\n",
      "Max Q is: 0.999999996573\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996916\n",
      "Old_Q is: 0.999999965204\n",
      "Max Q is: 0.999999996916\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999968684\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999968684\n",
      "Max Q is: 0.995361602313\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999971815\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.995361602313\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.983576796732\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.985219117059\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.983576796732\n",
      "Max Q is: 0.986697205353\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.985219117059\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.986697205353\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.985219117059\n",
      "Max Q is: 0.988027484817\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.986697205353\n",
      "Old_Q is: 0.986697205353\n",
      "Max Q is: 0.995361602313\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.988027484817\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.999999996916\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.999999996916\n",
      "Max Q is: 0.999999996916\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997224\n",
      "Old_Q is: 0.999999997224\n",
      "Max Q is: 0.999999997224\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997502\n",
      "Old_Q is: 0.999999997502\n",
      "Max Q is: 0.999999997502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997752\n",
      "Old_Q is: 0.999999997752\n",
      "Max Q is: 0.999999997752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997977\n",
      "Old_Q is: 0.999999971815\n",
      "Max Q is: 0.999999997977\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999974634\n",
      "Old_Q is: 0.999999974634\n",
      "Max Q is: 0.999999997977\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999997717\n",
      "Old_Q is: 0.999999997977\n",
      "Max Q is: 0.999999997977\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998179\n",
      "Old_Q is: 0.99999997717\n",
      "Max Q is: 0.999999998179\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999979453\n",
      "Old_Q is: 0.999999979453\n",
      "Max Q is: 0.999999998179\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999981508\n",
      "Old_Q is: 0.999999998179\n",
      "Max Q is: 0.999999998179\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998361\n",
      "Old_Q is: 0.999999998361\n",
      "Max Q is: 0.999999998361\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998525\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.999999998525\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.999999981508\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999983357\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.999999998525\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.999999998525\n",
      "Max Q is: 0.999999998525\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998672\n",
      "Old_Q is: 0.999999983357\n",
      "Max Q is: 0.999999998672\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999985021\n",
      "Old_Q is: 0.999999985021\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999986519\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.999999998672\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.999999986519\n",
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999987867\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.999999998672\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.999999987867\n",
      "Max Q is: 0.997534965295\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999989081\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.999999998672\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.999999989081\n",
      "Max Q is: 0.997781468766\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999990173\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.999999998672\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.999999998672\n",
      "Max Q is: 0.999999998672\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998805\n",
      "Old_Q is: 0.999999998805\n",
      "Max Q is: 0.999999998805\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998925\n",
      "Old_Q is: 0.999999998925\n",
      "Max Q is: 0.999999998925\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999032\n",
      "Old_Q is: 0.999999990173\n",
      "Max Q is: 0.999999999032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999991155\n",
      "Old_Q is: 0.999999991155\n",
      "Max Q is: 0.999999999032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999204\n",
      "Old_Q is: 0.99999999204\n",
      "Max Q is: 0.999999999032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999992836\n",
      "Old_Q is: 0.999999999032\n",
      "Max Q is: 0.999999999032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999129\n",
      "Old_Q is: 0.999999999129\n",
      "Max Q is: 0.999999999129\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999216\n",
      "Old_Q is: 0.999999999216\n",
      "Max Q is: 0.999999999216\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999294\n",
      "Old_Q is: 0.999999999294\n",
      "Max Q is: 0.999999999294\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999365\n",
      "Old_Q is: 0.999999992836\n",
      "Max Q is: 0.999999999365\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999993552\n",
      "Old_Q is: 0.999999993552\n",
      "Max Q is: 0.999999999365\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999994197\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999994197\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999994777\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.999997384311\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999764588\n",
      "Old_Q is: 0.99999764588\n",
      "Max Q is: 0.99838269073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997881292\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.999997881292\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999997881292\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 0.999997881292\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998093163\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.999998093163\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.999998093163\n",
      "Max Q is: 0.998689979491\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998283846\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.998689979491\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.998689979491\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.998820981542\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.998820981542\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.999998283846\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.999998283846\n",
      "Max Q is: 0.999998283846\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998455462\n",
      "Old_Q is: 0.999998455462\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998609915\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.999998609915\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999998609915\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998748924\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999998748924\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.999998748924\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998874032\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999994777\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9999999953\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.999999999365\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.9999999953\n",
      "Max Q is: 0.9982029897\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999577\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.999999999365\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "Old_Q is: 0.999999999365\n",
      "Max Q is: 0.999999999365\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999429\n",
      "Old_Q is: 0.999999999429\n",
      "Max Q is: 0.999999999429\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999486\n",
      "Old_Q is: 0.99999999577\n",
      "Max Q is: 0.999999999486\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996193\n",
      "Old_Q is: 0.999999999486\n",
      "Max Q is: 0.999999999486\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999537\n",
      "Old_Q is: 0.999999996193\n",
      "Max Q is: 0.999999999537\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996573\n",
      "Old_Q is: 0.999999999537\n",
      "Max Q is: 0.999999999537\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999583\n",
      "Old_Q is: 0.999999999583\n",
      "Max Q is: 0.999999999583\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999625\n",
      "Old_Q is: 0.999999996573\n",
      "Max Q is: 0.999999999625\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996916\n",
      "Old_Q is: 0.999999996916\n",
      "Max Q is: 0.999999999625\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997224\n",
      "Old_Q is: 0.999999997224\n",
      "Max Q is: 0.999999999625\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997502\n",
      "Old_Q is: 0.999999999625\n",
      "Max Q is: 0.999999999625\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999663\n",
      "Old_Q is: 0.999999999663\n",
      "Max Q is: 0.999999999663\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999696\n",
      "Old_Q is: 0.999999997502\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997752\n",
      "Old_Q is: 0.999999997752\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997977\n",
      "Old_Q is: 0.999999997977\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998179\n",
      "Old_Q is: 0.999999998179\n",
      "Max Q is: 0.99838269073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998361\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.99838269073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.99838269073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.99838269073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.271\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.19\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.271\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.3439\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.40951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.271\n",
      "Max Q is: 0.999373421252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.3439\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.999373421252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999998361\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998525\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.999998874032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.999998874032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.999998874032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.989224736336\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.999998874032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.999998874032\n",
      "Max Q is: 0.999998874032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998986628\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 0.999998986628\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 0.999998986628\n",
      "Max Q is: 0.999998986628\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999087966\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.999999087966\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999179169\n",
      "Old_Q is: 0.999999179169\n",
      "Max Q is: 0.999999179169\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999261252\n",
      "Old_Q is: 0.999999261252\n",
      "Max Q is: 0.999543224093\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999335127\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.999543224093\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.999543224093\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.999543224093\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.999543224093\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.999999335127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.999993923604\n",
      "Max Q is: 0.999999335127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999994531244\n",
      "Old_Q is: 0.999999335127\n",
      "Max Q is: 0.999999335127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999401614\n",
      "Old_Q is: 0.999994531244\n",
      "Max Q is: 0.999999401614\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995078119\n",
      "Old_Q is: 0.999999401614\n",
      "Max Q is: 0.999999401614\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999461453\n",
      "Old_Q is: 0.999995078119\n",
      "Max Q is: 0.999999461453\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995570307\n",
      "Old_Q is: 0.999999461453\n",
      "Max Q is: 0.999999461453\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999515307\n",
      "Old_Q is: 0.999999515307\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999563777\n",
      "Old_Q is: 0.999999563777\n",
      "Max Q is: 0.999543224093\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999607399\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999999607399\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.999999607399\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999646659\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999999646659\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999995570307\n",
      "Max Q is: 0.999999646659\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996013277\n",
      "Old_Q is: 0.999999646659\n",
      "Max Q is: 0.999999646659\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999681993\n",
      "Old_Q is: 0.999999681993\n",
      "Max Q is: 0.999630011515\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999713794\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.999630011515\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.999630011515\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999996013277\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996411949\n",
      "Old_Q is: 0.988027484817\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.989224736336\n",
      "Old_Q is: 0.989224736336\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.990302262702\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.991272036432\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.990302262702\n",
      "Max Q is: 0.992144832789\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.991272036432\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.999999998525\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998672\n",
      "Old_Q is: 0.999999998672\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998805\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.999999713794\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.999999713794\n",
      "Max Q is: 0.999999713794\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999742415\n",
      "Old_Q is: 0.999999742415\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999768173\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999999768173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999999768173\n",
      "Max Q is: 0.999730278394\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999791356\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999999791356\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.999999791356\n",
      "Max Q is: 0.999757250555\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999981222\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.99999981222\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 0.99999981222\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999830998\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.999999830998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.998689979491\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.998689979491\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.998820981542\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.999999830998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.999999830998\n",
      "Max Q is: 0.999999830998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999847898\n",
      "Old_Q is: 0.999999847898\n",
      "Max Q is: 0.999999847898\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999863109\n",
      "Old_Q is: 0.999996411949\n",
      "Max Q is: 0.999999863109\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996770754\n",
      "Old_Q is: 0.999999863109\n",
      "Max Q is: 0.999999863109\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999876798\n",
      "Old_Q is: 0.999999876798\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999889118\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.99985665888\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.99985665888\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.99985665888\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.99985665888\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999889118\n",
      "Max Q is: 0.999999889118\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999900206\n",
      "Old_Q is: 0.999999900206\n",
      "Max Q is: 0.999999900206\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999910186\n",
      "Old_Q is: 0.999996770754\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997093679\n",
      "Old_Q is: 0.999997093679\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997384311\n",
      "Old_Q is: 0.999997384311\n",
      "Max Q is: 0.99293034951\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999764588\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.99999764588\n",
      "Max Q is: 0.993637314559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997881292\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.999997881292\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998093163\n",
      "Old_Q is: 0.991272036432\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.992144832789\n",
      "Old_Q is: 0.992144832789\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99293034951\n",
      "Old_Q is: 0.99293034951\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.993637314559\n",
      "Old_Q is: 0.993637314559\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994273583103\n",
      "Old_Q is: 0.999999998805\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998925\n",
      "Old_Q is: 0.999999998925\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999032\n",
      "Old_Q is: 0.999999999696\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999727\n",
      "Old_Q is: 0.999999999032\n",
      "Max Q is: 0.999999999727\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999129\n",
      "Old_Q is: 0.999999999727\n",
      "Max Q is: 0.999999999727\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999754\n",
      "Old_Q is: 0.999999999129\n",
      "Max Q is: 0.999999999754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999216\n",
      "Old_Q is: 0.999999999216\n",
      "Max Q is: 0.999999999754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999294\n",
      "Old_Q is: 0.999999999754\n",
      "Max Q is: 0.999999999754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999779\n",
      "Old_Q is: 0.999999999779\n",
      "Max Q is: 0.999999999779\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999801\n",
      "Old_Q is: 0.999999999294\n",
      "Max Q is: 0.999999999801\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999365\n",
      "Old_Q is: 0.999999999365\n",
      "Max Q is: 0.999999999801\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999429\n",
      "Old_Q is: 0.999999999429\n",
      "Max Q is: 0.999999999801\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999486\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.999999999801\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999999999801\n",
      "Max Q is: 0.999999999801\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999821\n",
      "Old_Q is: 0.999999999486\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999537\n",
      "Old_Q is: 0.999999999537\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999583\n",
      "Old_Q is: 0.999999999583\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999625\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.999998093163\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998283846\n",
      "Old_Q is: 0.999998283846\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998455462\n",
      "Old_Q is: 0.999998455462\n",
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998609915\n",
      "Old_Q is: 0.999998609915\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998748924\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.994273583103\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.994846224793\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.995361602313\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.994273583103\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.994846224793\n",
      "Old_Q is: 0.994846224793\n",
      "Max Q is: 0.995825442082\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995361602313\n",
      "Old_Q is: 0.995361602313\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.995825442082\n",
      "Old_Q is: 0.999999999625\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999663\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.999044995049\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.999883893693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Q is: 0.999999910186\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.999999910186\n",
      "Max Q is: 0.999895504324\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999919167\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.999999919167\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 0.999999919167\n",
      "Max Q is: 0.999905953891\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999992725\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.99999992725\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.99999992725\n",
      "Max Q is: 0.999915358502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999934525\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 0.999999934525\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 0.999998748924\n",
      "Max Q is: 0.999999934525\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998874032\n",
      "Old_Q is: 0.999999934525\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999941073\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.999999941073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.999998874032\n",
      "Max Q is: 0.999999941073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998986628\n",
      "Old_Q is: 0.999999941073\n",
      "Max Q is: 0.999999941073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999946965\n",
      "Old_Q is: 0.999998986628\n",
      "Max Q is: 0.999999946965\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999087966\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.999999946965\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.999999946965\n",
      "Max Q is: 0.999999946965\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999952269\n",
      "Old_Q is: 0.999999952269\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999957042\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999999957042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.999999087966\n",
      "Max Q is: 0.999999957042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999179169\n",
      "Old_Q is: 0.999999957042\n",
      "Max Q is: 0.999999957042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999961338\n",
      "Old_Q is: 0.999999961338\n",
      "Max Q is: 0.999938296348\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999965204\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.999999965204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999999965204\n",
      "Max Q is: 0.999944466713\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999968684\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999944466713\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.999944466713\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999999821\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999839\n",
      "Old_Q is: 0.999999999663\n",
      "Max Q is: 0.999999999839\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999696\n",
      "Old_Q is: 0.999999999696\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999727\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999999999839\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.999999999839\n",
      "Max Q is: 0.999999999839\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999855\n",
      "Old_Q is: 0.999999999855\n",
      "Max Q is: 0.999999999855\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999869\n",
      "Old_Q is: 0.999999999727\n",
      "Max Q is: 0.999999999869\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999754\n",
      "Old_Q is: 0.999999999754\n",
      "Max Q is: 0.999999999869\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999779\n",
      "Old_Q is: 0.999999999869\n",
      "Max Q is: 0.999999999869\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999882\n",
      "Old_Q is: 0.999999999779\n",
      "Max Q is: 0.999999999882\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999801\n",
      "Old_Q is: 0.999999999801\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999821\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999999999882\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.999999999882\n",
      "Max Q is: 0.999999999882\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999894\n",
      "Old_Q is: 0.999999999821\n",
      "Max Q is: 0.999999999894\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999839\n",
      "Old_Q is: 0.999999999839\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999855\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999999999894\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999999999894\n",
      "Max Q is: 0.999999999894\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999905\n",
      "Old_Q is: 0.999999999905\n",
      "Max Q is: 0.999999999905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999914\n",
      "Old_Q is: 0.999999999855\n",
      "Max Q is: 0.999999999914\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999869\n",
      "Old_Q is: 0.999999999869\n",
      "Max Q is: 0.999999999914\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999882\n",
      "Old_Q is: 0.999999999914\n",
      "Max Q is: 0.999999999914\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999923\n",
      "Old_Q is: 0.999999999882\n",
      "Max Q is: 0.999999999923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999894\n",
      "Old_Q is: 0.999999999894\n",
      "Max Q is: 0.999373421252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999905\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.999999999923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.999999999905\n",
      "Max Q is: 0.999999999923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999914\n",
      "Old_Q is: 0.999999999914\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999923\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999999999923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.999999999923\n",
      "Max Q is: 0.999999999923\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999931\n",
      "Old_Q is: 0.999999999923\n",
      "Max Q is: 0.999999999931\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999931\n",
      "Old_Q is: 0.999999999931\n",
      "Max Q is: 0.999999999931\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999937\n",
      "Old_Q is: 0.999999999931\n",
      "Max Q is: 0.999999999937\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999937\n",
      "Old_Q is: 0.999999999937\n",
      "Max Q is: 0.999999999937\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999944\n",
      "Old_Q is: 0.999999999944\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999949\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.999999999949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.999999999937\n",
      "Max Q is: 0.999999999949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999944\n",
      "Old_Q is: 0.999999999949\n",
      "Max Q is: 0.999999999949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999954\n",
      "Old_Q is: 0.999999999944\n",
      "Max Q is: 0.999999999954\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999949\n",
      "Old_Q is: 0.999999999949\n",
      "Max Q is: 0.999999999954\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999954\n",
      "Old_Q is: 0.999999999954\n",
      "Max Q is: 0.999999999954\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999959\n",
      "Old_Q is: 0.999999999954\n",
      "Max Q is: 0.999999999959\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999959\n",
      "Old_Q is: 0.999999999959\n",
      "Max Q is: 0.999999999959\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999963\n",
      "Old_Q is: 0.999999999963\n",
      "Max Q is: 0.999999999963\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999967\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999999999967\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999999999959\n",
      "Max Q is: 0.999999999967\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999963\n",
      "Old_Q is: 0.999999999963\n",
      "Max Q is: 0.999999999967\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999967\n",
      "Old_Q is: 0.999999999967\n",
      "Max Q is: 0.999999999967\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999997\n",
      "Old_Q is: 0.999999999967\n",
      "Max Q is: 0.99999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999997\n",
      "Old_Q is: 0.99999999997\n",
      "Max Q is: 0.99999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999973\n",
      "Old_Q is: 0.99999999997\n",
      "Max Q is: 0.999999999973\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999973\n",
      "Old_Q is: 0.999999999973\n",
      "Max Q is: 0.999999999973\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999976\n",
      "Old_Q is: 0.999999999976\n",
      "Max Q is: 0.999999999976\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999978\n",
      "Old_Q is: 0.999999999978\n",
      "Max Q is: 0.999999999978\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999998\n",
      "Old_Q is: 0.99999999998\n",
      "Max Q is: 0.99999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999982\n",
      "Old_Q is: 0.999999999982\n",
      "Max Q is: 0.999999999982\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999984\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999999973\n",
      "Max Q is: 0.999730278394\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999976\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.999730278394\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999730278394\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.999999999984\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 0.999999999976\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999978\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.995825442082\n",
      "Max Q is: 0.996242897874\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996242897874\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.999999999984\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.999999999978\n",
      "Max Q is: 0.999999999984\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999998\n",
      "Old_Q is: 0.99999999998\n",
      "Max Q is: 0.999999999984\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999982\n",
      "Old_Q is: 0.999999999982\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999984\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 0.3439\n",
      "Max Q is: 0.999999968684\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.40951\n",
      "Old_Q is: 0.999999179169\n",
      "Max Q is: 0.999999968684\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999261252\n",
      "Old_Q is: 0.999999968684\n",
      "Max Q is: 0.999999968684\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999971815\n",
      "Old_Q is: 0.999999971815\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999974634\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.999959516234\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999999984\n",
      "Max Q is: 0.999999999984\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999986\n",
      "Old_Q is: 0.999999999984\n",
      "Max Q is: 0.999999999986\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999986\n",
      "Old_Q is: 0.999999999986\n",
      "Max Q is: 0.999999999986\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999987\n",
      "Old_Q is: 0.999999999987\n",
      "Max Q is: 0.999999999987\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999988\n",
      "Old_Q is: 0.999999999986\n",
      "Max Q is: 0.999999999988\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999987\n",
      "Old_Q is: 0.999999999987\n",
      "Max Q is: 0.999999999988\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999988\n",
      "Old_Q is: 0.999999999988\n",
      "Max Q is: 0.999999999988\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999999\n",
      "Old_Q is: 0.99999999999\n",
      "Max Q is: 0.99999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999991\n",
      "Old_Q is: 0.999999999988\n",
      "Max Q is: 0.999999999991\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999999\n",
      "Old_Q is: 0.999999999991\n",
      "Max Q is: 0.999999999991\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999992\n",
      "Old_Q is: 0.99999999999\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999991\n",
      "Old_Q is: 0.999999999991\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999992\n",
      "Old_Q is: 0.999999999992\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma is: 0\n",
      "new Q is 0.999999999992\n",
      "Old_Q is: 0.999999999992\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999993\n",
      "Old_Q is: 0.999999999992\n",
      "Max Q is: 0.999999999993\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999992\n",
      "Old_Q is: 0.999999999992\n",
      "Max Q is: 0.999999999993\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999993\n",
      "Old_Q is: 0.999999999993\n",
      "Max Q is: 0.999999999993\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999994\n",
      "Old_Q is: 0.999999999993\n",
      "Max Q is: 0.999999999994\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999994\n",
      "Old_Q is: 0.999999999994\n",
      "Max Q is: 0.999999999994\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999994\n",
      "Old_Q is: 0.999999999994\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999995\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.999999999995\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.999999999994\n",
      "Max Q is: 0.999999999995\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999994\n",
      "Old_Q is: 0.999999999994\n",
      "Max Q is: 0.999999999995\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999995\n",
      "Old_Q is: 0.999999999995\n",
      "Max Q is: 0.999999999995\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999995\n",
      "Max Q is: 0.999999999996\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999996\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999996\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.996242897874\n",
      "Max Q is: 0.999999974634\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996618608086\n",
      "Old_Q is: 0.999999261252\n",
      "Max Q is: 0.999999974634\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999335127\n",
      "Old_Q is: 0.999999335127\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999401614\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.996618608086\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.996956747278\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.996618608086\n",
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.996956747278\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.99726107255\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.999999999996\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999996\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999996\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.997781468766\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.996956747278\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99726107255\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 0.40951\n",
      "Max Q is: 0.999999974634\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.468559\n",
      "Old_Q is: 0.999999974634\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999997717\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.99999997717\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.99999997717\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999979453\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 0.999999979453\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 0.999999979453\n",
      "Max Q is: 0.999978485267\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999981508\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.999999981508\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999981508\n",
      "Max Q is: 0.999999981508\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999983357\n",
      "Old_Q is: 0.999999401614\n",
      "Max Q is: 0.999999983357\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999461453\n",
      "Old_Q is: 0.999999461453\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999515307\n",
      "Old_Q is: 0.99726107255\n",
      "Max Q is: 0.999999983357\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997534965295\n",
      "Old_Q is: 0.999999983357\n",
      "Max Q is: 0.999999983357\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999985021\n",
      "Old_Q is: 0.999999515307\n",
      "Max Q is: 0.999999985021\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999563777\n",
      "Old_Q is: 0.999999563777\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999607399\n",
      "Old_Q is: 0.997534965295\n",
      "Max Q is: 0.999999985021\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.997781468766\n",
      "Old_Q is: 0.999999607399\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999646659\n",
      "Old_Q is: 0.997781468766\n",
      "Max Q is: 0.999999985021\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998003321889\n",
      "Old_Q is: 0.999999985021\n",
      "Max Q is: 0.999999985021\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999986519\n",
      "Old_Q is: 0.999999986519\n",
      "Max Q is: 0.999999986519\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999987867\n",
      "Old_Q is: 0.999999646659\n",
      "Max Q is: 0.999999987867\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999681993\n",
      "Old_Q is: 0.999999681993\n",
      "Max Q is: 0.999999987867\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999713794\n",
      "Old_Q is: 0.999999713794\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999742415\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.998003321889\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.9982029897\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "Old_Q is: 0.998003321889\n",
      "Max Q is: 0.99838269073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9982029897\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.99838269073\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.9982029897\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99838269073\n",
      "Old_Q is: 0.99838269073\n",
      "Max Q is: 0.999999987867\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998544421657\n",
      "Old_Q is: 0.999999742415\n",
      "Max Q is: 0.998544421657\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999768173\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.999999987867\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.999999768173\n",
      "Max Q is: 0.998689979491\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999791356\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.999999987867\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.999999987867\n",
      "Max Q is: 0.999999987867\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999989081\n",
      "Old_Q is: 0.999999989081\n",
      "Max Q is: 0.999999989081\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999990173\n",
      "Old_Q is: 0.999999791356\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999981222\n",
      "Old_Q is: 0.99999981222\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999830998\n",
      "Old_Q is: 0.999999830998\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999847898\n",
      "Old_Q is: 0.999999847898\n",
      "Max Q is: 0.998820981542\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999863109\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.999999863109\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999876798\n",
      "Old_Q is: 0.998544421657\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998689979491\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.998938883388\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.999999876798\n",
      "Max Q is: 0.999140495544\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999889118\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.999999889118\n",
      "Max Q is: 0.99922644599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999900206\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.999999900206\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999910186\n",
      "Old_Q is: 0.999999990173\n",
      "Max Q is: 0.999999990173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999991155\n",
      "Old_Q is: 0.999999910186\n",
      "Max Q is: 0.999999991155\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999919167\n",
      "Old_Q is: 0.999999991155\n",
      "Max Q is: 0.999999991155\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999204\n",
      "Old_Q is: 0.999999919167\n",
      "Max Q is: 0.99999999204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999992725\n",
      "Old_Q is: 0.99999992725\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999934525\n",
      "Old_Q is: 0.998689979491\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998820981542\n",
      "Old_Q is: 0.998820981542\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.998938883388\n",
      "Old_Q is: 0.998938883388\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999044995049\n",
      "Old_Q is: 0.999044995049\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999140495544\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999303801391\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999140495544\n",
      "Max Q is: 0.999373421252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99922644599\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.999373421252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.99922644599\n",
      "Max Q is: 0.999436079127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999303801391\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999895504324\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999905953891\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999934525\n",
      "Max Q is: 0.999492471214\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999941073\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.99999999204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "Old_Q is: 0.999999941073\n",
      "Max Q is: 0.999543224093\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999946965\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999915358502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.99999999204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.999999946965\n",
      "Max Q is: 0.99999999204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999952269\n",
      "Old_Q is: 0.99999999204\n",
      "Max Q is: 0.99999999204\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999992836\n",
      "Old_Q is: 0.999999952269\n",
      "Max Q is: 0.999999992836\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999957042\n",
      "Old_Q is: 0.999999957042\n",
      "Max Q is: 0.999588901683\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999961338\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999999992836\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999999961338\n",
      "Max Q is: 0.999630011515\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999965204\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.999999992836\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.999999992836\n",
      "Max Q is: 0.999999992836\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999993552\n",
      "Old_Q is: 0.999999965204\n",
      "Max Q is: 0.999999993552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999968684\n",
      "Old_Q is: 0.999999993552\n",
      "Max Q is: 0.999999993552\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999994197\n",
      "Old_Q is: 0.999999968684\n",
      "Max Q is: 0.999999994197\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999971815\n",
      "Old_Q is: 0.999999971815\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999974634\n",
      "Old_Q is: 0.999303801391\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999373421252\n",
      "Old_Q is: 0.999373421252\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999436079127\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999667010363\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.999436079127\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999492471214\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999994197\n",
      "Max Q is: 0.999999994197\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999994777\n",
      "Old_Q is: 0.999999974634\n",
      "Max Q is: 0.999999994777\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999997717\n",
      "Old_Q is: 0.999999994777\n",
      "Max Q is: 0.999999994777\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9999999953\n",
      "Old_Q is: 0.9999999953\n",
      "Max Q is: 0.9999999953\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999577\n",
      "Old_Q is: 0.99999997717\n",
      "Max Q is: 0.99999999577\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999979453\n",
      "Old_Q is: 0.99999999577\n",
      "Max Q is: 0.99999999577\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996193\n",
      "Old_Q is: 0.999999979453\n",
      "Max Q is: 0.999999996193\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999981508\n",
      "Old_Q is: 0.999999996193\n",
      "Max Q is: 0.999999996193\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996573\n",
      "Old_Q is: 0.999999996573\n",
      "Max Q is: 0.99998063674\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996916\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.999999996916\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 0.999999996916\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997224\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.999999997224\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.999999981508\n",
      "Max Q is: 0.999999997224\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999983357\n",
      "Old_Q is: 0.999999983357\n",
      "Max Q is: 0.999999997224\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999985021\n",
      "Old_Q is: 0.999999985021\n",
      "Max Q is: 0.999999997224\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999986519\n",
      "Old_Q is: 0.999999986519\n",
      "Max Q is: 0.999700309327\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999987867\n",
      "Old_Q is: 0.999492471214\n",
      "Max Q is: 0.999700309327\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999543224093\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999999997224\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999999997224\n",
      "Max Q is: 0.999999997224\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997502\n",
      "Old_Q is: 0.999999987867\n",
      "Max Q is: 0.999999997502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999989081\n",
      "Old_Q is: 0.999999997502\n",
      "Max Q is: 0.999999997502\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997752\n",
      "Old_Q is: 0.999999997752\n",
      "Max Q is: 0.999999997752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997977\n",
      "Old_Q is: 0.999999989081\n",
      "Max Q is: 0.999999997977\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999990173\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999997977\n",
      "Max Q is: 0.999999997977\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998179\n",
      "Old_Q is: 0.999999990173\n",
      "Max Q is: 0.999999998179\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999991155\n",
      "Old_Q is: 0.999999991155\n",
      "Max Q is: 0.999730278394\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999204\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999999998179\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.99999999204\n",
      "Max Q is: 0.999757250555\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999992836\n",
      "Old_Q is: 0.999543224093\n",
      "Max Q is: 0.999757250555\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999588901683\n",
      "Old_Q is: 0.999588901683\n",
      "Max Q is: 0.999757250555\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999630011515\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.999757250555\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999998179\n",
      "Max Q is: 0.999999998179\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998361\n",
      "Old_Q is: 0.999999998361\n",
      "Max Q is: 0.999999998361\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998525\n",
      "Old_Q is: 0.999999992836\n",
      "Max Q is: 0.999999998525\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999993552\n",
      "Old_Q is: 0.999999998525\n",
      "Max Q is: 0.999999998525\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998672\n",
      "Old_Q is: 0.999999993552\n",
      "Max Q is: 0.999999998672\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999994197\n",
      "Old_Q is: 0.999999998672\n",
      "Max Q is: 0.999999998672\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998805\n",
      "Old_Q is: 0.999999998805\n",
      "Max Q is: 0.99998431576\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998925\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.999999998925\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.999999998925\n",
      "Max Q is: 0.999985884184\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999032\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.999999999032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.999999999032\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999129\n",
      "Old_Q is: 0.999630011515\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999667010363\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.999988566189\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999994197\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999994777\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999781525499\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.999938296348\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.999999999129\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.999999994777\n",
      "Max Q is: 0.999999999129\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.9999999953\n",
      "Old_Q is: 0.999999999129\n",
      "Max Q is: 0.999999999129\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999216\n",
      "Old_Q is: 0.999999999216\n",
      "Max Q is: 0.99998970957\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999294\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.999999999294\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.9999999953\n",
      "Max Q is: 0.999999999294\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999577\n",
      "Old_Q is: 0.999999999294\n",
      "Max Q is: 0.999999999294\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999365\n",
      "Old_Q is: 0.999999999365\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999429\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.999999999429\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.999999999429\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999486\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 0.999999999486\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 0.99999999577\n",
      "Max Q is: 0.999999999486\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996193\n",
      "Old_Q is: 0.999999999486\n",
      "Max Q is: 0.999992498276\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999537\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999999999537\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.999999996193\n",
      "Max Q is: 0.999999999537\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996573\n",
      "Old_Q is: 0.999999999537\n",
      "Max Q is: 0.999999999537\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999583\n",
      "Old_Q is: 0.999999996573\n",
      "Max Q is: 0.999999999583\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999996916\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999999999583\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.999999999583\n",
      "Max Q is: 0.999999999583\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999625\n",
      "Old_Q is: 0.999999999625\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999663\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.999999999663\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.999999999663\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999696\n",
      "Old_Q is: 0.999667010363\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999700309327\n",
      "Old_Q is: 0.999700309327\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999730278394\n",
      "Old_Q is: 0.999730278394\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999757250555\n",
      "Old_Q is: 0.999993923604\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999994531244\n",
      "Old_Q is: 0.999994531244\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995078119\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.999999996916\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997224\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.999823035655\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 0.999938296348\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.999938296348\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.468559\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.999938296348\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999999997224\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997502\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.999840732089\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.99985665888\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999944466713\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.999950020042\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.5217031\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.468559\n",
      "Max Q is: 0.999995078119\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.5217031\n",
      "Old_Q is: 0.999995078119\n",
      "Max Q is: 0.999995078119\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995570307\n",
      "Old_Q is: 0.999757250555\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999781525499\n",
      "Old_Q is: 0.999781525499\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99980337295\n",
      "Old_Q is: 0.99980337295\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999823035655\n",
      "Old_Q is: 0.999823035655\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999840732089\n",
      "Old_Q is: 0.999840732089\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99985665888\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 0.999870992992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 0.999883893693\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999895504324\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.999999997502\n",
      "Max Q is: 0.999895504324\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997752\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.999895504324\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.999905953891\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999959516234\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999999696\n",
      "Max Q is: 0.999999999696\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999727\n",
      "Old_Q is: 0.999999997752\n",
      "Max Q is: 0.999999999727\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999997977\n",
      "Old_Q is: 0.999999999727\n",
      "Max Q is: 0.999999999727\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999754\n",
      "Old_Q is: 0.999999999754\n",
      "Max Q is: 0.999999999754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999779\n",
      "Old_Q is: 0.999999999779\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999801\n",
      "Old_Q is: 0.99985665888\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999870992992\n",
      "Old_Q is: 0.999995570307\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996013277\n",
      "Old_Q is: 0.999996013277\n",
      "Max Q is: 0.999999999801\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996411949\n",
      "Old_Q is: 0.999999999801\n",
      "Max Q is: 0.999996411949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999821\n",
      "Old_Q is: 0.999996411949\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996770754\n",
      "Old_Q is: 0.999999997977\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998179\n",
      "Old_Q is: 0.999999998179\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998361\n",
      "Old_Q is: 0.999999998361\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998525\n",
      "Old_Q is: 0.999999998525\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998672\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 0.999999998672\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998805\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.999999998805\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999998925\n",
      "Old_Q is: 0.999999999821\n",
      "Max Q is: 0.999999999821\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999839\n",
      "Old_Q is: 0.999999999839\n",
      "Max Q is: 0.999999999839\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999855\n",
      "Old_Q is: 0.999999998925\n",
      "Max Q is: 0.999999999855\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999032\n",
      "Old_Q is: 0.999999999855\n",
      "Max Q is: 0.999999999855\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999869\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.999923822652\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.999999999869\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.999999999032\n",
      "Max Q is: 0.999931440387\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999129\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999999999869\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.999999999129\n",
      "Max Q is: 0.999938296348\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999216\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.999999999869\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999999999216\n",
      "Max Q is: 0.999999999869\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999294\n",
      "Old_Q is: 0.999999999869\n",
      "Max Q is: 0.999999999869\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999882\n",
      "Old_Q is: 0.999999999882\n",
      "Max Q is: 0.999999999882\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999894\n",
      "Old_Q is: 0.999999999294\n",
      "Max Q is: 0.999999999894\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999365\n",
      "Old_Q is: 0.999999999365\n",
      "Max Q is: 0.999999999894\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999429\n",
      "Old_Q is: 0.999999999894\n",
      "Max Q is: 0.999999999894\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999905\n",
      "Old_Q is: 0.999999999429\n",
      "Max Q is: 0.999999999905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999486\n",
      "Old_Q is: 0.999999999905\n",
      "Max Q is: 0.999999999905\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999914\n",
      "Old_Q is: 0.999999999914\n",
      "Max Q is: 0.999999999914\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999923\n",
      "Old_Q is: 0.999999999923\n",
      "Max Q is: 0.999996770754\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999931\n",
      "Old_Q is: 0.999996770754\n",
      "Max Q is: 0.999999999931\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997093679\n",
      "Old_Q is: 0.999999999486\n",
      "Max Q is: 0.999999999931\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999537\n",
      "Old_Q is: 0.999999999537\n",
      "Max Q is: 0.999999999931\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999583\n",
      "Old_Q is: 0.999999999931\n",
      "Max Q is: 0.999999999931\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999937\n",
      "Old_Q is: 0.999999999937\n",
      "Max Q is: 0.999997093679\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999944\n",
      "Old_Q is: 0.999870992992\n",
      "Max Q is: 0.999997093679\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999883893693\n",
      "Old_Q is: 0.999997093679\n",
      "Max Q is: 0.999997093679\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997384311\n",
      "Old_Q is: 0.999883893693\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999895504324\n",
      "Old_Q is: 0.999997384311\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999764588\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999944466713\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.999944466713\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999999999944\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.999999999944\n",
      "Max Q is: 0.999999999944\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999949\n",
      "Old_Q is: 0.999999999583\n",
      "Max Q is: 0.999999999949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999625\n",
      "Old_Q is: 0.999999999949\n",
      "Max Q is: 0.999999999949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999954\n",
      "Old_Q is: 0.999999999625\n",
      "Max Q is: 0.999999999954\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999663\n",
      "Old_Q is: 0.999999999954\n",
      "Max Q is: 0.999999999954\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999959\n",
      "Old_Q is: 0.999999999663\n",
      "Max Q is: 0.999999999959\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999696\n",
      "Old_Q is: 0.999999999959\n",
      "Max Q is: 0.999999999959\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999963\n",
      "Old_Q is: 0.999999999963\n",
      "Max Q is: 0.99999764588\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999967\n",
      "Old_Q is: 0.99999764588\n",
      "Max Q is: 0.999999999967\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997881292\n",
      "Old_Q is: 0.999999999696\n",
      "Max Q is: 0.999999999967\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999727\n",
      "Old_Q is: 0.999999999727\n",
      "Max Q is: 0.999999999967\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999754\n",
      "Old_Q is: 0.999999999967\n",
      "Max Q is: 0.999999999967\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999997\n",
      "Old_Q is: 0.999999999754\n",
      "Max Q is: 0.99999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999779\n",
      "Old_Q is: 0.99999999997\n",
      "Max Q is: 0.99999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999973\n",
      "Old_Q is: 0.999999999779\n",
      "Max Q is: 0.999999999973\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999801\n",
      "Old_Q is: 0.999999999973\n",
      "Max Q is: 0.999999999973\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999976\n",
      "Old_Q is: 0.999999999976\n",
      "Max Q is: 0.999997881292\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999978\n",
      "Old_Q is: 0.999895504324\n",
      "Max Q is: 0.999997881292\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999905953891\n",
      "Old_Q is: 0.999997881292\n",
      "Max Q is: 0.999997881292\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998093163\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.999970487335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 0.999970487335\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.5217031\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.999998093163\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.999998093163\n",
      "Max Q is: 0.999998093163\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998283846\n",
      "Old_Q is: 0.999905953891\n",
      "Max Q is: 0.999998283846\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999915358502\n",
      "Old_Q is: 0.999915358502\n",
      "Max Q is: 0.999998283846\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999923822652\n",
      "Old_Q is: 0.999998283846\n",
      "Max Q is: 0.999998283846\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998455462\n",
      "Old_Q is: 0.999923822652\n",
      "Max Q is: 0.999998455462\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999931440387\n",
      "Old_Q is: 0.999998455462\n",
      "Max Q is: 0.999998455462\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998609915\n",
      "Old_Q is: 0.999998609915\n",
      "Max Q is: 0.999998609915\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998748924\n",
      "Old_Q is: 0.999931440387\n",
      "Max Q is: 0.999998748924\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999938296348\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.999955018038\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999999999978\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 0.999999999801\n",
      "Max Q is: 0.999959516234\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999821\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.999959516234\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.999999999978\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.999999999978\n",
      "Max Q is: 0.999999999978\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999998\n",
      "Old_Q is: 0.999999999821\n",
      "Max Q is: 0.99999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999839\n",
      "Old_Q is: 0.999999999839\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999855\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 0.999976094741\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.999978485267\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 0.5217031\n",
      "Max Q is: 0.99999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.56953279\n",
      "Old_Q is: 0.999999999855\n",
      "Max Q is: 0.99999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999869\n",
      "Old_Q is: 0.999999999869\n",
      "Max Q is: 0.99999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999882\n",
      "Old_Q is: 0.999999999882\n",
      "Max Q is: 0.999963564611\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999894\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.99999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.99999999998\n",
      "Max Q is: 0.99999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999982\n",
      "Old_Q is: 0.999999999894\n",
      "Max Q is: 0.999999999982\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999905\n",
      "Old_Q is: 0.999999999982\n",
      "Max Q is: 0.999999999982\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999984\n",
      "Old_Q is: 0.999999999984\n",
      "Max Q is: 0.999998748924\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999986\n",
      "Old_Q is: 0.999938296348\n",
      "Max Q is: 0.999998748924\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999944466713\n",
      "Old_Q is: 0.999944466713\n",
      "Max Q is: 0.999998748924\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999950020042\n",
      "Old_Q is: 0.999998748924\n",
      "Max Q is: 0.999998748924\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998874032\n",
      "Old_Q is: 0.999998874032\n",
      "Max Q is: 0.999998874032\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998986628\n",
      "Old_Q is: 0.999998986628\n",
      "Max Q is: 0.999998986628\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999087966\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.99998431576\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.56953279\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.999950020042\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999955018038\n",
      "Old_Q is: 0.999955018038\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999959516234\n",
      "Old_Q is: 0.999999087966\n",
      "Max Q is: 0.999999087966\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999179169\n",
      "Old_Q is: 0.999999179169\n",
      "Max Q is: 0.999999179169\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999261252\n",
      "Old_Q is: 0.999999261252\n",
      "Max Q is: 0.999999261252\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999335127\n",
      "Old_Q is: 0.999999335127\n",
      "Max Q is: 0.999999335127\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999401614\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.99996720815\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.999988566189\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.99998970957\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.99998970957\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.6513215599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.56953279\n",
      "Max Q is: 0.999999999986\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.612579511\n",
      "Old_Q is: 0.999999999905\n",
      "Max Q is: 0.999999999986\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999914\n",
      "Old_Q is: 0.999999999986\n",
      "Max Q is: 0.999999999986\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999987\n",
      "Old_Q is: 0.999999999914\n",
      "Max Q is: 0.999999999987\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999923\n",
      "Old_Q is: 0.999999999987\n",
      "Max Q is: 0.999999999987\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999988\n",
      "Old_Q is: 0.999999999923\n",
      "Max Q is: 0.999999999988\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999931\n",
      "Old_Q is: 0.999999999931\n",
      "Max Q is: 0.999999999988\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999937\n",
      "Old_Q is: 0.999999999988\n",
      "Max Q is: 0.999999999988\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999999\n",
      "Old_Q is: 0.999999999937\n",
      "Max Q is: 0.99999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999944\n",
      "Old_Q is: 0.99999999999\n",
      "Max Q is: 0.99999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999991\n",
      "Old_Q is: 0.999999999991\n",
      "Max Q is: 0.999999999991\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999992\n",
      "Old_Q is: 0.999999999944\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999949\n",
      "Old_Q is: 0.999999999949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999954\n",
      "Old_Q is: 0.999999999992\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999992\n",
      "Old_Q is: 0.999999999954\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999959\n",
      "Old_Q is: 0.999999999959\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999963\n",
      "Old_Q is: 0.999999999963\n",
      "Max Q is: 0.999970487335\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999967\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.999999999967\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999997\n",
      "Old_Q is: 0.99999999997\n",
      "Max Q is: 0.999973438601\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999973\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 0.999999999973\n",
      "Max Q is: 0.999976094741\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999976\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 0.999999999992\n",
      "Max Q is: 0.999999999992\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999993\n",
      "Old_Q is: 0.999999999993\n",
      "Max Q is: 0.999999401614\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999994\n",
      "Old_Q is: 0.999959516234\n",
      "Max Q is: 0.999999401614\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999963564611\n",
      "Old_Q is: 0.999999401614\n",
      "Max Q is: 0.999999401614\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999461453\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.999999999994\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 0.999999999994\n",
      "Max Q is: 0.999999999994\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999994\n",
      "Old_Q is: 0.999999999994\n",
      "Max Q is: 0.999999461453\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999995\n",
      "Old_Q is: 0.999999461453\n",
      "Max Q is: 0.999999999995\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999515307\n",
      "Old_Q is: 0.999999999995\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999996\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999996\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.999999515307\n",
      "Max Q is: 0.999999515307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999563777\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999999976\n",
      "Max Q is: 0.99998063674\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999978\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 0.999999999978\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999998\n",
      "Old_Q is: 0.999963564611\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99996720815\n",
      "Old_Q is: 0.99996720815\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999970487335\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999970487335\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999973438601\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 0.999982573066\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 0.999973438601\n",
      "Max Q is: 0.99998431576\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999976094741\n",
      "Old_Q is: 0.999976094741\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999978485267\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 0.999991664752\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.999992498276\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999992498276\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.999993923604\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999994531244\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.99999999998\n",
      "Max Q is: 0.99998431576\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999982\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.999999999982\n",
      "Max Q is: 0.999985884184\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999984\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999999997\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999999563777\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999563777\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999607399\n",
      "Old_Q is: 0.999999999984\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999986\n",
      "Old_Q is: 0.999999999986\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999987\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999987\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999988\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999607399\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.999999607399\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.999999607399\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999999607399\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999994531244\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995078119\n",
      "Old_Q is: 0.999995078119\n",
      "Max Q is: 0.999995078119\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995570307\n",
      "Old_Q is: 0.999995570307\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996013277\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.999999999988\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999999999\n",
      "Old_Q is: 0.99999999999\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999991\n",
      "Old_Q is: 0.999999999991\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999992\n",
      "Old_Q is: 0.999978485267\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998063674\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999987295765\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 0.999999999992\n",
      "Max Q is: 0.99998970957\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999992\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.999999999992\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999993\n",
      "Old_Q is: 0.99998063674\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999982573066\n",
      "Old_Q is: 0.999982573066\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998431576\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999996013277\n",
      "Max Q is: 0.999996013277\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996411949\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.999996411949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.999996411949\n",
      "Max Q is: 0.999996411949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996770754\n",
      "Old_Q is: 0.999996770754\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997093679\n",
      "Old_Q is: 0.999999999993\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999994\n",
      "Old_Q is: 0.999999999994\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999994\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999994\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999995\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999995\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.99998431576\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999985884184\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.999990738613\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999996\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999996\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999992498276\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999993248449\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999997\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.999999999997\n",
      "Max Q is: 0.999993923604\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999993923604\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999994531244\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999985884184\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999987295765\n",
      "Old_Q is: 0.999994531244\n",
      "Max Q is: 0.999994531244\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995078119\n",
      "Old_Q is: 0.999995078119\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999995570307\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999998\n",
      "Old_Q is: 0.999987295765\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999988566189\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.999995570307\n",
      "Max Q is: 0.999995570307\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996013277\n",
      "Old_Q is: 0.999996013277\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996411949\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999997093679\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999997093679\n",
      "Max Q is: 0.999997093679\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997384311\n",
      "Old_Q is: 0.999993923604\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999994531244\n",
      "Old_Q is: 0.999997384311\n",
      "Max Q is: 0.999997384311\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999764588\n",
      "Old_Q is: 0.99999764588\n",
      "Max Q is: 0.99999764588\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999997881292\n",
      "Old_Q is: 0.999997881292\n",
      "Max Q is: 0.6513215599\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998093163\n",
      "Old_Q is: 0.612579511\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.6513215599\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999607399\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999607399\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999646659\n",
      "Old_Q is: 0.999999999998\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999646659\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999646659\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999681993\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999681993\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 0.999999681993\n",
      "Max Q is: 0.999999681993\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999713794\n",
      "Old_Q is: 0.999988566189\n",
      "Max Q is: 0.999999713794\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99998970957\n",
      "Old_Q is: 0.999999713794\n",
      "Max Q is: 0.999999713794\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999742415\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 0.999999999999\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999999742415\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999742415\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999768173\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999999768173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.99998970957\n",
      "Max Q is: 0.999999768173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999990738613\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.999999768173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.999999768173\n",
      "Max Q is: 0.999999768173\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999791356\n",
      "Old_Q is: 0.999999791356\n",
      "Max Q is: 0.999999791356\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.99999981222\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.99999981222\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.99999981222\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999830998\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999999999\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999999830998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999991664752\n",
      "Max Q is: 0.999999830998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999992498276\n",
      "Old_Q is: 0.999992498276\n",
      "Max Q is: 0.999999830998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993248449\n",
      "Old_Q is: 0.999999830998\n",
      "Max Q is: 0.999999830998\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999847898\n",
      "completed training\n",
      "nth episode 0\n",
      "Old_Q is: 0.999990738613\n",
      "Max Q is: 0.999996411949\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999991664752\n",
      "Old_Q is: 0.999996411949\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999996770754\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999998093163\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999998093163\n",
      "Max Q is: 0.999998093163\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998283846\n",
      "Old_Q is: 0.999998283846\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999998455462\n",
      "Old_Q is: 0.999999999999\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999999847898\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999999847898\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999863109\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 1.0\n",
      "Max Q is: 0.999999863109\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 1.0\n",
      "Old_Q is: 0.999993248449\n",
      "Max Q is: 0.999999863109\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999993923604\n",
      "Old_Q is: 0.999999863109\n",
      "Max Q is: 0.999999863109\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999876798\n",
      "Old_Q is: 0.999999876798\n",
      "Max Q is: 1.0\n",
      "reward is: 1.0\n",
      "alpha is: 0.1\n",
      "gamma is: 0\n",
      "new Q is 0.999999889118\n",
      "completed training\n",
      "((0, 0, 2, 1), [0.9999999999999336, 0.9999999999997099])\n",
      "((0, 0, 2, 0), [0.9999967707539819, 0.9999916647515821])\n",
      "((0, 0, 1, 0), [0.9999999999999996, 0.9999999999999996])\n",
      "((0, 0, 1, 1), [0.9999984554616402, 0.9999945312435129])\n",
      "((0, 0, 2, 2), [0.9999939236039033, 0.9999998891179019])\n",
      "((0, 0, 1, 2), [0.6513215599000001, 0.6513215599000001])\n",
      "running, count is : 1\n",
      "running, count is : 2\n",
      "running, count is : 3\n",
      "running, count is : 4\n",
      "running, count is : 5\n",
      "running, count is : 6\n",
      "running, count is : 7\n",
      "running, count is : 8\n",
      "running, count is : 9\n",
      "running, count is : 10\n",
      "running, count is : 11\n",
      "running, count is : 12\n",
      "running, count is : 13\n",
      "running, count is : 14\n",
      "('Total Reward: ', 14.0)\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# implementation using a python dictionary as the Q table\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "class QTabular():\n",
    "    \n",
    "    #, buckets = (1,1,6,12,)\n",
    "    def __init__(self, alpha = 0.1, gamma = 0, epsilon = 0.1,numberOfEpisodes = 2):\n",
    "        \n",
    "        self.alpha = alpha # learning rate, 0.1 is the optimal paramters taken from Ferdinands Tabular Grid Search\n",
    "        self.gamma = gamma # discount factor, used to control the \"weight\" that is given to future reward\n",
    "        self.epsilon = epsilon # how much we want to explore...see multi armed bandit workbook\n",
    "        #self.buckets = buckets # downscaling the feature space from continous to these 'bucket' sizes\n",
    "                               # tuned parameters borrowed from Ferdinand \n",
    "        \n",
    "            \n",
    "            \n",
    "        self.numberOfEpisodes = numberOfEpisodes\n",
    "            \n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        self.S_initial = self.env.reset() # intitial observation from openAi gym\n",
    "        \n",
    "        \n",
    "        self.buckets = []\n",
    "        \n",
    "        #this is bad, should write code to automatically detect thesem but, uh, I am lazy\n",
    "        # parameter numbers  from Will's notebook referanced above\n",
    "        self.buckets.append(np.linspace(-2.4, 2.4, 1))\n",
    "        self.buckets.append(np.linspace(-0.5, 0.5, 1))\n",
    "        self.buckets.append(np.linspace(-41.8, 41.8, 5))\n",
    "        self.buckets.append(np.linspace(-math.radians(50), math.radians(50), 3))\n",
    "        \n",
    "        #\n",
    "        #print self.buckets[0], self.buckets[1], self.buckets[2], self.buckets[3]\n",
    "        \n",
    "        \n",
    "        #using Wills idea of a python dictionary for the Q table\n",
    "        self.Q = {} \n",
    "        \n",
    "        #measuring the dimensionality (length) of the observation space\n",
    "        \n",
    "        self.observationSpaceLength = env.observation_space.shape\n",
    "        self.obs_length = self.observationSpaceLength[0]\n",
    "        \n",
    "        #measuring the dimensionality of the action space\n",
    "        self.n = self.env.action_space.n\n",
    "        \n",
    "        print \"dimensionality of action space is is : \", self.n\n",
    "        print \"dimensionality of observation space is is : \", self.obs_length\n",
    "        \n",
    "               \n",
    "    \n",
    "    \n",
    "    #this function allocated the raw observation data into its right \"bucket\" allocated above\n",
    "    def quantizeState(self, state):\n",
    "        \n",
    "        quantizedState_index = []\n",
    "        \n",
    "        #print state[0], state[1], state[2], state[3]\n",
    "        \n",
    "        for i in range(4):\n",
    "            \n",
    "            x =  np.digitize(state[i], self.buckets[i])\n",
    "            \n",
    "            #digitize notation starts at 1 therefore we have to subtract 1 to get it inline\n",
    "            #with our array notation... (this took 4 hours to figure out)\n",
    "            x = x-1\n",
    "            \n",
    "\n",
    "            if x < 0:\n",
    "                x = 0\n",
    "            \n",
    "            quantizedState_index.append(x)                       \n",
    "    \n",
    "        #returns a pointer that can be used to retrieve the specific \"bucket\" in questions value\n",
    "        return tuple(quantizedState_index)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def resetState(self):\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def getEnvironmentVariables(self,action):\n",
    "        \n",
    "        observation, reward, done, _ =  self.env.step(action)\n",
    "        \n",
    "        return(observation, reward, done)\n",
    "    \n",
    "    \n",
    "    def randomAction(self):\n",
    "                          \n",
    "        action = self.env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "                          \n",
    "    \n",
    "    def getAction(self, currentState, episodeCounter):\n",
    "    \n",
    "        #implementing the epsilon-greedy algorithm here\n",
    "        if(np.random.random(1)[0] < (1.0-epsilon)):\n",
    "            print \"yo mama\"\n",
    "            \n",
    "            #print 'in GREEDY/exploitation mode'\n",
    "            \n",
    "            #best_action_index = np.argmax(q_table)\n",
    " \n",
    "        \n",
    "            #print best_action_index\n",
    "        \n",
    "        \n",
    "            #action = best_action_index\n",
    "            \n",
    "            #def get_best_action(self, s):\n",
    "            #return np.argmax(self.Q[s[0], s[1], s[2], s[3]])\n",
    "            \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            action = self.env.action_space.sample()\n",
    "            #print 'in Exploration mode'\n",
    "            \n",
    "        return action\n",
    "                          \n",
    "            \n",
    "    def getQvalues(self, state, action=None):\n",
    "        \n",
    "        #print type(state)\n",
    "        #print state\n",
    "        #if the state key 'state' is not in the current table, set its Q values to 0.\n",
    "        if not state in self.Q:\n",
    "            self.Q[state] = [0, 0]\n",
    "        \n",
    "        #if the state already has a Q value, return it, otherwise return the zeroed version of it\n",
    "        if action is not None:\n",
    "            return self.Q[state][action] #returns a single Q value given an action\n",
    "        else:\n",
    "            return self.Q[state] #returns array of q values , [0, 0] , given above\n",
    "            \n",
    "                          \n",
    "    \n",
    "    def getMaxQ(self, S_dash):\n",
    "        \n",
    "        tempQvaluesArray = self.getQvalues(S_dash)\n",
    "        \n",
    "        #maxQ\n",
    "        #print(\"maxQ:\", np.max(tempQvaluesArray))\n",
    "\n",
    "        #argmax the \"index\" of the maximum Q value (0 or 1 as we only have two possible actions)\n",
    "        #print(\"argmaxQ:\", np.argmax(tempQvaluesArray))\n",
    "        \n",
    "        return np.argmax(tempQvaluesArray)\n",
    "        \n",
    "        \n",
    "    def updateQ(self, S_initial, S_dash, action, reward, maxQindex):\n",
    "\n",
    "        \n",
    "        #action is just an int.\n",
    "        ###################################################################################\n",
    "        \n",
    "        Old_Q = self.getQvalues(S_initial, action)\n",
    "        \n",
    "        print \"Old_Q is:\",Old_Q\n",
    "        \n",
    "        print \"Max Q is:\",self.getQvalues(S_dash, maxQindex)\n",
    "        print \"reward is:\",reward\n",
    "        print \"alpha is:\", self.alpha\n",
    "        print \"gamma is:\", self.gamma\n",
    "        \n",
    "        new_Q = Old_Q + self.alpha*(reward+self.gamma*(self.getQvalues(S_dash, maxQindex))-Old_Q)\n",
    "        \n",
    "        print \"new Q is\", new_Q\n",
    "        \n",
    "        self.Q[S_initial][action] = new_Q\n",
    "        \n",
    "\n",
    "            \n",
    "            #self.Q[k,int(S_dash[k]),action] = Old_q + self.alpha*(reward+self.gamma*(int(maxQVector[k]))-Old_q)\n",
    "    \n",
    "    def printQ(self):\n",
    "        for s in self.Q:\n",
    "            print(s, self.Q[s])\n",
    "            \n",
    "\n",
    "\n",
    "  \n",
    "    def train(self):\n",
    "        \n",
    "        for k in range(self.numberOfEpisodes):\n",
    "            \n",
    "                          \n",
    "            #first we take want to quantize the observation outputs from the environment\n",
    "            S_initial = self.quantizeState(self.resetState()) #selecting a random initial state\n",
    "            \n",
    "            #set the Q values for this initial state (0,0)\n",
    "            self.getQvalues(S_initial)\n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            print \"nth episode\", counter\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                \n",
    "                \n",
    "                #env.render()\n",
    "                \n",
    "                #S_initial_temp = S_initial\n",
    "\n",
    "                #take some action defined using epsilon greedy maybe? (to be implemented)\n",
    "                #take a random action\n",
    "                action = self.randomAction()\n",
    "                \n",
    "                #print \"action is:\", action\n",
    "                               \n",
    "                #observation, reward, done\n",
    "                \n",
    "                #print \"getting a new observation\"\n",
    "                \n",
    "                S_dash, reward, done = self.getEnvironmentVariables(action)\n",
    "                S_dash = self.quantizeState(S_dash)\n",
    "                \n",
    "                #print type(S_dash)\n",
    "                \n",
    "                \n",
    "                #getting the max Q value index ( so, which action should we take....)\n",
    "                Qindex = self.getMaxQ(S_dash)\n",
    "                \n",
    "                #getMaxQ\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #print MaxQVector[0], MaxQVector[1], MaxQVector[2], MaxQVector[3]\n",
    "                \n",
    "                self.updateQ(S_initial,S_dash, action, reward, Qindex)\n",
    "                \n",
    "                S_initial = S_dash\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "                if done:\n",
    "                    print \"completed training\"\n",
    "                    break;\n",
    "                    \n",
    "                \n",
    "\n",
    "                \n",
    "                #break()\n",
    "        \n",
    "                \n",
    "                #print \"running....\"\n",
    "                          \n",
    "                #getAction(self, currentState, episodeCounter)\n",
    "                #action = getAction(S_initial)\n",
    "                \n",
    "                #implement tabular Q here\n",
    "                    \n",
    "\n",
    "    def run(self):\n",
    "            \n",
    "        self.env.reset()\n",
    "        self.env.render()\n",
    "            \n",
    "        S_initial = self.quantizeState(self.resetState())\n",
    "            \n",
    "        episode_reward = 0\n",
    "        \n",
    "        counter = 0\n",
    "            \n",
    "        while True:\n",
    "            self.env.render()\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            print \"running, count is :\", counter\n",
    "                \n",
    "            #getMaxQ\n",
    "            \n",
    "            MaxQIndex = self.getMaxQ(S_initial)\n",
    "                \n",
    "            S_dash, reward, done = self.getEnvironmentVariables(MaxQIndex)\n",
    "            S_dash = self.quantizeState(S_dash)\n",
    "                #S_dash = self.quantizeState(S_dash)\n",
    "                \n",
    "                #S_dash, reward, done = self.getEnvironmentVariables(action)\n",
    "                #S_dash = self.quantizeState(S_dash)\n",
    "                \n",
    "                #action = self.get_best_action(self.s)\n",
    "                #r, done = self.step(action)\n",
    "                \n",
    "            S_initial = S_dash\n",
    "            episode_reward += reward\n",
    "#             time.sleep(0.1)\n",
    "            if done:\n",
    "                break;\n",
    "        \n",
    "        self.env.render(close=True)\n",
    "        print(\"Total Reward: \", episode_reward)              \n",
    "                \n",
    "                \n",
    "                \n",
    "                   \n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                #updating openAi gym with our new action determined from the epsilon-greedy algorithm above \n",
    "                \n",
    "                #S, reward, done, _ =  getState(action)  # _ = delete info, previously the 'info' was here\n",
    "        \n",
    " \n",
    "    \n",
    "#tester = QTabular()        \n",
    "                          \n",
    "#QTabular.run()   \n",
    "\n",
    "#test = QTabular(numberOfEpisodes = 200)\n",
    "test = QTabular(numberOfEpisodes = 100)\n",
    "test.train()\n",
    "test.printQ()\n",
    "test.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "buckets = []\n",
    "buckets = buckets.append(np.linspace(3.40282347e+38 , -3.40282347e+38, 12))\n",
    "\n",
    "print buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for k in range(4):\n",
    "    print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "QTable = np.zeros((5, 5, 2))\n",
    "print QTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((4,12,2))\n",
    "\n",
    "\n",
    "testQ = []\n",
    "\n",
    "testQ = testQ.append((12,2))\n",
    "\n",
    "print testQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-23 14:09:12,103] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02610283 -0.01238608  0.04940541  0.0400357 ]\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "print observation\n",
    "print type(observation)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "action = env.action_space.sample()\n",
    "\n",
    "n = env.action_space.n\n",
    "\n",
    "print action\n",
    "print n\n",
    "\n",
    "observationSpaceLength = env.observation_space.shape\n",
    "\n",
    "print observationSpaceLength[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buckets = []\n",
    "        \n",
    "        #this is bad, should write code to automatically detect thesem but, uh, I am lazy\n",
    "buckets.append(np.linspace(-4.8, 4.8, 12))\n",
    "buckets.append(np.linspace(-3.40282347e+38, 3.40282347e+38, 12))\n",
    "buckets.append(np.linspace(-4.18879020e-01, 4.18879020e-01 , 12))\n",
    "buckets.append(np.linspace(-3.40282347e+38 , 3.40282347e+38, 12))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rewriting this using Wills numbers, lol\n",
    "\n",
    "buckets = []\n",
    "        \n",
    "buckets.append(np.linspace(-2.4, 2.4, 1))\n",
    "buckets.append(np.linspace(-0.5, 0.5, 1))\n",
    "buckets.append(np.linspace(-41.8, 41.8, 5))\n",
    "buckets.append(np.linspace(-math.radians(50), math.radians(50), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.]\n",
      "[ 0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([])\n",
    "n = 2\n",
    "        \n",
    "for i in range(4):\n",
    "            \n",
    "    t = buckets[i].shape            \n",
    "    Q = np.append(Q, np.zeros((t[0],n)))\n",
    "    print Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "5\n",
      "3\n",
      "[array([[ 0.,  0.]]), array([[ 0.,  0.]]), array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]]), array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "Q = []\n",
    "n = 2\n",
    "\n",
    "#Q = np.zeros((4,12,2))\n",
    "\n",
    "for i in range(4):\n",
    "            \n",
    "    t = buckets[i].shape \n",
    "    print (t[0])\n",
    "    Qtemp =  np.zeros((t[0],n))\n",
    "    #Qtemp =  np.zeros((2,2))\n",
    "    Q.append(Qtemp)\n",
    "    \n",
    "    \n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "<type 'tuple'>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "\n",
    "    n = buckets[2].shape\n",
    "\n",
    "#print n\n",
    "\n",
    "#print type(n)\n",
    "#print n[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state + 1 is : [-0.01823837 -0.02303988  0.03254898 -0.01319891]\n",
      "\n",
      "\n",
      "quantized bucket # 1 is : [-2.4]\n",
      "\n",
      "\n",
      "quantized bucket # 2 is : [-0.5]\n",
      "\n",
      "\n",
      "quantized bucket # 3 is : [-41.8 -20.9   0.   20.9  41.8]\n",
      "\n",
      "\n",
      "quantized bucket # 4 is : [-0.87266463  0.          0.87266463]\n",
      "\n",
      "\n",
      "now printing allocated buckets...\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#[np.digitize(s_plus1[i], self.bins[i]) for i in range(4)]\n",
    "\n",
    "quantizedState_index = []\n",
    "\n",
    "print \"state + 1 is :\", state\n",
    "print \"\\n\"\n",
    "print \"quantized bucket # 1 is :\", buckets[0]\n",
    "print \"\\n\"\n",
    "print \"quantized bucket # 2 is :\", buckets[1]\n",
    "print \"\\n\"\n",
    "print \"quantized bucket # 3 is :\", buckets[2]\n",
    "print \"\\n\"\n",
    "print \"quantized bucket # 4 is :\", buckets[3]\n",
    "print \"\\n\"\n",
    "\n",
    "print \"now printing allocated buckets...\"\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    quantizedState_index.append(np.digitize(state[i], buckets[i]))\n",
    "    \n",
    "    print np.digitize(state[i], buckets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(1), array(1), array(3), array(1)]\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "now printing allocated buckets NUMBERS...\n",
      "1\n",
      "-2.4\n",
      "1\n",
      "-0.5\n",
      "3\n",
      "0.0\n",
      "1\n",
      "-0.872664625997\n"
     ]
    }
   ],
   "source": [
    "#quatnizedState = buckets[quantizedState_index[0],quantizedState_index[1],quantizedState_index[2],quantizedState_index[3]]\n",
    "\n",
    "print quantizedState_index\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print quantizedState_index[i]\n",
    "    \n",
    "\n",
    "print \"now printing allocated buckets NUMBERS...\"\n",
    "\n",
    "quantizedState = []\n",
    "\n",
    "for i in range(4):\n",
    "    #print \"test\"\n",
    "    #print np.digitize(state[i], buckets[i])\n",
    "    \n",
    "    tempvar = quantizedState_index[i]\n",
    "    \n",
    "    #array3[0][1]\n",
    "    \n",
    "    print tempvar\n",
    "    \n",
    "    print buckets[i][tempvar-1]\n",
    "    \n",
    "    quantizedState.append(buckets[i][tempvar-1])\n",
    "    \n",
    "\n",
    "        \n",
    "        #fuck yeah working!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3999999999999999, -0.5, 0.0, -0.87266462599716477]\n",
      "-2.4\n",
      "-0.5\n",
      "0.0\n",
      "-0.872664625997\n"
     ]
    }
   ],
   "source": [
    "print quantizedState\n",
    "\n",
    "for i in range(4):\n",
    "    print quantizedState[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]]\n",
      "<type 'list'>\n",
      "<type 'numpy.ndarray'>\n",
      "[3, 4]\n",
      "11\n",
      "[2 3]\n",
      "1\n",
      "5\n",
      "5\n",
      "[[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]\n",
      "\n",
      " [[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]\n",
      "\n",
      " [[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b = [3,4]\n",
    "\n",
    "\n",
    "c = np.arange(12).reshape(6,2)\n",
    "d = np.array([c,c,c])\n",
    "\n",
    "Q = [a,b] \n",
    "\n",
    "k = 1\n",
    "\n",
    "maxQ = np.argmax(c)\n",
    "\n",
    "print c\n",
    "\n",
    "print type(a)\n",
    "print type(c)\n",
    "print Q[k]\n",
    "print maxQ\n",
    "\n",
    "\n",
    "print c[1,:]\n",
    "\n",
    "#how the hell does argmax work?\n",
    "print np.argmax(c[1,:])\n",
    "\n",
    "#max_value = max(my_list)\n",
    "\n",
    "print max(c[2,:])\n",
    "\n",
    "print c[2,1]\n",
    "\n",
    "print d\n",
    "\n",
    "#first one equals matrix (3rd) dimension\n",
    "#second one equals row dimension\n",
    "#third one equals column dimension\n",
    "#print d[0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-19f0a820442c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "Q = np.array([1,2,3])\n",
    "\n",
    "c = np.array([7,8,9])\n",
    "        \n",
    "\n",
    "Q.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#a = reshape(arange(9),(3,3))\n",
    "\n",
    "a = [[1,2],[3,4]]\n",
    "\n",
    "\n",
    "Q = np.zeros((5,2))\n",
    "\n",
    "print type(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets write this code the traditional way, then change it into a class\n",
    "\n",
    "#alpha = 0.1, gamma = 0, epsilon = 0.1,numberOfEpisodes = 200):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12)\n",
      "(2,)\n",
      "shape of buckets is: (4,)\n",
      "[[[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "#numpy array dimension experiment....\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "A = np.zeros((6,12))\n",
    "print np.shape(A)\n",
    "\n",
    "#B = np.zeros((1,2))\n",
    "B = np.zeros((2,))\n",
    "print np.shape(B)\n",
    "\n",
    "#C = A + B\n",
    "#print np.shape(C)\n",
    "\n",
    "\n",
    "\n",
    "buckets=(1, 1, 6, 12,)\n",
    "print \"shape of buckets is:\", np.shape(buckets)\n",
    "\n",
    "#print A\n",
    "\n",
    "Q = np.zeros((4,12,2))\n",
    "\n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-21 21:38:28,466] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Discrete.contains of Discrete(2)>\n",
      "(2,)\n",
      "(1, 1, 6, 12)\n",
      "2\n",
      "[[[[[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]]]]\n"
     ]
    }
   ],
   "source": [
    "#perhaps instead of trying to reverse engineer it tomorrow, write it from scratch?\n",
    "\n",
    "#figuring out how the Q matrix is defined for n dimensional space for example below\n",
    "#  by playing around with the different values for 'buckets' and different permutations of 'Q'\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "buckets=(1, 1, 12, 12,) #original, optimal bucket sizes are (1, 1, 6, 12,) \n",
    "#buckets=( 6, 12, )\n",
    "\n",
    "#buckets= (1, 1,6, 12)\n",
    "#np.zeros((2, 1))\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# for a Q table:\n",
    "\n",
    "# ROWS = STATES\n",
    "# COLUMNS = ACTIONS\n",
    "\n",
    "Q = np.zeros(buckets + (env.action_space.n,)) #original\n",
    "\n",
    "#Q = np.zeros((2,)) #produces 1 x 2\n",
    "\n",
    "\n",
    "#Q = np.zeros(buckets) # produces a  6 x 12 matrix\n",
    "#Q = np.zeros((env.action_space.n,)) # produces a 1 x 2 matrix\n",
    "\n",
    "print env.action_space.contains\n",
    "print env.action_space.shape\n",
    "\n",
    "print buckets\n",
    "print env.action_space.n\n",
    "\n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-20 15:10:39,512] Making new env: CartPole-v0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-14d815ddb378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQCartPoleSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;31m# gym.upload('tmp/cartpole-1', api_key='')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14d815ddb378>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14d815ddb378>\u001b[0m in \u001b[0;36mget_alpha\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mada_divisor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mada_divisor\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m    \u001b[0;31m# (math.log(boxCount if boxCount>0 else 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "# (not quite) working example off the internet (not my code)\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "class QCartPoleSolver():\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), n_episodes=1000, n_win_ticks=195, min_alpha=0.1, min_epsilon=0.1, gamma=1.0, ada_divisor=25, max_env_steps=None, quiet=False, monitor=False):\n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        self.n_episodes = n_episodes # training episodes \n",
    "        self.n_win_ticks = n_win_ticks # average ticks over 100 episodes required for win\n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.ada_divisor = ada_divisor # only for development purposes\n",
    "        self.quiet = quiet\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True) # record results for upload\n",
    "\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(self.buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    def update_q(self, state_old, action, reward, state_new, alpha):\n",
    "        self.Q[state_old][action] += alpha * (reward + self.gamma * np.max(self.Q[state_new]) - self.Q[state_old][action])\n",
    "        \n",
    "        print self.Q\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - math.log10(((t + 1) if (t + 1)>0 else 1) / (self.ada_divisor if self.ada_divisor>0 else 1))))\n",
    "    \n",
    "   # (math.log(boxCount if boxCount>0 else 1))\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "\n",
    "            alpha = self.get_alpha(e)\n",
    "            epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            i = 0\n",
    "\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.choose_action(current_state, epsilon)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize(obs)\n",
    "                #print self.\n",
    "                self.update_q(current_state, action, reward, new_state, alpha)\n",
    "                current_state = new_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    solver = QCartPoleSolver()\n",
    "    solver.run()\n",
    "# gym.upload('tmp/cartpole-1', api_key='')\n",
    "\n",
    "#(math.log(boxCount if boxCount>0 else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01823837 -0.02303988  0.03254898 -0.01319891]\n",
      "<type 'numpy.ndarray'>\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#old code (not using any of this)\n",
    "\n",
    "state = observation\n",
    "\n",
    "print state\n",
    "\n",
    "quantizedState_index = [np.digitize(state[0], buckets[0]), np.digitize(state[1], buckets[1]), np.digitize(state[2], buckets[2]), np.digitize(state[3], buckets[3])]\n",
    "#quatnizedState = buckets[quantizedState_index[0],quantizedState_index[1],quantizedState_index[2],quantizedState_index[3]]\n",
    "\n",
    "#print quantizedState[0]\n",
    "print type(quantizedState_index[0])\n",
    "print quantizedState_index[0]\n",
    "print quantizedState_index[1]\n",
    "print quantizedState_index[2]\n",
    "print quantizedState_index[3]\n",
    "\n",
    "#quatnizedState = buckets[3,6,9,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1, 2, 3): [1, 2], (1, 1, 3, 4): [1, 2], (1, 1, 3, 5): [1, 2], (1, 1, 3, 3): [1, 2]}\n",
      "((1, 1, 2, 3), [1, 2])\n",
      "((1, 1, 3, 4), [1, 2])\n",
      "((1, 1, 3, 5), [1, 2])\n",
      "((1, 1, 3, 3), [1, 2])\n",
      "('actions are: ', [1, 2])\n",
      "('maxQ:', 2)\n",
      "('argmaxQ:', 1)\n",
      "{(1, 1, 2, 3): [1, 2], (1, 1, 3, 4): [1, 2], (1, 1, 3, 5): [1, 2], (1, 1, 3, 3): [1, 2], (2, 2, 2, 3): [2, 2]}\n",
      "('state:', (1, 1, 2, 3), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 4), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 5), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 3), 'actions:', [1, 2])\n",
      "('state:', (2, 2, 2, 3), 'actions:', [2, 2])\n",
      "('argmaxQ:', 0)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Q = {}\n",
    "\n",
    "Q[(1, 1, 3, 3)] = [1, 2]\n",
    "Q[(1, 1, 3, 4)] = [1, 2]\n",
    "Q[(1, 1, 3, 5)] = [1, 2]\n",
    "Q[(1, 1, 2, 3)] = [1, 2]\n",
    "print(Q)\n",
    "\n",
    "for s in Q:\n",
    "    print(s, Q[s])\n",
    "\n",
    "actions = Q[(1, 1, 3, 3)]  # retrieve both actions Q values from Q(s)\n",
    "print(\"actions are: \",actions)\n",
    "\n",
    "#maxQ\n",
    "print(\"maxQ:\", np.max(actions))\n",
    "\n",
    "#argmax i.e. action with highest value\n",
    "print(\"argmaxQ:\", np.argmax(actions))\n",
    "\n",
    "\n",
    "Q[(2, 2, 2, 3)] = [2, 2]\n",
    "print(Q)\n",
    "for i in Q:\n",
    "    print(\"state:\", i, \"actions:\", Q[i])\n",
    "    \n",
    "    \n",
    "# to take an action, action can only be 0 or 1, which essentially is the position\n",
    "#argmax i.e. action with highest value\n",
    "print(\"argmaxQ:\", np.argmax([15, 10]))\n",
    "\n",
    "#Q = Q(s) + alpha * (gamma * maxQ(s') - Q(s))\n",
    "\n",
    "print(np.max([25, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n",
      "{(1, 1, 2, 3): [1, 2], (1, 1, 3, 4): [1, 2], (1, 1, 3, 5): [1, 2], (1, 1, 3, 3): [1, 2]}\n",
      "((1, 1, 2, 3), [1, 2])\n",
      "((1, 1, 3, 4), [1, 2])\n",
      "((1, 1, 3, 5), [1, 2])\n",
      "((1, 1, 3, 3), [1, 2])\n",
      "('Qvalues are: ', [1, 2])\n",
      "('maxQ:', 2)\n",
      "('argmaxQ:', 1)\n",
      "{(1, 1, 2, 3): [1, 2], (1, 1, 3, 4): [1, 2], (1, 1, 3, 5): [1, 2], (1, 1, 3, 3): [1, 2], (2, 2, 2, 3): [2, 2]}\n",
      "('state:', (1, 1, 2, 3), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 4), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 5), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 3), 'actions:', [1, 2])\n",
      "('state:', (2, 2, 2, 3), 'actions:', [2, 2])\n",
      "('argmaxQ:', 0)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Q = {}\n",
    "\n",
    "d =(1, 1, 3, 3)\n",
    "\n",
    "print type(d)\n",
    "\n",
    "Q[(1, 1, 3, 3)] = [1, 2]\n",
    "Q[(1, 1, 3, 4)] = [1, 2]\n",
    "Q[(1, 1, 3, 5)] = [1, 2]\n",
    "Q[(1, 1, 2, 3)] = [1, 2]\n",
    "print(Q)\n",
    "\n",
    "for s in Q:\n",
    "    print(s, Q[s])\n",
    "\n",
    "Qvalues = Q[(1, 1, 3, 3)]  # retrieve both actions Q values from Q(s)\n",
    "print(\"Qvalues are: \",Qvalues)\n",
    "\n",
    "#maxQ\n",
    "print(\"maxQ:\", np.max(Qvalues))\n",
    "\n",
    "#argmax i.e. action with highest value\n",
    "print(\"argmaxQ:\", np.argmax(Qvalues))\n",
    "\n",
    "\n",
    "Q[(2, 2, 2, 3)] = [2, 2]\n",
    "print(Q)\n",
    "for i in Q:\n",
    "    print(\"state:\", i, \"actions:\", Q[i])\n",
    "    \n",
    "    \n",
    "# to take an action, action can only be 0 or 1, which essentially is the position\n",
    "#argmax i.e. action with highest value\n",
    "print(\"argmaxQ:\", np.argmax([15, 10]))\n",
    "\n",
    "#Q = Q(s) + alpha * (gamma * maxQ(s') - Q(s))\n",
    "\n",
    "print(np.max([25, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "#testing referancing python dictionaries\n",
    "\n",
    "import numpy as np\n",
    "Q = {}\n",
    "\n",
    "Q[(1, 1, 3, 3)] = [1, 2]\n",
    "\n",
    "print Q[(1, 1, 3, 3)][0] # this notation correctly calls the right Q value\n",
    "print Q[(1, 1, 3, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# This original implementation used 2 dimensional matrices stored in an array to maintain the Q table\n",
    "# turns out this approach is extremely hard to get working correctly\n",
    "#\n",
    "# switched to using Will's idea of a using a python dictionary, as it is far superior!\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "class QTabular():\n",
    "    \n",
    "    #, buckets = (1,1,6,12,)\n",
    "    def __init__(self, alpha = 0.1, gamma = 0, epsilon = 0.1,numberOfEpisodes = 2):\n",
    "        \n",
    "        self.alpha = alpha # learning rate, 0.1 is the optimal paramters taken from Ferdinands Tabular Grid Search\n",
    "        self.gamma = gamma # discount factor, used to control the \"weight\" that is given to future reward\n",
    "        self.epsilon = epsilon # how much we want to explore...see multi armed bandit workbook\n",
    "        #self.buckets = buckets # downscaling the feature space from continous to these 'bucket' sizes\n",
    "                               # tuned parameters borrowed from Ferdinand \n",
    "        \n",
    "            \n",
    "            \n",
    "        self.numberOfEpisodes = numberOfEpisodes\n",
    "            \n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        self.S_initial = self.env.reset() # intitial observation from openAi gym\n",
    "        \n",
    "        \n",
    "        self.buckets = []\n",
    "        \n",
    "        #this is bad, should write code to automatically detect thesem but, uh, I am lazy\n",
    "        # parameter numbers  from Will's notebook referanced above\n",
    "        self.buckets.append(np.linspace(-2.4, 2.4, 1))\n",
    "        self.buckets.append(np.linspace(-0.5, 0.5, 1))\n",
    "        self.buckets.append(np.linspace(-41.8, 41.8, 5))\n",
    "        self.buckets.append(np.linspace(-math.radians(50), math.radians(50), 3))\n",
    "        \n",
    "        #\n",
    "        print self.buckets[0], self.buckets[1], self.buckets[2], self.buckets[3]\n",
    "        \n",
    "        #self.Q = np.zeros((4,12,2))\n",
    "        \n",
    "        #using Wills idea of a python dictionary for the Q table\n",
    "        #self.Q = {} #this method doesnt make sense either :(\n",
    "        \n",
    "        #measuring the dimensionality (length) of the observation space\n",
    "        \n",
    "        self.observationSpaceLength = env.observation_space.shape\n",
    "        self.obs_length = self.observationSpaceLength[0]\n",
    "        \n",
    "        #measuring the dimensionality of the action space\n",
    "        self.n = self.env.action_space.n\n",
    "        \n",
    "        print \"dimensionality of action space is is : \", self.n\n",
    "        print \"dimensionality of observation space is is : \", self.obs_length\n",
    "        \n",
    "        # for a Q table:\n",
    "\n",
    "        #  \n",
    "        \n",
    "        #np.array(test)   \n",
    "        \n",
    "        self.Q = [] \n",
    "        #np.array([])\n",
    "               \n",
    "    \n",
    "    \n",
    "    #this function allocated the raw observation data into its right \"bucket\" allocated above\n",
    "    def quantizeState(self, state):\n",
    "        \n",
    "        quantizedState_index = []\n",
    "        \n",
    "        print state[0], state[1], state[2], state[3]\n",
    "        \n",
    "        for i in range(4):\n",
    "    \n",
    "            #quantizedState_index.append((np.digitize(state[i], self.buckets[i])))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "            #print \"test\"\n",
    "            \n",
    "            x =  np.digitize(state[i], self.buckets[i])\n",
    "            \n",
    "            #digitize notation starts at 1 therefore we have to subtract 1 to get it inline\n",
    "            #with our array notation... (this took 4 hours to figure out)\n",
    "            x = x-1\n",
    "            \n",
    "            #print \"state[i] is:\", state[i]\n",
    "            #print \"buckets[i] Is:\", self.buckets[i]\n",
    "            #print \"bucket assignment is:\", x\n",
    "            \n",
    "            if x < 0:\n",
    "                x = 0\n",
    "            \n",
    "            quantizedState_index.append(x)\n",
    "        \n",
    "        #because the state matrix is one dimensional, digitize is fucking out and assigning 1 instead of the\n",
    "        #correct state index when we only have a 1x1 matrix (scalar)\n",
    "        #quantizedState_index[0] = 0\n",
    "        #quantizedState_index[1] = 0\n",
    "            \n",
    "        \n",
    "        #print quantizedState_index[0], quantizedState_index[1], quantizedState_index[2], quantizedState_index[3]\n",
    "        \n",
    "        #quantizedState = []\n",
    "        \n",
    "        #for i in range(4):\n",
    "\n",
    "\n",
    "            #tempvar = quantizedState_index[i]\n",
    "            \n",
    "            #quantizedState.append(buckets[i][tempvar-1])\n",
    "                          \n",
    "    \n",
    "        #returns a pointer that can be used to retrieve the specific \"bucket\" in questions value\n",
    "        return quantizedState_index\n",
    "        \n",
    "        \n",
    "    \n",
    "    def resetState(self):\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def getEnvironmentVariables(self,action):\n",
    "        \n",
    "        observation, reward, done, _ =  self.env.step(action)\n",
    "        \n",
    "        return(observation, reward, done)\n",
    "    \n",
    "    \n",
    "    def randomAction(self):\n",
    "                          \n",
    "        action = self.env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "                          \n",
    "    \n",
    "    def getAction(self, currentState, episodeCounter):\n",
    "    \n",
    "        #implementing the epsilon-greedy algorithm here\n",
    "        if(np.random.random(1)[0] < (1.0-epsilon)):\n",
    "            print \"yo mama\"\n",
    "            \n",
    "            #print 'in GREEDY/exploitation mode'\n",
    "            \n",
    "            #best_action_index = np.argmax(q_table)\n",
    " \n",
    "        \n",
    "            #print best_action_index\n",
    "        \n",
    "        \n",
    "            #action = best_action_index\n",
    "            \n",
    "            #def get_best_action(self, s):\n",
    "            #return np.argmax(self.Q[s[0], s[1], s[2], s[3]])\n",
    "            \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            action = self.env.action_space.sample()\n",
    "            #print 'in Exploration mode'\n",
    "            \n",
    "        return action\n",
    "                          \n",
    "    def initializeQ(self):\n",
    "        \n",
    "        for i in range(self.obs_length):\n",
    "            \n",
    "\n",
    "            t = self.buckets[i].shape            \n",
    "            self.Q.append(np.zeros((t[0],self.n)))\n",
    "                          \n",
    "    \n",
    "    def getMaxQ(self, S_dash):\n",
    "            \n",
    "        MaxQVector = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        print \"S_dash is:\", S_dash[0],S_dash[1],S_dash[2], S_dash[3]\n",
    "        \n",
    "                          \n",
    "        for k in range(self.obs_length):\n",
    "            \n",
    "           \n",
    "            #print \"S_dash is:\",S_dash\n",
    "            \n",
    "            #print \"the type of s_dash is:\",type(S_dash)\n",
    "            \n",
    "            \n",
    "            #print \"k is:\", k\n",
    "            #print \"the kth element of s dash is\", S_dash[k]\n",
    "            localQ = self.Q[k]\n",
    "            \n",
    "            print localQ\n",
    "            \n",
    "            tempVar = S_dash[k]\n",
    "            \n",
    "            #print \"S_dash for this bin is:\", tempVar\n",
    "            \n",
    "            MaxQ = max(localQ[tempVar,:])\n",
    "            \n",
    "            #print \"tempq is\", tempQ\n",
    "            \n",
    "            #MaxQ = max(localQ[(S_dash[k]),:])\n",
    "            \n",
    "            #print \"maxQ is: \", MaxQ\n",
    "            #this is all wrong....\n",
    "            #we want to get the max Q value (not the max S.....)\n",
    "            # the S just gives the index of which row to go to in the Q table\n",
    "            \n",
    "            #MaxQ =  #max(S_dash[k,:])\n",
    "            MaxQVector.append(MaxQ)\n",
    "            \n",
    "            #MaxQVector = 1\n",
    "            \n",
    "            #print MaxQVector\n",
    "                          \n",
    "        return MaxQVector\n",
    "                          \n",
    "            #maxQ = np.argmax(self.Q[k])\n",
    "            #max_value = max(my_list)\n",
    "            #max_index = my_list.index(max_value)\n",
    "        \n",
    "    def updateQ(self, S_initial, S_dash, action, reward, maxQVector):\n",
    "        print \"test\"\n",
    "        \n",
    "        #action is just an int.\n",
    "        \n",
    "        for k in range(self.obs_length):\n",
    "            \n",
    "            print \"k is:\", k\n",
    "            \n",
    "            localQ = self.Q[k]\n",
    "            \n",
    "            Old_q = localQ[(S_initial[k]),action]\n",
    "            \n",
    "            print type(k)\n",
    "            print type(S_dash[k])\n",
    "            print type(action)\n",
    "            print type(maxQVector[k])\n",
    "            \n",
    "            #self.Q[k,int(S_dash[k]),action] = Old_q + self.alpha*(reward+self.gamma*(int(maxQVector[k]))-Old_q)\n",
    "            self.Q[k][int(S_dash[k])][action] = Old_q + self.alpha*(reward+self.gamma*(int(maxQVector[k]))-Old_q)\n",
    "            \n",
    "            #first one equals matrix (3rd) dimension\n",
    "            #second one equals row dimension\n",
    "            #third one equals column dimension\n",
    "            #print d[0,1,0]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        for k in range(self.numberOfEpisodes):\n",
    "            \n",
    "            #initializing our Q matrix\n",
    "            self.initializeQ()\n",
    "                          \n",
    "            #first we take want to quantize the observation outputs from the environment\n",
    "            S_initial = self.quantizeState(self.resetState()) #selecting a random initial state\n",
    "            \n",
    "            #f = 1 \n",
    "            \n",
    "            #print self.buckets[0]\n",
    "            #print self.buckets[1]\n",
    "            #print self.buckets[2]\n",
    "            #print self.buckets[3]\n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                print \"nth episode\", counter\n",
    "                \n",
    "                #env.render()\n",
    "                \n",
    "                #S_initial_temp = S_initial\n",
    "                \n",
    "                #action = #take some action defined using epsilon greedy maybe?\n",
    "                \n",
    "                #take a random action\n",
    "                action = self.randomAction()\n",
    "                \n",
    "                print \"action is:\", action\n",
    "                               \n",
    "                #observation, reward, done\n",
    "                \n",
    "                print \"getting a new observation\"\n",
    "                \n",
    "                S_dash, reward, done = self.getEnvironmentVariables(action)\n",
    "                S_dash = self.quantizeState(S_dash)\n",
    "                \n",
    "                #print type(S_dash)\n",
    "                \n",
    "                \n",
    "                MaxQVector = self.getMaxQ(S_dash)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                print MaxQVector[0], MaxQVector[1], MaxQVector[2], MaxQVector[3]\n",
    "                \n",
    "                self.updateQ(S_initial,S_dash, action, reward, MaxQVector)\n",
    "                \n",
    "                S_initial = S_dash\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "                if done:\n",
    "                    print \"completed training\"\n",
    "                    break;\n",
    "                    \n",
    "                \n",
    "\n",
    "                \n",
    "                #break()\n",
    "        \n",
    "                \n",
    "                #print \"running....\"\n",
    "                          \n",
    "                #getAction(self, currentState, episodeCounter)\n",
    "                #action = getAction(S_initial)\n",
    "                \n",
    "                #implement tabular Q here\n",
    "                    \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                   \n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                #updating openAi gym with our new action determined from the epsilon-greedy algorithm above \n",
    "                \n",
    "                #S, reward, done, _ =  getState(action)  # _ = delete info, previously the 'info' was here\n",
    "                \n",
    "\n",
    "        \n",
    " \n",
    "    \n",
    "#tester = QTabular()        \n",
    "                          \n",
    "#QTabular.run()   \n",
    "\n",
    "test = QTabular()\n",
    "test.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
