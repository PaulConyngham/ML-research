{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q Learning with OpenAi gym\n",
    "\n",
    "The purpose of this workbook is to work through a simple tabular q problem (cartpole) using OpenAi's gym\n",
    "\n",
    "https://gym.openai.com/envs/CartPole-v0/\n",
    "\n",
    "decription below:\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-01 11:59:27,434] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# put this up here incase the rest crashes..\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03994349  0.00017641 -0.01114045  0.04450254]\n",
      "[ 0.03994702  0.19545633 -0.0102504  -0.25167438]\n",
      "[ 0.04385614  0.39072314 -0.01528388 -0.54757277]\n",
      "[ 0.05167061  0.58605644 -0.02623534 -0.84503179]\n",
      "[ 0.06339174  0.3913021  -0.04313597 -0.56071312]\n",
      "[ 0.07121778  0.58700206 -0.05435024 -0.86666804]\n",
      "[ 0.08295782  0.78281985 -0.0716836  -1.17593211]\n",
      "[ 0.09861422  0.97879615 -0.09520224 -1.49019854]\n",
      "[ 0.11819014  1.17493957 -0.12500621 -1.81102857]\n",
      "[ 0.14168893  1.37121346 -0.16122678 -2.13979827]\n",
      "[ 0.1691132   1.56752054 -0.20402275 -2.47763563]\n",
      "Episode finished after 11 timesteps\n",
      "[ 0.04454963 -0.0260319  -0.02706994  0.00961162]\n",
      "[ 0.044029   -0.22075538 -0.0268777   0.29363225]\n",
      "[ 0.03961389 -0.41548402 -0.02100506  0.57771862]\n",
      "[ 0.03130421 -0.61030537 -0.00945069  0.86371114]\n",
      "[ 0.0190981  -0.80529739  0.00782354  1.15340765]\n",
      "[ 0.00299215 -1.00052051  0.03089169  1.44853347]\n",
      "[-0.01701826 -0.80579168  0.05986236  1.16566029]\n",
      "[-0.03313409 -1.00163949  0.08317556  1.47649483]\n",
      "[-0.05316688 -0.80762619  0.11270546  1.21090639]\n",
      "[-0.0693194  -0.61412515  0.13692359  0.95556164]\n",
      "[-0.08160191 -0.81079663  0.15603482  1.28793763]\n",
      "[-0.09781784 -1.00752108  0.18179357  1.62512769]\n",
      "Episode finished after 12 timesteps\n",
      "[ 0.02841858  0.03019321 -0.0061155  -0.02116198]\n",
      "[ 0.02902244  0.22540233 -0.00653874 -0.31576813]\n",
      "[ 0.03353049  0.42061681 -0.0128541  -0.61050596]\n",
      "[ 0.04194283  0.22567687 -0.02506422 -0.32189921]\n",
      "[ 0.04645636  0.42114661 -0.03150221 -0.62237984]\n",
      "[ 0.0548793   0.61669395 -0.0439498  -0.92481544]\n",
      "[ 0.06721318  0.81238109 -0.06244611 -1.23097979]\n",
      "[ 0.0834608   1.00824822 -0.08706571 -1.54255505]\n",
      "[ 0.10362576  0.81427407 -0.11791681 -1.27826093]\n",
      "[ 0.11991124  0.62083594 -0.14348203 -1.02470589]\n",
      "[ 0.13232796  0.42788579 -0.16397615 -0.78029389]\n",
      "[ 0.14088568  0.62483648 -0.17958202 -1.11975032]\n",
      "[ 0.15338241  0.82180058 -0.20197703 -1.46295902]\n",
      "Episode finished after 13 timesteps\n",
      "[-0.01592055 -0.00733719 -0.0460637   0.04588803]\n",
      "[-0.0160673   0.18841398 -0.04514594 -0.26096521]\n",
      "[-0.01229902  0.38415035 -0.05036524 -0.56753899]\n",
      "[-0.00461601  0.18976971 -0.06171602 -0.29113889]\n",
      "[-0.00082061 -0.00442044 -0.0675388  -0.01854122]\n",
      "[-0.00090902 -0.19851206 -0.06790962  0.25209115]\n",
      "[-0.00487926 -0.00248943 -0.0628678  -0.06121514]\n",
      "[-0.00492905  0.19347493 -0.0640921  -0.37305195]\n",
      "[-0.00105955 -0.00068078 -0.07155314 -0.10124653]\n",
      "[-0.00107317  0.19538981 -0.07357807 -0.41561852]\n",
      "[ 0.00283463  0.39147325 -0.08189044 -0.73056032]\n",
      "[ 0.01066409  0.58762576 -0.09650165 -1.04785197]\n",
      "[ 0.02241661  0.78388658 -0.11745869 -1.36920096]\n",
      "[ 0.03809434  0.98026594 -0.14484271 -1.69619382]\n",
      "[ 0.05769966  1.17673159 -0.17876658 -2.03024141]\n",
      "Episode finished after 15 timesteps\n",
      "[-0.03586726 -0.03493217  0.03789007 -0.00960215]\n",
      "[-0.0365659  -0.23057645  0.03769803  0.29479066]\n",
      "[-0.04117743 -0.03601166  0.04359384  0.01423142]\n",
      "[-0.04189766 -0.23173081  0.04387847  0.32034377]\n",
      "[-0.04653228 -0.03726032  0.05028535  0.04181493]\n",
      "[-0.04727748  0.15710583  0.05112164 -0.23458811]\n",
      "[-0.04413537  0.35146151  0.04642988 -0.51071754]\n",
      "[-0.03710614  0.54589971  0.03621553 -0.78841505]\n",
      "[-0.02618814  0.74050602  0.02044723 -1.06948807]\n",
      "[ -1.13780218e-02   9.35351669e-01  -9.42531815e-04  -1.35568438e+00]\n",
      "[ 0.00732901  1.13048543 -0.02805622 -1.648662  ]\n",
      "[ 0.02993872  1.3259239  -0.06102946 -1.94995177]\n",
      "[ 0.0564572   1.1315017  -0.10002849 -1.67679293]\n",
      "[ 0.07908723  0.93767226 -0.13356435 -1.41686147]\n",
      "[ 0.09784068  0.74443304 -0.16190158 -1.16873554]\n",
      "[ 0.11272934  0.55174418 -0.18527629 -0.93087345]\n",
      "[ 0.12376422  0.74881777 -0.20389376 -1.2755847 ]\n",
      "Episode finished after 17 timesteps\n",
      "[-0.01584099  0.01683364 -0.02349683 -0.02145174]\n",
      "[-0.01550431 -0.1779436  -0.02392587  0.26372604]\n",
      "[-0.01906319 -0.37271602 -0.01865135  0.54876751]\n",
      "[-0.02651751 -0.56757107 -0.007676    0.83551606]\n",
      "[-0.03786893 -0.3723451   0.00903432  0.54042903]\n",
      "[-0.04531583 -0.1773513   0.01984291  0.25060631]\n",
      "[-0.04886286 -0.37275091  0.02485503  0.54948146]\n",
      "[-0.05631787 -0.17798675  0.03584466  0.26473208]\n",
      "[-0.05987761  0.01660574  0.0411393  -0.01643309]\n",
      "[-0.05954549 -0.17908131  0.04081064  0.28894068]\n",
      "[-0.06312712  0.01543563  0.04658945  0.0094032 ]\n",
      "[-0.06281841 -0.18032244  0.04677752  0.31641403]\n",
      "[-0.06642486  0.01410309  0.0531058   0.03884232]\n",
      "[-0.0661428  -0.18173859  0.05388265  0.34779647]\n",
      "[-0.06977757  0.01257722  0.06083858  0.07257962]\n",
      "[-0.06952602 -0.18336178  0.06229017  0.38381987]\n",
      "[-0.07319326 -0.37931025  0.06996656  0.69547401]\n",
      "[-0.08077946 -0.57532927  0.08387604  1.00933653]\n",
      "[-0.09228605 -0.3814209   0.10406278  0.74412718]\n",
      "[-0.09991447 -0.57781335  0.11894532  1.06766149]\n",
      "[-0.11147073 -0.38444847  0.14029855  0.81455107]\n",
      "[-0.1191597  -0.19149786  0.15658957  0.56908157]\n",
      "[-0.12298966  0.00112139  0.1679712   0.32953718]\n",
      "[-0.12296723  0.19350371  0.17456195  0.09417673]\n",
      "[-0.11909716  0.38574994  0.17644548 -0.1387492 ]\n",
      "[-0.11138216  0.57796355  0.1736705  -0.3709869 ]\n",
      "[-0.09982289  0.38085454  0.16625076 -0.02896817]\n",
      "[-0.0922058   0.18378703  0.16567139  0.31121027]\n",
      "[-0.08853006 -0.01325995  0.1718956   0.65121863]\n",
      "[-0.08879526  0.179104    0.18491997  0.41721401]\n",
      "[-0.08521318  0.37119003  0.19326425  0.18805444]\n",
      "[-0.07778938  0.56309743  0.19702534 -0.03798078]\n",
      "[-0.06652743  0.36577594  0.19626573  0.30982655]\n",
      "[-0.05921191  0.55763928  0.20246226  0.08488913]\n",
      "[-0.04805912  0.74937049  0.20416004 -0.13771527]\n",
      "[-0.03307171  0.55199866  0.20140573  0.21180078]\n",
      "[-0.02203174  0.74375688  0.20564175 -0.01120858]\n",
      "[-0.0071566   0.93542733  0.20541758 -0.23262309]\n",
      "[ 0.01155194  0.73805321  0.20076512  0.1171826 ]\n",
      "[ 0.02631301  0.92981731  0.20310877 -0.10605675]\n",
      "[ 0.04490936  0.73245176  0.20098763  0.24321035]\n",
      "[ 0.05955839  0.92422053  0.20585184  0.020041  ]\n",
      "[ 0.0780428   1.11588711  0.20625266 -0.20129502]\n",
      "[ 0.10036054  0.91850419  0.20222676  0.14871339]\n",
      "[ 0.11873063  1.11024271  0.20520103 -0.07398318]\n",
      "[ 0.14093548  0.91286053  0.20372136  0.27578508]\n",
      "[ 0.15919269  1.1045818   0.20923707  0.05363089]\n",
      "Episode finished after 47 timesteps\n",
      "[-0.03340122  0.0327991  -0.04956704 -0.01944725]\n",
      "[-0.03274523 -0.16157825 -0.04995598  0.25719394]\n",
      "[-0.0359768  0.03422   -0.0448121 -0.050818 ]\n",
      "[-0.0352924   0.22995491 -0.04582846 -0.35729596]\n",
      "[-0.0306933   0.03551344 -0.05297438 -0.07940884]\n",
      "[-0.02998303 -0.15881066 -0.05456256  0.19610123]\n",
      "[-0.03315925  0.03704758 -0.05064054 -0.11328236]\n",
      "[-0.03241829  0.2328572  -0.05290618 -0.42150222]\n",
      "[-0.02776115  0.42868725 -0.06133623 -0.73038345]\n",
      "[-0.01918741  0.23446424 -0.0759439  -0.45761833]\n",
      "[-0.01449812  0.43057306 -0.08509626 -0.77323972]\n",
      "[-0.00588666  0.62675625 -0.10056106 -1.0914378 ]\n",
      "[ 0.00664847  0.43309294 -0.12238981 -0.83192555]\n",
      "[ 0.01531032  0.23983709 -0.13902832 -0.58010303]\n",
      "[ 0.02010707  0.04690902 -0.15063038 -0.33424601]\n",
      "[ 0.02104525 -0.14578406 -0.15731531 -0.09259734]\n",
      "[ 0.01812957 -0.3383424  -0.15916725  0.14661328]\n",
      "[ 0.01136272 -0.14134155 -0.15623499 -0.19175057]\n",
      "[ 0.00853589  0.05563022 -0.16007    -0.52935605]\n",
      "[ 0.00964849  0.25259909 -0.17065712 -0.86789218]\n",
      "[ 0.01470047  0.06015869 -0.18801496 -0.63335485]\n",
      "[ 0.01590365  0.25733677 -0.20068206 -0.97886104]\n",
      "Episode finished after 22 timesteps\n",
      "[ 0.01582562  0.04186437 -0.01518861  0.04953181]\n",
      "[ 0.01666291  0.23720078 -0.01419798 -0.24790429]\n",
      "[ 0.02140693  0.43252259 -0.01915606 -0.54503154]\n",
      "[ 0.03005738  0.23767498 -0.03005669 -0.25844529]\n",
      "[ 0.03481088  0.43321285 -0.0352256  -0.56045495]\n",
      "[ 0.04347513  0.23860253 -0.0464347  -0.27907476]\n",
      "[ 0.04824718  0.04417269 -0.05201619 -0.00139125]\n",
      "[ 0.04913064  0.24000055 -0.05204402 -0.31002161]\n",
      "[ 0.05393065  0.04565725 -0.05824445 -0.03419539]\n",
      "[ 0.05484379  0.24156398 -0.05892836 -0.34467137]\n",
      "[ 0.05967507  0.43747255 -0.06582178 -0.65533906]\n",
      "[ 0.06842453  0.24332575 -0.07892857 -0.38408713]\n",
      "[ 0.07329104  0.43947439 -0.08661031 -0.70057501]\n",
      "[ 0.08208053  0.63568341 -0.10062181 -1.01921696]\n",
      "[ 0.0947942   0.44203597 -0.12100615 -0.75974796]\n",
      "[ 0.10363492  0.63859889 -0.13620111 -1.08792539]\n",
      "[ 0.11640689  0.83522805 -0.15795962 -1.42005709]\n",
      "[ 0.13311145  1.0319123  -0.18636076 -1.75765771]\n",
      "Episode finished after 18 timesteps\n",
      "[-0.04376026 -0.03781762  0.04050575 -0.01515279]\n",
      "[-0.04451661 -0.23349636  0.0402027   0.29002999]\n",
      "[-0.04918654 -0.03897005  0.0460033   0.01029271]\n",
      "[-0.04996594 -0.23472053  0.04620915  0.31712798]\n",
      "[-0.05466035 -0.04028616  0.05255171  0.03936842]\n",
      "[-0.05546607 -0.23612078  0.05333908  0.3481579 ]\n",
      "[-0.06018849 -0.43195921  0.06030224  0.65717202]\n",
      "[-0.06882767 -0.23772621  0.07344568  0.3840702 ]\n",
      "[-0.07358219 -0.04371967  0.08112708  0.11541855]\n",
      "[-0.07445659  0.15015179  0.08343545 -0.15060697]\n",
      "[-0.07145355 -0.04605964  0.08042331  0.16718791]\n",
      "[-0.07237475 -0.24223518  0.08376707  0.48411907]\n",
      "[-0.07721945 -0.04838912  0.09344945  0.21896827]\n",
      "[-0.07818723 -0.24471404  0.09782882  0.5396049 ]\n",
      "[-0.08308151 -0.44106525  0.10862092  0.86143928]\n",
      "[-0.09190282 -0.24757676  0.1258497   0.60478881]\n",
      "[-0.09685435 -0.44421311  0.13794548  0.93431287]\n",
      "[-0.10573861 -0.64089917  0.15663174  1.26696682]\n",
      "[-0.1185566  -0.83763631  0.18197107  1.6043216 ]\n",
      "Episode finished after 19 timesteps\n",
      "[-0.03240064 -0.03350129 -0.04218623  0.02111532]\n",
      "[-0.03307066 -0.22799365 -0.04176392  0.30019523]\n",
      "[-0.03763054 -0.4224962  -0.03576002  0.57941964]\n",
      "[-0.04608046 -0.22689184 -0.02417162  0.27568947]\n",
      "[-0.0506183  -0.03143352 -0.01865783 -0.02451814]\n",
      "[-0.05124697 -0.226283   -0.0191482   0.26222013]\n",
      "[-0.05577263 -0.42112646 -0.01390379  0.54880252]\n",
      "[-0.06419516 -0.22581198 -0.00292774  0.25177154]\n",
      "[-0.0687114  -0.03064835  0.00210769 -0.04183342]\n",
      "[-0.06932436 -0.22580046  0.00127102  0.25151376]\n",
      "[-0.07384037 -0.03069668  0.00630129 -0.04076799]\n",
      "[-0.07445431  0.16433435  0.00548594 -0.33145616]\n",
      "[-0.07116762 -0.03086526 -0.00114319 -0.03704829]\n",
      "[-0.07178493  0.16427307 -0.00188415 -0.33009169]\n",
      "[-0.06849946  0.35942179 -0.00848599 -0.62336819]\n",
      "[-0.06131103  0.16441934 -0.02095335 -0.33336991]\n",
      "[-0.05802264 -0.03039821 -0.02762075 -0.04736756]\n",
      "[-0.05863061 -0.22511343 -0.0285681   0.2364744 ]\n",
      "[-0.06313287 -0.41981583 -0.02383861  0.52001095]\n",
      "[-0.07152919 -0.61459422 -0.01343839  0.80508764]\n",
      "[-0.08382108 -0.41929065  0.00266336  0.50820796]\n",
      "[-0.09220689 -0.61445002  0.01282752  0.80172901]\n",
      "[-0.10449589 -0.80974553  0.0288621   1.09841935]\n",
      "[-0.1206908  -0.61501517  0.05083049  0.81492974]\n",
      "[-0.1329911  -0.42062474  0.06712908  0.53865844]\n",
      "[-0.1414036  -0.61662302  0.07790225  0.85171481]\n",
      "[-0.15373606 -0.81271571  0.09493655  1.16784187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16999037 -1.00893587  0.11829338  1.48871616]\n",
      "[-0.19016909 -0.81543666  0.14806771  1.23519295]\n",
      "[-0.20647782 -1.01211841  0.17277156  1.57036044]\n",
      "[-0.22672019 -1.20883015  0.20417877  1.91157762]\n",
      "Episode finished after 31 timesteps\n",
      "[-0.0252764  -0.03116018  0.00927371  0.04671561]\n",
      "[-0.0258996   0.16382757  0.01020802 -0.24302701]\n",
      "[-0.02262305 -0.03143869  0.00534748  0.05285823]\n",
      "[-0.02325182 -0.2266369   0.00640465  0.34722352]\n",
      "[-0.02778456 -0.42184937  0.01334912  0.64191916]\n",
      "[-0.03622155 -0.61715483  0.0261875   0.93877577]\n",
      "[-0.04856465 -0.42239553  0.04496302  0.65443524]\n",
      "[-0.05701256 -0.22792751  0.05805172  0.37624269]\n",
      "[-0.06157111 -0.03367607  0.06557657  0.10241399]\n",
      "[-0.06224463 -0.22967354  0.06762485  0.41504398]\n",
      "[-0.0668381  -0.03557197  0.07592573  0.14442366]\n",
      "[-0.06754954 -0.23169451  0.07881421  0.46006067]\n",
      "[-0.07218343 -0.42783688  0.08801542  0.77650831]\n",
      "[-0.08074017 -0.62405195  0.10354559  1.09553477]\n",
      "[-0.09322121 -0.82037381  0.12545628  1.41882832]\n",
      "[-0.10962868 -1.01680538  0.15383285  1.74794742]\n",
      "[-0.12996479 -1.21330457  0.1887918   2.08426422]\n",
      "Episode finished after 17 timesteps\n",
      "[ 0.04664893 -0.037138   -0.00980353 -0.0432486 ]\n",
      "[ 0.04590617 -0.23211801 -0.0106685   0.24632517]\n",
      "[ 0.04126381 -0.42708598 -0.005742    0.535624  ]\n",
      "[ 0.03272209 -0.23188376  0.00497048  0.24113736]\n",
      "[ 0.02808441 -0.42707636  0.00979323  0.53538396]\n",
      "[ 0.01954288 -0.23209348  0.02050091  0.24580283]\n",
      "[ 0.01490101 -0.03727024  0.02541697 -0.04034377]\n",
      "[ 0.01415561 -0.23274727  0.02461009  0.26024885]\n",
      "[ 0.00950066 -0.03798513  0.02981507 -0.02457127]\n",
      "[ 0.00874096  0.15669684  0.02932364 -0.30770011]\n",
      "[ 0.0118749   0.35138895  0.02316964 -0.59099281]\n",
      "[ 0.01890268  0.54617898  0.01134978 -0.87628816]\n",
      "[ 0.02982626  0.35090461 -0.00617598 -0.58005871]\n",
      "[ 0.03684435  0.15586975 -0.01777715 -0.28932772]\n",
      "[ 0.03996174 -0.03899425 -0.02356371 -0.00230411]\n",
      "[ 0.03918186 -0.23377048 -0.02360979  0.28285203]\n",
      "[ 0.03450645 -0.42854785 -0.01795275  0.56799604]\n",
      "[ 0.02593549 -0.62341345 -0.00659283  0.8549695 ]\n",
      "[ 0.01346722 -0.42820227  0.01050656  0.56022082]\n",
      "[ 0.00490318 -0.6234701   0.02171098  0.85619526]\n",
      "[-0.00756622 -0.81888105  0.03883488  1.15562514]\n",
      "[-0.02394385 -0.62428637  0.06194739  0.87536778]\n",
      "[-0.03642957 -0.82019324  0.07945474  1.18686532]\n",
      "[-0.05283344 -0.62618635  0.10319205  0.9201082 ]\n",
      "[-0.06535716 -0.43259906  0.12159421  0.66155662]\n",
      "[-0.07400915 -0.23936005  0.13482534  0.40949893]\n",
      "[-0.07879635 -0.43611014  0.14301532  0.74146443]\n",
      "[-0.08751855 -0.63288635  0.15784461  1.07551865]\n",
      "[-0.10017628 -0.82970139  0.17935498  1.41328496]\n",
      "[-0.1167703  -1.02653476  0.20762068  1.75624701]\n",
      "Episode finished after 30 timesteps\n",
      "[-0.0463431   0.04307761  0.01202726 -0.03967819]\n",
      "[-0.04548155 -0.15221473  0.0112337   0.25677506]\n",
      "[-0.04852584 -0.34749525  0.0163692   0.55298   ]\n",
      "[-0.05547575 -0.5428432   0.0274288   0.850775  ]\n",
      "[-0.06633261 -0.34810577  0.0444443   0.56684179]\n",
      "[-0.07329473 -0.15363454  0.05578113  0.28848532]\n",
      "[-0.07636742  0.04064943  0.06155084  0.01390379]\n",
      "[-0.07555443 -0.15529877  0.06182892  0.32535401]\n",
      "[-0.07866041  0.03889086  0.068336    0.05279228]\n",
      "[-0.07788259  0.23296979  0.06939184 -0.2175715 ]\n",
      "[-0.07322319  0.03692802  0.06504041  0.09616829]\n",
      "[-0.07248463  0.2310604   0.06696378 -0.17530618]\n",
      "[-0.06786342  0.42516328  0.06345765 -0.44613531]\n",
      "[-0.05936016  0.61933277  0.05453495 -0.71815929]\n",
      "[-0.0469735   0.42350026  0.04017176 -0.4088219 ]\n",
      "[-0.0385035   0.22783245  0.03199532 -0.10374965]\n",
      "[-0.03394685  0.42248161  0.02992033 -0.38616895]\n",
      "[-0.02549722  0.22694798  0.02219695 -0.0842045 ]\n",
      "[-0.02095826  0.42174483  0.02051286 -0.36980243]\n",
      "[-0.01252336  0.61656941  0.01311681 -0.65594747]\n",
      "[ -1.91971940e-04   4.21267335e-01  -2.13618491e-06  -3.59163317e-01]\n",
      "[ 0.00823337  0.22614541 -0.0071854  -0.06648106]\n",
      "[ 0.01275628  0.42136964 -0.00851502 -0.36142234]\n",
      "[ 0.02118368  0.22636975 -0.01574347 -0.07143649]\n",
      "[ 0.02571107  0.031477   -0.0171722   0.21623801]\n",
      "[ 0.02634061  0.22684018 -0.01284744 -0.08181193]\n",
      "[ 0.03087741  0.42214392 -0.01448368 -0.37852041]\n",
      "[ 0.03932029  0.22723063 -0.02205409 -0.09043918]\n",
      "[ 0.04386491  0.42266163 -0.02386287 -0.38999778]\n",
      "[ 0.05231814  0.618114   -0.03166283 -0.69010788]\n",
      "[ 0.06468042  0.4234454  -0.04546498 -0.40755866]\n",
      "[ 0.07314933  0.61918154 -0.05361616 -0.71422157]\n",
      "[ 0.08553296  0.81500313 -0.06790059 -1.02328746]\n",
      "[ 0.10183302  0.62084807 -0.08836634 -0.75267341]\n",
      "[ 0.11424998  0.4270485  -0.10341981 -0.4890542 ]\n",
      "[ 0.12279095  0.62346583 -0.11320089 -0.81245724]\n",
      "[ 0.13526027  0.43006134 -0.12945003 -0.55741732]\n",
      "[ 0.14386149  0.23697152 -0.14059838 -0.30815807]\n",
      "[ 0.14860093  0.04410365 -0.14676154 -0.06291086]\n",
      "[ 0.149483   -0.14864259 -0.14801976  0.18010725]\n",
      "[ 0.14651015 -0.34137054 -0.14441761  0.4226789 ]\n",
      "[ 0.13968274 -0.53418288 -0.13596404  0.66657528]\n",
      "[ 0.12899908 -0.33745817 -0.12263253  0.33436115]\n",
      "[ 0.12224991 -0.14082371 -0.11594531  0.00566086]\n",
      "[ 0.11943344 -0.33410842 -0.11583209  0.25962973]\n",
      "[ 0.11275127 -0.52740268 -0.1106395   0.51364967]\n",
      "[ 0.10220322 -0.33091061 -0.1003665   0.188252  ]\n",
      "[ 0.09558501 -0.52446408 -0.09660146  0.44766289]\n",
      "[ 0.08509572 -0.32811784 -0.0876482   0.12615998]\n",
      "[ 0.07853337 -0.52188193 -0.08512501  0.38995484]\n",
      "[ 0.06809573 -0.32566146 -0.07732591  0.07169335]\n",
      "[ 0.0615825  -0.51959456 -0.07589204  0.33901259]\n",
      "[ 0.05119061 -0.7135592  -0.06911179  0.60683123]\n",
      "[ 0.03691942 -0.90765023 -0.05697517  0.87697005]\n",
      "[ 0.01876642 -0.71180213 -0.03943576  0.56693295]\n",
      "[ 0.00453038 -0.90634934 -0.02809711  0.84693576]\n",
      "[-0.01359661 -0.71085559 -0.01115839  0.54555143]\n",
      "[ -2.78137212e-02  -9.05818994e-01  -2.47361273e-04   8.34697826e-01]\n",
      "[-0.0459301  -1.10093756  0.0164466   1.12730295]\n",
      "[-0.06794885 -1.29627109  0.03899265  1.42509869]\n",
      "[-0.09387427 -1.4918527   0.06749463  1.72970904]\n",
      "[-0.12371133 -1.29756358  0.10208881  1.45876729]\n",
      "[-0.1496626  -1.49377879  0.13126415  1.7815197 ]\n",
      "[-0.17953818 -1.69011033  0.16689455  2.11196444]\n",
      "[-0.21334038 -1.88646222  0.20913384  2.45123947]\n",
      "Episode finished after 65 timesteps\n",
      "[-0.02025995  0.03499469 -0.02044043 -0.02750195]\n",
      "[-0.01956005  0.23040372 -0.02099047 -0.32656333]\n",
      "[-0.01495198  0.03558681 -0.02752173 -0.04057308]\n",
      "[-0.01424024  0.23109239 -0.02833319 -0.34181076]\n",
      "[-0.0096184   0.03638476 -0.03516941 -0.05819539]\n",
      "[-0.0088907  -0.15821573 -0.03633332  0.22318718]\n",
      "[-0.01205501 -0.35280005 -0.03186957  0.50419137]\n",
      "[-0.01911102 -0.54745869 -0.02178575  0.786663  ]\n",
      "[-0.03006019 -0.35204434 -0.00605249  0.48720652]\n",
      "[-0.03710108 -0.54708037  0.00369164  0.77797579]\n",
      "[-0.04804268 -0.35200938  0.01925116  0.48645664]\n",
      "[-0.05508287 -0.15716429  0.02898029  0.19990265]\n",
      "[-0.05822616  0.03753145  0.03297835 -0.08349931]\n",
      "[-0.05747553 -0.15804734  0.03130836  0.21940323]\n",
      "[-0.06063647  0.03661341  0.03569642 -0.06324165]\n",
      "[-0.05990421  0.23120586  0.03443159 -0.34445186]\n",
      "[-0.05528009  0.03561145  0.02754255 -0.04111324]\n",
      "[-0.05456786  0.23032784  0.02672029 -0.32498056]\n",
      "[-0.0499613   0.03483582  0.02022068 -0.02399239]\n",
      "[-0.04926459 -0.16057019  0.01974083  0.27500118]\n",
      "[-0.05247599  0.03426463  0.02524085 -0.01139067]\n",
      "[-0.0517907  -0.16121005  0.02501304  0.28914797]\n",
      "[-0.0550149   0.03354646  0.030796    0.00445766]\n",
      "[-0.05434397  0.22821352  0.03088515 -0.27835189]\n",
      "[-0.0497797   0.42288157  0.02531811 -0.56113591]\n",
      "[-0.04132207  0.61763922  0.0140954  -0.84573603]\n",
      "[-0.02896928  0.81256605 -0.00281932 -1.13395332]\n",
      "[-0.01271796  1.00772479 -0.02549839 -1.42751914]\n",
      "[ 0.00743653  0.81292694 -0.05404877 -1.14291307]\n",
      "[ 0.02369507  0.61855128 -0.07690703 -0.86765795]\n",
      "[ 0.0360661   0.42455531 -0.09426019 -0.60011252]\n",
      "[ 0.0445572   0.23086967 -0.10626244 -0.33854525]\n",
      "[ 0.0491746   0.42733056 -0.11303335 -0.66275677]\n",
      "[ 0.05772121  0.23394759 -0.12628848 -0.40769449]\n",
      "[ 0.06240016  0.43061279 -0.13444237 -0.7373732 ]\n",
      "[ 0.07101242  0.62731001 -0.14918984 -1.0691637 ]\n",
      "[ 0.08355862  0.82405614 -0.17057311 -1.40470394]\n",
      "[ 0.10003974  0.63141267 -0.19866719 -1.16983797]\n",
      "Episode finished after 38 timesteps\n",
      "[-0.00174457 -0.04664721  0.0166203   0.0448555 ]\n",
      "[-0.00267752  0.14823252  0.01751741 -0.24253757]\n",
      "[ 0.00028713 -0.04713521  0.01266666  0.05561892]\n",
      "[-0.00065557 -0.24243647  0.01377904  0.35227121]\n",
      "[-0.0055043  -0.43775163  0.02082446  0.64926704]\n",
      "[-0.01425933 -0.63315739  0.0338098   0.94843407]\n",
      "[-0.02692248 -0.43850656  0.05277848  0.66656287]\n",
      "[-0.03569261 -0.24415683  0.06610974  0.39095389]\n",
      "[-0.04057575 -0.44015169  0.07392882  0.70372608]\n",
      "[-0.04937878 -0.63621608  0.08800334  1.01873462]\n",
      "[-0.0621031  -0.4423703   0.10837803  0.75493076]\n",
      "[-0.07095051 -0.6388061   0.12347665  1.07965648]\n",
      "[-0.08372663 -0.83532275  0.14506978  1.40839714]\n",
      "[-0.10043309 -0.6422677   0.17323772  1.16435745]\n",
      "[-0.11327844 -0.44977133  0.19652487  0.93061239]\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.04897481 -0.01546329 -0.01718091 -0.00196353]\n",
      "[ 0.04866554 -0.21033468 -0.01722018  0.28524948]\n",
      "[ 0.04445885 -0.01497142 -0.01151519 -0.01281444]\n",
      "[ 0.04415942 -0.20992635 -0.01177148  0.27621317]\n",
      "[ 0.03996089 -0.01463845 -0.00624722 -0.02015912]\n",
      "[ 0.03966812  0.18057253 -0.0066504  -0.31480655]\n",
      "[ 0.04327957 -0.01445406 -0.01294653 -0.02422834]\n",
      "[ 0.04299049  0.18085114 -0.0134311  -0.32096776]\n",
      "[ 0.04660751  0.37616176 -0.01985046 -0.6178559 ]\n",
      "[ 0.05413075  0.18132265 -0.03220757 -0.33149045]\n",
      "[ 0.0577572   0.37688789 -0.03883738 -0.63415357]\n",
      "[ 0.06529496  0.18232859 -0.05152045 -0.35395001]\n",
      "[ 0.06894153 -0.01202437 -0.05859945 -0.07794771]\n",
      "[ 0.06870104  0.18388655 -0.06015841 -0.38852806]\n",
      "[ 0.07237878 -0.01033221 -0.06792897 -0.11540283]\n",
      "[ 0.07217213  0.18569398 -0.07023703 -0.42871974]\n",
      "[ 0.07588601 -0.00836662 -0.07881142 -0.15897926]\n",
      "[ 0.07571868  0.18779002 -0.08199101 -0.47544749]\n",
      "[ 0.07947448  0.38396823 -0.09149996 -0.79280461]\n",
      "[ 0.08715384  0.19021355 -0.10735605 -0.53025182]\n",
      "[ 0.09095811 -0.00324743 -0.11796108 -0.2732331 ]\n",
      "[ 0.09089317 -0.19650602 -0.12342575 -0.01995995]\n",
      "[ 0.08696305 -0.38966162 -0.12382495  0.23137426]\n",
      "[ 0.07916981 -0.19300788 -0.11919746 -0.0976594 ]\n",
      "[ 0.07530966  0.00360285 -0.12115065 -0.42544401]\n",
      "[ 0.07538171 -0.18961342 -0.12965953 -0.17327517]\n",
      "[ 0.07158944  0.00710285 -0.13312503 -0.50388589]\n",
      "[ 0.0717315   0.20382505 -0.14320275 -0.83538155]\n",
      "[ 0.075808    0.40058239 -0.15991038 -1.1694544 ]\n",
      "[ 0.08381965  0.20785997 -0.18329947 -0.93087257]\n",
      "[ 0.08797685  0.40491907 -0.20191692 -1.27509818]\n",
      "Episode finished after 31 timesteps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02087857 -0.01931018 -0.04514107 -0.03286098]\n",
      "[ 0.02049237  0.17642907 -0.04579829 -0.33943773]\n",
      "[ 0.02402095 -0.01801231 -0.05258705 -0.06154147]\n",
      "[ 0.0236607  -0.21234238 -0.05381788  0.21409721]\n",
      "[ 0.01941386 -0.40665526 -0.04953593  0.48932971]\n",
      "[ 0.01128075 -0.60104465 -0.03974934  0.76599851]\n",
      "[-0.00074014 -0.40539859 -0.02442937  0.46107801]\n",
      "[-0.00884811 -0.60016689 -0.01520781  0.74596176]\n",
      "[ -2.08514527e-02  -4.04838434e-01  -2.88573231e-04   4.48532034e-01]\n",
      "[-0.02894822 -0.2097124   0.00868207  0.15575816]\n",
      "[-0.03314247 -0.40495758  0.01179723  0.45116738]\n",
      "[-0.04124162 -0.21000444  0.02082058  0.16222633]\n",
      "[-0.04544171 -0.40541817  0.0240651   0.4614042 ]\n",
      "[-0.05355007 -0.60087184  0.03329319  0.76157427]\n",
      "[-0.06556751 -0.79643622  0.04852467  1.06454467]\n",
      "[-0.08149623 -0.60198894  0.06981557  0.78747773]\n",
      "[-0.09353601 -0.40789191  0.08556512  0.51755055]\n",
      "[-0.10169385 -0.21407235  0.09591613  0.25301029]\n",
      "[-0.1059753  -0.02044152  0.10097634 -0.00794481]\n",
      "[-0.10638413 -0.21685575  0.10081744  0.31481241]\n",
      "[-0.11072124 -0.02330362  0.10711369  0.05554858]\n",
      "[-0.11118732  0.17013244  0.10822466 -0.20151133]\n",
      "[-0.10778467 -0.0263575   0.10419444  0.12325519]\n",
      "[-0.10831182 -0.22280593  0.10665954  0.44690749]\n",
      "[-0.11276794 -0.02934175  0.11559769  0.1896588 ]\n",
      "[-0.11335477 -0.22591142  0.11939087  0.51645554]\n",
      "[-0.117873   -0.42249433  0.12971998  0.84424839]\n",
      "[-0.12632289 -0.22935847  0.14660494  0.59500945]\n",
      "[-0.13091006 -0.42619515  0.15850513  0.93004481]\n",
      "[-0.13943396 -0.62306033  0.17710603  1.26804645]\n",
      "[-0.15189516 -0.43058633  0.20246696  1.03564895]\n",
      "Episode finished after 31 timesteps\n",
      "[-0.03718942  0.03630058 -0.04464811  0.03898934]\n",
      "[-0.03646341  0.23203341 -0.04386832 -0.26743948]\n",
      "[-0.03182274  0.42775308 -0.04921711 -0.57362971]\n",
      "[-0.02326768  0.62352928 -0.0606897  -0.88140238]\n",
      "[-0.01079709  0.81942077 -0.07831775 -1.19253048]\n",
      "[ 0.00559132  1.01546497 -0.10216836 -1.50869727]\n",
      "[ 0.02590062  1.21166617 -0.13234231 -1.83144966]\n",
      "[ 0.05013395  1.40798208 -0.1689713  -2.16214367]\n",
      "Episode finished after 8 timesteps\n",
      "[-0.00648674  0.01738422  0.01331587  0.03540043]\n",
      "[-0.00613905  0.21231271  0.01402388 -0.25305164]\n",
      "[-0.0018928   0.40723163  0.00896285 -0.54127839]\n",
      "[ 0.00625184  0.21198486 -0.00186272 -0.24578495]\n",
      "[ 0.01049153  0.01688956 -0.00677842  0.04630985]\n",
      "[ 0.01082932  0.21210805 -0.00585222 -0.248504  ]\n",
      "[ 0.01507148  0.40731309 -0.0108223  -0.54302708]\n",
      "[ 0.02321775  0.21234489 -0.02168284 -0.2537736 ]\n",
      "[ 0.02746464  0.40776962 -0.02675831 -0.55321605]\n",
      "[ 0.03562004  0.60325692 -0.03782264 -0.85420803]\n",
      "[ 0.04768518  0.7988734  -0.0549068  -1.1585401 ]\n",
      "[ 0.06366264  0.9946662  -0.0780776  -1.4679212 ]\n",
      "[ 0.08355597  0.80058188 -0.10743602 -1.20061313]\n",
      "[ 0.0995676   0.99691681 -0.13144828 -1.52494298]\n",
      "[ 0.11950594  1.19335799 -0.16194714 -1.85559733]\n",
      "[ 0.1433731   1.00034373 -0.19905909 -1.61726888]\n",
      "Episode finished after 16 timesteps\n",
      "[ 0.02147086  0.01752925 -0.01294498  0.03979996]\n",
      "[ 0.02182145 -0.17740471 -0.01214898  0.32837067]\n",
      "[ 0.01827335 -0.37235161 -0.00558157  0.61719774]\n",
      "[ 0.01082632 -0.56739515  0.00676238  0.90811751]\n",
      "[ -5.21584295e-04  -7.62607985e-01   2.49247337e-02   1.20291820e+00]\n",
      "[-0.01577374 -0.95804318  0.0489831   1.50330705]\n",
      "[-0.03493461 -0.76354876  0.07904924  1.22631061]\n",
      "[-0.05020558 -0.95959437  0.10357545  1.542677  ]\n",
      "[-0.06939747 -1.15579762  0.13442899  1.86580143]\n",
      "[-0.09251342 -1.35211113  0.17174502  2.19701818]\n",
      "Episode finished after 10 timesteps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# first lets copy in the cartpole code from OpenAi's example \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        \n",
    "        # so we need to implement our tabular Q agents activities around here\n",
    "        \n",
    "        #action = 0 #applying a force to the left\n",
    "        action = env.action_space.sample() # sampling the \"action\" array which in this case only contains two elements\n",
    "        \n",
    "        # action contains 0 (apply force to the left) and 1 (apply force from the right)\n",
    "        # by sampling the action array above we are choosing one of these two possible actions at random\n",
    "        \n",
    "        # and here\n",
    "        \n",
    "        \n",
    "        observation, reward, done, info = env.step(action) \n",
    "        \n",
    "        # here we taking the next \"step\" in our environment by taking in our action variable randomly selected above\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to understand what the observation variables are (are they velocity, acceleration, angle from the centre line? etc etc) and also which variable is which."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found out what these were doing a quick google search, the answer is located here:\n",
    "\n",
    "https://github.com/openai/gym/issues/238\n",
    "\n",
    "turns out to be:\n",
    "\n",
    "[position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what I learned from my conversation with Will, is first we need to discretize the outputs (put then in \"buckets\" from our cartpole, I am going to just copy his method to discretize, line by line (so that I know what is going on) to do this\n",
    "\n",
    "we need to do this because we need to convert \"the continuous, 4-dimensional input space to a discrete space with a finite and preferably small, yet expressive, number of discrete states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "[  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "[ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n"
     ]
    }
   ],
   "source": [
    "#so observation space high - prints out the highest value observed in the observation space over one training episode\n",
    "# observation space low - prints out the lowest value observed in the oberservation.....\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  6  1 11  8]\n"
     ]
    }
   ],
   "source": [
    "   #    self.bins = []\n",
    "#    self.bins.append(np.linspace(-2.4, 2.4, 5))\n",
    " #       self.bins.append(np.linspace(-0.5, 0.5, 5))\n",
    "        self.bins.append(np.linspace(-41.8, 41.8, 5))\n",
    "        self.bins.append(np.linspace(-math.radians(50), math.radians(50), 5))\n",
    "\n",
    "\n",
    "\n",
    "#next Will created a numpy array with values...\n",
    "\n",
    "#x is just an example of some random input for testing purposes....\n",
    "\n",
    "x = np.array([-2.4, 0, -4.0, 4, 2])\n",
    "\n",
    "# next he created the bins using the max and the minimum values from our printing of the observation space above...\n",
    "# with 12 'bins' for discretization purposes\n",
    "bins = np.linspace(-4.8, 4.8, 12) \n",
    "\n",
    "#the next line of code implements discretization on our test variables x\n",
    "out = np.digitize(x, bins)\n",
    "\n",
    "print out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is important to note that the output of the above is the POSITION of the discretized version of X in the new bin array!!! aka the index."
   ]
  },
  {
   "attachments": {
    "cartpole3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok time to implement our Q learner, I am going to do this using these two websites as referances:\n",
    "\n",
    "http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    "\n",
    "https://towardsdatascience.com/introduction-to-q-learning-88d1c4f2b49c\n",
    " \n",
    "and this\n",
    "\n",
    "https://dev.to/n1try/cartpole-with-q-learning---first-experiences-with-openai-gym\n",
    "\n",
    "and Will's workbook:\n",
    "\n",
    "https://github.com/whathelll/DeepRLBootCampLabs/blob/master/pytorch/4-CartPoleQLearning.ipynb\n",
    "\n",
    "I also would like to implement the Q agent uses Python Classes, to refresh my memory on how to write them\n",
    "\n",
    "finally the most important thing in all of the formula to update the Q table. It goes something like this:\n",
    "\n",
    "Q = Q(s,a) + alpha * (R + discount * max(Q(s',a')) - Q)\n",
    "\n",
    "or\n",
    "\n",
    "Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "\n",
    "or graphically:\n",
    "\n",
    "![cartpole3.png](attachment:cartpole3.png)\n",
    "\n",
    "https://res.cloudinary.com/practicaldev/image/fetch/s--k7Z0JGcN--/c_limit,f_auto,fl_progressive,q_auto,w_880/https://ferdinand-muetsch.de/images/cartpole3.png\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q learning algorithm:\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "    1. Initialise Q-matrix by all zeros. Set value for . Fill rewards matrix.\n",
    "    2. For each episode. Select a random starting state (here we will restrict our starting state to state-1).\n",
    "    3. Select one among all possible actions for the current state (S).\n",
    "    4. Travel to the next state (S) as a result of that action (a).\n",
    "    5. for all possible actions from the state (S) select the one with the highest Q value.\n",
    "    6. Update Q-table using eqn.1 .\n",
    "    7. Set the next state as the current state.\n",
    "    8. If goal state reached then end.\n",
    "    \n",
    "as implemented by the other dude:\n",
    "\n",
    "The Q-Learning algorithm goes as follows:\n",
    "\n",
    "    1. Set the gamma parameter, and environment rewards in matrix R.\n",
    "\n",
    "    2. Initialize matrix Q to zero.\n",
    "\n",
    "    3. For each episode:\n",
    "\n",
    "        Select a random initial state.\n",
    "\n",
    "        Do While the goal state hasn't been reached.\n",
    "\n",
    "            Select one among all possible actions for the current state.\n",
    "            Using this possible action, consider going to the next state.\n",
    "            Get maximum Q value for this next state based on all possible actions.\n",
    "            Compute: Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "            Set the next state as the current state.\n",
    "\n",
    "        End Do\n",
    "\n",
    "    End For\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "\n",
    "- the Q table is only discovering 6 states every time, there should be 15 (3x5) and therefore 30 Q values (3x5x2)\n",
    "- the Q values are converging to 1 (they should be -43 , 90 etc: (from Wills example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-01 11:59:41,414] Making new env: CartPole-v0\n",
      "[2018-04-01 11:59:41,429] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensionality of action space is is :  2\n",
      "dimensionality of observation space is is :  4\n",
      "completed training\n",
      "((0, 0, 2, 1), [31.273250494460235, 45.668050325034336])\n",
      "((0, 0, 1, 3), [14.867160291292862, 36.510927476833814])\n",
      "((0, 0, 2, 0), [44.850440062213465, 26.80702707550874])\n",
      "((0, 0, 1, 4), [-4.310321355654857, 3.8826045576571273])\n",
      "((0, 0, 1, 0), [46.76670001047435, -5.41894467186013])\n",
      "((0, 0, 2, 3), [-26.22589801820334, 45.92458975964449])\n",
      "((0, 0, 1, 1), [47.012080651374504, 30.968955497307615])\n",
      "((0, 0, 2, 2), [31.551300614783564, 45.658965492428955])\n",
      "((0, 0, 1, 2), [46.85427001214968, 41.78378591120261])\n",
      "((0, 0, 2, 4), [-51.33377283345862, -39.235903513696464])\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# implementation using a python dictionary as the Q table\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "class QTabular():\n",
    "    \n",
    "    #, buckets = (1,1,6,12,)\n",
    "    def __init__(self, alpha = 0.1, gamma = 0.99, epsilon = 0.01,numberOfEpisodes = 2):\n",
    "        \n",
    "        self.alpha = alpha # learning rate, 0.1 is the optimal paramters taken from Ferdinands Tabular Grid Search\n",
    "        self.gamma = gamma # discount factor, used to control the \"weight\" that is given to future reward\n",
    "        self.epsilon = epsilon # how much we want to explore...see multi armed bandit workbookd\n",
    "        #self.buckets = buckets # downscaling the feature space from continous to these 'bucket' sizes\n",
    "                               # tuned parameters borrowed from Ferdinand \n",
    "        \n",
    "            \n",
    "            \n",
    "        self.numberOfEpisodes = numberOfEpisodes\n",
    "            \n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        self.S_initial = self.env.reset() # intitial observation from openAi gym\n",
    "        \n",
    "        \n",
    "        self.buckets = []\n",
    "        \n",
    "        #this is bad, should write code to automatically detect thesem but, uh, I am lazy\n",
    "        # parameter numbers  from Will's notebook referanced above\n",
    "        self.buckets.append(np.linspace(-2.4, 2.4, 1))\n",
    "        self.buckets.append(np.linspace(-0.5, 0.5, 1))\n",
    "        self.buckets.append(np.linspace(-41.8, 41.8, 5))\n",
    "        self.buckets.append(np.linspace(-math.radians(50), math.radians(50), 5))\n",
    "        \n",
    "        \n",
    "        #using Wills idea of a python dictionary for the Q table\n",
    "        self.Q = {} \n",
    "        \n",
    "        #measuring the dimensionality (length) of the observation space\n",
    "        self.observationSpaceLength = env.observation_space.shape\n",
    "        self.obs_length = self.observationSpaceLength[0]\n",
    "        \n",
    "        #measuring the dimensionality of the action space\n",
    "        self.n = self.env.action_space.n\n",
    "        \n",
    "        print \"dimensionality of action space is is : \", self.n\n",
    "        print \"dimensionality of observation space is is : \", self.obs_length\n",
    "        \n",
    "                   \n",
    "    \n",
    "    #this function allocated the raw observation data into its right \"bucket\" allocated above\n",
    "    def quantizeState(self, state):\n",
    "        \n",
    "        quantizedState_index = []\n",
    "        \n",
    "        for i in range(4):\n",
    "            \n",
    "            x =  np.digitize(state[i], self.buckets[i])\n",
    "            #digitize notation starts at 1 therefore we have to subtract 1 to get it inline\n",
    "            #with our array notation... (this took 4 hours to figure out)\n",
    "            x = x-1\n",
    "            \n",
    "\n",
    "            if x < 0:\n",
    "                x = 0\n",
    "            \n",
    "            quantizedState_index.append(x)                       \n",
    "    \n",
    "        #returns a pointer that can be used to retrieve the specific \"bucket\" in questions value\n",
    "        return tuple(quantizedState_index)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def resetState(self):\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def getEnvironmentVariables(self,action):\n",
    "        \n",
    "        observation, reward, done, _ =  self.env.step(action)\n",
    "        \n",
    "        return(observation, reward, done)\n",
    "    \n",
    "    \n",
    "    def randomAction(self):\n",
    "                          \n",
    "        action = self.env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "                          \n",
    "    \n",
    "\n",
    "                          \n",
    "            \n",
    "    def getQvalues(self, state, action=None):\n",
    "        \n",
    "        #if the state key 'state' is not in the current table, set its Q values to 0.\n",
    "        if not state in self.Q:\n",
    "            self.Q[state] = [0, 0]\n",
    "        \n",
    "        #if the state already has a Q value, return it, otherwise return the zeroed version of it\n",
    "        if action is not None:\n",
    "            return self.Q[state][action] #returns a single Q value given an action\n",
    "        else:\n",
    "            return self.Q[state] #returns array of q values , [0, 0] , given above\n",
    "                                  \n",
    "    \n",
    "    def getMaxQ(self, S_dash):\n",
    "        \n",
    "        tempQvaluesArray = self.getQvalues(S_dash)\n",
    "\n",
    "        #argmax the \"index\" of the maximum Q value (0 or 1 as we only have two possible actions)\n",
    "        return np.argmax(tempQvaluesArray)\n",
    "    \n",
    "    \n",
    "    def getAction(self, state, episodeCounter):\n",
    "    \n",
    "        ZeroToOne = 0.0\n",
    "        \n",
    "        if episodeCounter > 100:\n",
    "            ZeroToOne = 1.0\n",
    "        else:\n",
    "            ZeroToOne = episodeCounter/100\n",
    "    \n",
    "        #implementing the epsilon-greedy algorithm here\n",
    "        #if(np.random.random(1)[0] < (1.0-self.epsilon)):\n",
    "        \n",
    "        if(np.random.random(1)[0] < (ZeroToOne-self.epsilon)):\n",
    "            \n",
    "            #print 'in GREEDY/exploitation mode'\n",
    "            \n",
    "            best_action_index = self.getMaxQ(state)\n",
    "         \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            best_action_index = self.env.action_space.sample()\n",
    "            #print 'in Exploration mode'\n",
    "            \n",
    "        return best_action_index\n",
    "        \n",
    "        \n",
    "    def updateQ(self, S_initial, S_dash, action, reward, maxQindex):\n",
    "\n",
    "        \n",
    "        Old_Q = self.getQvalues(S_initial, action)\n",
    "        \n",
    "        new_Q = Old_Q + self.alpha*(reward+self.gamma*(self.getQvalues(S_dash, maxQindex))-Old_Q)\n",
    "        \n",
    "        self.Q[S_initial][action] = new_Q\n",
    "        \n",
    "    \n",
    "    def printQ(self):\n",
    "        for s in self.Q:\n",
    "            print(s, self.Q[s])\n",
    "            \n",
    "\n",
    "\n",
    "  \n",
    "    def train(self, TRandom = False):\n",
    "        \n",
    "        episodeCounter = 0\n",
    "        \n",
    "        for k in range(self.numberOfEpisodes):\n",
    "            \n",
    "                          \n",
    "            #first we take want to quantize the observation outputs from the environment\n",
    "            S_initial = self.quantizeState(self.resetState()) #selecting a random initial state\n",
    "            \n",
    "            #set the Q values for this initial state (0,0)\n",
    "            self.getQvalues(S_initial)\n",
    "                       \n",
    "            #print \"nth episode\", episodeCounter\n",
    "            episodeCounter += 1\n",
    "            \n",
    "            \n",
    "            step = 0\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                #If TRandom == True:\n",
    "                    \n",
    "                    #take a random action\n",
    "                #action = self.randomAction()\n",
    "                #else:\n",
    "                    #take some action defined using epsilon-greedy\n",
    "                action = self.getAction(S_initial,episodeCounter)\n",
    "                    \n",
    "                \n",
    "                S_dash, reward, done = self.getEnvironmentVariables(action)\n",
    "                \n",
    "                #this next code insentivises the algorithm if is does not succesfully \"learn\"\n",
    "                #by severly penalizing a final \"step\" that causes termination of the cartpole pole\n",
    "                #simulation - due to the pole falling over.\n",
    "                if done and step < 200:\n",
    "                    reward = -100\n",
    "                \n",
    "                S_dash = self.quantizeState(S_dash)\n",
    "                \n",
    "                \n",
    "                #getting the max Q value index ( so, which action should we take....)\n",
    "                Qindex = self.getMaxQ(S_dash)\n",
    "                \n",
    "                self.updateQ(S_initial,S_dash, action, reward, Qindex)\n",
    "                \n",
    "                S_initial = S_dash\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                if done:\n",
    "                    #print \"completed training\"\n",
    "                    break;\n",
    "                    \n",
    "                    \n",
    "\n",
    "    def run(self):\n",
    "            \n",
    "        self.env.reset()\n",
    "        self.env.render()\n",
    "            \n",
    "        S_initial = self.quantizeState(self.resetState())\n",
    "            \n",
    "        episode_reward = 0\n",
    "        \n",
    "        counter = 0\n",
    "            \n",
    "        while True:\n",
    "            self.env.render()\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            #print \"running, count is :\", counter\n",
    "                \n",
    "            #getMaxQ\n",
    "            \n",
    "            MaxQIndex = self.getMaxQ(S_initial)\n",
    "                \n",
    "            S_dash, reward, done = self.getEnvironmentVariables(MaxQIndex)\n",
    "            S_dash = self.quantizeState(S_dash)\n",
    "\n",
    "                \n",
    "            S_initial = S_dash\n",
    "            episode_reward += reward\n",
    "            \n",
    "\n",
    "            if done:\n",
    "                self.env.close()\n",
    "                break;\n",
    "        \n",
    "        self.env.render(close=True)\n",
    "        print(\"Total Reward: \", episode_reward)              \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "#test = QTabular(numberOfEpisodes = 200)\n",
    "test = QTabular(numberOfEpisodes = 500)\n",
    "test.train()\n",
    "\n",
    "print \"completed training\"\n",
    "\n",
    "test.printQ()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total Reward: ', 200.0)\n"
     ]
    }
   ],
   "source": [
    "test.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### key learninings\n",
    "\n",
    "-  even with minor fluctations in variables such as learning rate, discount factor and epsilon the algorithm fails to find a stable solution\n",
    "- You have to hack the reward with a massive negative penalization to get the algorithm to work correctly\n",
    "- you have to implement episilon greedy, otherwise the algorithm fails to learn by never starting to exploit the best method it has found so far\n",
    "- in the sbove implementation, you have to enable the position and velocity variables in order to get better stabilization ( it can converge a solution, but it is not as elegant... run it to see it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# legacy code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the code below are the hundreds of mini experiments I ran to get the above to work, left below if any of the python notation above does not make sense to you...have a look below. left behind for my own future referance sakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ae389968884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbuckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbuckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.40282347e+38\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3.40282347e+38\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "buckets = []\n",
    "buckets = buckets.append(np.linspace(3.40282347e+38 , -3.40282347e+38, 12))\n",
    "\n",
    "print buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for k in range(4):\n",
    "    print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "QTable = np.zeros((5, 5, 2))\n",
    "print QTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((4,12,2))\n",
    "\n",
    "\n",
    "testQ = []\n",
    "\n",
    "testQ = testQ.append((12,2))\n",
    "\n",
    "print testQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-23 14:09:12,103] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02610283 -0.01238608  0.04940541  0.0400357 ]\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "print observation\n",
    "print type(observation)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "action = env.action_space.sample()\n",
    "\n",
    "n = env.action_space.n\n",
    "\n",
    "print action\n",
    "print n\n",
    "\n",
    "observationSpaceLength = env.observation_space.shape\n",
    "\n",
    "print observationSpaceLength[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buckets = []\n",
    "        \n",
    "        #this is bad, should write code to automatically detect thesem but, uh, I am lazy\n",
    "buckets.append(np.linspace(-4.8, 4.8, 12))\n",
    "buckets.append(np.linspace(-3.40282347e+38, 3.40282347e+38, 12))\n",
    "buckets.append(np.linspace(-4.18879020e-01, 4.18879020e-01 , 12))\n",
    "buckets.append(np.linspace(-3.40282347e+38 , 3.40282347e+38, 12))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rewriting this using Wills numbers, lol\n",
    "\n",
    "buckets = []\n",
    "        \n",
    "buckets.append(np.linspace(-2.4, 2.4, 1))\n",
    "buckets.append(np.linspace(-0.5, 0.5, 1))\n",
    "buckets.append(np.linspace(-41.8, 41.8, 5))\n",
    "buckets.append(np.linspace(-math.radians(50), math.radians(50), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.]\n",
      "[ 0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([])\n",
    "n = 2\n",
    "        \n",
    "for i in range(4):\n",
    "            \n",
    "    t = buckets[i].shape            \n",
    "    Q = np.append(Q, np.zeros((t[0],n)))\n",
    "    print Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "5\n",
      "3\n",
      "[array([[ 0.,  0.]]), array([[ 0.,  0.]]), array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]]), array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "Q = []\n",
    "n = 2\n",
    "\n",
    "#Q = np.zeros((4,12,2))\n",
    "\n",
    "for i in range(4):\n",
    "            \n",
    "    t = buckets[i].shape \n",
    "    print (t[0])\n",
    "    Qtemp =  np.zeros((t[0],n))\n",
    "    #Qtemp =  np.zeros((2,2))\n",
    "    Q.append(Qtemp)\n",
    "    \n",
    "    \n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "<type 'tuple'>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "\n",
    "    n = buckets[2].shape\n",
    "\n",
    "#print n\n",
    "\n",
    "#print type(n)\n",
    "#print n[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state + 1 is : [-0.01823837 -0.02303988  0.03254898 -0.01319891]\n",
      "\n",
      "\n",
      "quantized bucket # 1 is : [-2.4]\n",
      "\n",
      "\n",
      "quantized bucket # 2 is : [-0.5]\n",
      "\n",
      "\n",
      "quantized bucket # 3 is : [-41.8 -20.9   0.   20.9  41.8]\n",
      "\n",
      "\n",
      "quantized bucket # 4 is : [-0.87266463  0.          0.87266463]\n",
      "\n",
      "\n",
      "now printing allocated buckets...\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#[np.digitize(s_plus1[i], self.bins[i]) for i in range(4)]\n",
    "\n",
    "quantizedState_index = []\n",
    "\n",
    "print \"state + 1 is :\", state\n",
    "print \"\\n\"\n",
    "print \"quantized bucket # 1 is :\", buckets[0]\n",
    "print \"\\n\"\n",
    "print \"quantized bucket # 2 is :\", buckets[1]\n",
    "print \"\\n\"\n",
    "print \"quantized bucket # 3 is :\", buckets[2]\n",
    "print \"\\n\"\n",
    "print \"quantized bucket # 4 is :\", buckets[3]\n",
    "print \"\\n\"\n",
    "\n",
    "print \"now printing allocated buckets...\"\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    quantizedState_index.append(np.digitize(state[i], buckets[i]))\n",
    "    \n",
    "    print np.digitize(state[i], buckets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(1), array(1), array(3), array(1)]\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "now printing allocated buckets NUMBERS...\n",
      "1\n",
      "-2.4\n",
      "1\n",
      "-0.5\n",
      "3\n",
      "0.0\n",
      "1\n",
      "-0.872664625997\n"
     ]
    }
   ],
   "source": [
    "#quatnizedState = buckets[quantizedState_index[0],quantizedState_index[1],quantizedState_index[2],quantizedState_index[3]]\n",
    "\n",
    "print quantizedState_index\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    print quantizedState_index[i]\n",
    "    \n",
    "\n",
    "print \"now printing allocated buckets NUMBERS...\"\n",
    "\n",
    "quantizedState = []\n",
    "\n",
    "for i in range(4):\n",
    "    #print \"test\"\n",
    "    #print np.digitize(state[i], buckets[i])\n",
    "    \n",
    "    tempvar = quantizedState_index[i]\n",
    "    \n",
    "    #array3[0][1]\n",
    "    \n",
    "    print tempvar\n",
    "    \n",
    "    print buckets[i][tempvar-1]\n",
    "    \n",
    "    quantizedState.append(buckets[i][tempvar-1])\n",
    "    \n",
    "\n",
    "        \n",
    "        #fuck yeah working!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3999999999999999, -0.5, 0.0, -0.87266462599716477]\n",
      "-2.4\n",
      "-0.5\n",
      "0.0\n",
      "-0.872664625997\n"
     ]
    }
   ],
   "source": [
    "print quantizedState\n",
    "\n",
    "for i in range(4):\n",
    "    print quantizedState[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]]\n",
      "<type 'list'>\n",
      "<type 'numpy.ndarray'>\n",
      "[3, 4]\n",
      "11\n",
      "[2 3]\n",
      "1\n",
      "5\n",
      "5\n",
      "[[[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]\n",
      "\n",
      " [[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]\n",
      "\n",
      " [[ 0  1]\n",
      "  [ 2  3]\n",
      "  [ 4  5]\n",
      "  [ 6  7]\n",
      "  [ 8  9]\n",
      "  [10 11]]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b = [3,4]\n",
    "\n",
    "\n",
    "c = np.arange(12).reshape(6,2)\n",
    "d = np.array([c,c,c])\n",
    "\n",
    "Q = [a,b] \n",
    "\n",
    "k = 1\n",
    "\n",
    "maxQ = np.argmax(c)\n",
    "\n",
    "print c\n",
    "\n",
    "print type(a)\n",
    "print type(c)\n",
    "print Q[k]\n",
    "print maxQ\n",
    "\n",
    "\n",
    "print c[1,:]\n",
    "\n",
    "#how the hell does argmax work?\n",
    "print np.argmax(c[1,:])\n",
    "\n",
    "#max_value = max(my_list)\n",
    "\n",
    "print max(c[2,:])\n",
    "\n",
    "print c[2,1]\n",
    "\n",
    "print d\n",
    "\n",
    "#first one equals matrix (3rd) dimension\n",
    "#second one equals row dimension\n",
    "#third one equals column dimension\n",
    "#print d[0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-19f0a820442c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "Q = np.array([1,2,3])\n",
    "\n",
    "c = np.array([7,8,9])\n",
    "        \n",
    "\n",
    "Q.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#a = reshape(arange(9),(3,3))\n",
    "\n",
    "a = [[1,2],[3,4]]\n",
    "\n",
    "\n",
    "Q = np.zeros((5,2))\n",
    "\n",
    "print type(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets write this code the traditional way, then change it into a class\n",
    "\n",
    "#alpha = 0.1, gamma = 0, epsilon = 0.1,numberOfEpisodes = 200):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12)\n",
      "(2,)\n",
      "shape of buckets is: (4,)\n",
      "[[[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "#numpy array dimension experiment....\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "A = np.zeros((6,12))\n",
    "print np.shape(A)\n",
    "\n",
    "#B = np.zeros((1,2))\n",
    "B = np.zeros((2,))\n",
    "print np.shape(B)\n",
    "\n",
    "#C = A + B\n",
    "#print np.shape(C)\n",
    "\n",
    "\n",
    "\n",
    "buckets=(1, 1, 6, 12,)\n",
    "print \"shape of buckets is:\", np.shape(buckets)\n",
    "\n",
    "#print A\n",
    "\n",
    "Q = np.zeros((4,12,2))\n",
    "\n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-21 21:38:28,466] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Discrete.contains of Discrete(2)>\n",
      "(2,)\n",
      "(1, 1, 6, 12)\n",
      "2\n",
      "[[[[[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]\n",
      "\n",
      "   [[ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]\n",
      "    [ 0.  0.]]]]]\n"
     ]
    }
   ],
   "source": [
    "#perhaps instead of trying to reverse engineer it tomorrow, write it from scratch?\n",
    "\n",
    "#figuring out how the Q matrix is defined for n dimensional space for example below\n",
    "#  by playing around with the different values for 'buckets' and different permutations of 'Q'\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "buckets=(1, 1, 12, 12,) #original, optimal bucket sizes are (1, 1, 6, 12,) \n",
    "#buckets=( 6, 12, )\n",
    "\n",
    "#buckets= (1, 1,6, 12)\n",
    "#np.zeros((2, 1))\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# for a Q table:\n",
    "\n",
    "# ROWS = STATES\n",
    "# COLUMNS = ACTIONS\n",
    "\n",
    "Q = np.zeros(buckets + (env.action_space.n,)) #original\n",
    "\n",
    "#Q = np.zeros((2,)) #produces 1 x 2\n",
    "\n",
    "\n",
    "#Q = np.zeros(buckets) # produces a  6 x 12 matrix\n",
    "#Q = np.zeros((env.action_space.n,)) # produces a 1 x 2 matrix\n",
    "\n",
    "print env.action_space.contains\n",
    "print env.action_space.shape\n",
    "\n",
    "print buckets\n",
    "print env.action_space.n\n",
    "\n",
    "print Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-20 15:10:39,512] Making new env: CartPole-v0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-14d815ddb378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQCartPoleSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;31m# gym.upload('tmp/cartpole-1', api_key='')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14d815ddb378>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-14d815ddb378>\u001b[0m in \u001b[0;36mget_alpha\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mada_divisor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mada_divisor\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m    \u001b[0;31m# (math.log(boxCount if boxCount>0 else 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "# this is an old implementation of the tabular Q algorithm using matrices instead of dictionaries\n",
    "# proved to be too complex to get working\n",
    "\n",
    "# (not quite) working example off the internet (not my code)\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "class QCartPoleSolver():\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), n_episodes=1000, n_win_ticks=195, min_alpha=0.1, min_epsilon=0.1, gamma=1.0, ada_divisor=25, max_env_steps=None, quiet=False, monitor=False):\n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        self.n_episodes = n_episodes # training episodes \n",
    "        self.n_win_ticks = n_win_ticks # average ticks over 100 episodes required for win\n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.ada_divisor = ada_divisor # only for development purposes\n",
    "        self.quiet = quiet\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True) # record results for upload\n",
    "\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        new_obs = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        new_obs = [min(self.buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        return tuple(new_obs)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    def update_q(self, state_old, action, reward, state_new, alpha):\n",
    "        self.Q[state_old][action] += alpha * (reward + self.gamma * np.max(self.Q[state_new]) - self.Q[state_old][action])\n",
    "        \n",
    "        print self.Q\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - math.log10(((t + 1) if (t + 1)>0 else 1) / (self.ada_divisor if self.ada_divisor>0 else 1))))\n",
    "    \n",
    "   # (math.log(boxCount if boxCount>0 else 1))\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "\n",
    "            alpha = self.get_alpha(e)\n",
    "            epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            i = 0\n",
    "\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.choose_action(current_state, epsilon)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize(obs)\n",
    "                #print self.\n",
    "                self.update_q(current_state, action, reward, new_state, alpha)\n",
    "                current_state = new_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials '.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "        if not self.quiet: print('Did not solve after {} episodes '.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    solver = QCartPoleSolver()\n",
    "    solver.run()\n",
    "# gym.upload('tmp/cartpole-1', api_key='')\n",
    "\n",
    "#(math.log(boxCount if boxCount>0 else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01823837 -0.02303988  0.03254898 -0.01319891]\n",
      "<type 'numpy.ndarray'>\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#old code (not using any of this)\n",
    "\n",
    "state = observation\n",
    "\n",
    "print state\n",
    "\n",
    "quantizedState_index = [np.digitize(state[0], buckets[0]), np.digitize(state[1], buckets[1]), np.digitize(state[2], buckets[2]), np.digitize(state[3], buckets[3])]\n",
    "#quatnizedState = buckets[quantizedState_index[0],quantizedState_index[1],quantizedState_index[2],quantizedState_index[3]]\n",
    "\n",
    "#print quantizedState[0]\n",
    "print type(quantizedState_index[0])\n",
    "print quantizedState_index[0]\n",
    "print quantizedState_index[1]\n",
    "print quantizedState_index[2]\n",
    "print quantizedState_index[3]\n",
    "\n",
    "#quatnizedState = buckets[3,6,9,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1, 2, 3): [1, 2], (1, 1, 3, 4): [1, 2], (1, 1, 3, 5): [1, 2], (1, 1, 3, 3): [1, 2]}\n",
      "((1, 1, 2, 3), [1, 2])\n",
      "((1, 1, 3, 4), [1, 2])\n",
      "((1, 1, 3, 5), [1, 2])\n",
      "((1, 1, 3, 3), [1, 2])\n",
      "('actions are: ', [1, 2])\n",
      "('maxQ:', 2)\n",
      "('argmaxQ:', 1)\n",
      "{(1, 1, 2, 3): [1, 2], (1, 1, 3, 4): [1, 2], (1, 1, 3, 5): [1, 2], (1, 1, 3, 3): [1, 2], (2, 2, 2, 3): [2, 2]}\n",
      "('state:', (1, 1, 2, 3), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 4), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 5), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 3), 'actions:', [1, 2])\n",
      "('state:', (2, 2, 2, 3), 'actions:', [2, 2])\n",
      "('argmaxQ:', 0)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Q = {}\n",
    "\n",
    "Q[(1, 1, 3, 3)] = [1, 2]\n",
    "Q[(1, 1, 3, 4)] = [1, 2]\n",
    "Q[(1, 1, 3, 5)] = [1, 2]\n",
    "Q[(1, 1, 2, 3)] = [1, 2]\n",
    "print(Q)\n",
    "\n",
    "for s in Q:\n",
    "    print(s, Q[s])\n",
    "\n",
    "actions = Q[(1, 1, 3, 3)]  # retrieve both actions Q values from Q(s)\n",
    "print(\"actions are: \",actions)\n",
    "\n",
    "#maxQ\n",
    "print(\"maxQ:\", np.max(actions))\n",
    "\n",
    "#argmax i.e. action with highest value\n",
    "print(\"argmaxQ:\", np.argmax(actions))\n",
    "\n",
    "\n",
    "Q[(2, 2, 2, 3)] = [2, 2]\n",
    "print(Q)\n",
    "for i in Q:\n",
    "    print(\"state:\", i, \"actions:\", Q[i])\n",
    "    \n",
    "    \n",
    "# to take an action, action can only be 0 or 1, which essentially is the position\n",
    "#argmax i.e. action with highest value\n",
    "print(\"argmaxQ:\", np.argmax([15, 10]))\n",
    "\n",
    "#Q = Q(s) + alpha * (gamma * maxQ(s') - Q(s))\n",
    "\n",
    "print(np.max([25, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n",
      "{(1, 1, 2, 3): [1, 2], (1, 1, 3, 4): [1, 2], (1, 1, 3, 5): [1, 2], (1, 1, 3, 3): [1, 2]}\n",
      "((1, 1, 2, 3), [1, 2])\n",
      "((1, 1, 3, 4), [1, 2])\n",
      "((1, 1, 3, 5), [1, 2])\n",
      "((1, 1, 3, 3), [1, 2])\n",
      "('Qvalues are: ', [1, 2])\n",
      "('maxQ:', 2)\n",
      "('argmaxQ:', 1)\n",
      "{(1, 1, 2, 3): [1, 2], (1, 1, 3, 4): [1, 2], (1, 1, 3, 5): [1, 2], (1, 1, 3, 3): [1, 2], (2, 2, 2, 3): [2, 2]}\n",
      "('state:', (1, 1, 2, 3), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 4), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 5), 'actions:', [1, 2])\n",
      "('state:', (1, 1, 3, 3), 'actions:', [1, 2])\n",
      "('state:', (2, 2, 2, 3), 'actions:', [2, 2])\n",
      "('argmaxQ:', 0)\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Q = {}\n",
    "\n",
    "d =(1, 1, 3, 3)\n",
    "\n",
    "print type(d)\n",
    "\n",
    "Q[(1, 1, 3, 3)] = [1, 2]\n",
    "Q[(1, 1, 3, 4)] = [1, 2]\n",
    "Q[(1, 1, 3, 5)] = [1, 2]\n",
    "Q[(1, 1, 2, 3)] = [1, 2]\n",
    "print(Q)\n",
    "\n",
    "for s in Q:\n",
    "    print(s, Q[s])\n",
    "\n",
    "Qvalues = Q[(1, 1, 3, 3)]  # retrieve both actions Q values from Q(s)\n",
    "print(\"Qvalues are: \",Qvalues)\n",
    "\n",
    "#maxQ\n",
    "print(\"maxQ:\", np.max(Qvalues))\n",
    "\n",
    "#argmax i.e. action with highest value\n",
    "print(\"argmaxQ:\", np.argmax(Qvalues))\n",
    "\n",
    "\n",
    "Q[(2, 2, 2, 3)] = [2, 2]\n",
    "print(Q)\n",
    "for i in Q:\n",
    "    print(\"state:\", i, \"actions:\", Q[i])\n",
    "    \n",
    "    \n",
    "# to take an action, action can only be 0 or 1, which essentially is the position\n",
    "#argmax i.e. action with highest value\n",
    "print(\"argmaxQ:\", np.argmax([15, 10]))\n",
    "\n",
    "#Q = Q(s) + alpha * (gamma * maxQ(s') - Q(s))\n",
    "\n",
    "print(np.max([25, 20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "#testing referancing python dictionaries\n",
    "\n",
    "import numpy as np\n",
    "Q = {}\n",
    "\n",
    "Q[(1, 1, 3, 3)] = [1, 2]\n",
    "\n",
    "print Q[(1, 1, 3, 3)][0] # this notation correctly calls the right Q value\n",
    "print Q[(1, 1, 3, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# This original implementation used 2 dimensional matrices stored in an array to maintain the Q table\n",
    "# turns out this approach is extremely hard to get working correctly\n",
    "#\n",
    "# switched to using Will's idea of a using a python dictionary, as it is far superior!\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "class QTabular():\n",
    "    \n",
    "    #, buckets = (1,1,6,12,)\n",
    "    def __init__(self, alpha = 0.1, gamma = 0, epsilon = 0.1,numberOfEpisodes = 2):\n",
    "        \n",
    "        self.alpha = alpha # learning rate, 0.1 is the optimal paramters taken from Ferdinands Tabular Grid Search\n",
    "        self.gamma = gamma # discount factor, used to control the \"weight\" that is given to future reward\n",
    "        self.epsilon = epsilon # how much we want to explore...see multi armed bandit workbook\n",
    "        #self.buckets = buckets # downscaling the feature space from continous to these 'bucket' sizes\n",
    "                               # tuned parameters borrowed from Ferdinand \n",
    "        \n",
    "            \n",
    "            \n",
    "        self.numberOfEpisodes = numberOfEpisodes\n",
    "            \n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        self.S_initial = self.env.reset() # intitial observation from openAi gym\n",
    "        \n",
    "        \n",
    "        self.buckets = []\n",
    "        \n",
    "        #this is bad, should write code to automatically detect thesem but, uh, I am lazy\n",
    "        # parameter numbers  from Will's notebook referanced above\n",
    "        self.buckets.append(np.linspace(-2.4, 2.4, 1))\n",
    "        self.buckets.append(np.linspace(-0.5, 0.5, 1))\n",
    "        self.buckets.append(np.linspace(-41.8, 41.8, 5))\n",
    "        self.buckets.append(np.linspace(-math.radians(50), math.radians(50), 3))\n",
    "        \n",
    "        #\n",
    "        print self.buckets[0], self.buckets[1], self.buckets[2], self.buckets[3]\n",
    "        \n",
    "        #self.Q = np.zeros((4,12,2))\n",
    "        \n",
    "        #using Wills idea of a python dictionary for the Q table\n",
    "        #self.Q = {} #this method doesnt make sense either :(\n",
    "        \n",
    "        #measuring the dimensionality (length) of the observation space\n",
    "        \n",
    "        self.observationSpaceLength = env.observation_space.shape\n",
    "        self.obs_length = self.observationSpaceLength[0]\n",
    "        \n",
    "        #measuring the dimensionality of the action space\n",
    "        self.n = self.env.action_space.n\n",
    "        \n",
    "        print \"dimensionality of action space is is : \", self.n\n",
    "        print \"dimensionality of observation space is is : \", self.obs_length\n",
    "        \n",
    "        # for a Q table:\n",
    "\n",
    "        #  \n",
    "        \n",
    "        #np.array(test)   \n",
    "        \n",
    "        self.Q = [] \n",
    "        #np.array([])\n",
    "               \n",
    "    \n",
    "    \n",
    "    #this function allocated the raw observation data into its right \"bucket\" allocated above\n",
    "    def quantizeState(self, state):\n",
    "        \n",
    "        quantizedState_index = []\n",
    "        \n",
    "        print state[0], state[1], state[2], state[3]\n",
    "        \n",
    "        for i in range(4):\n",
    "    \n",
    "            #quantizedState_index.append((np.digitize(state[i], self.buckets[i])))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "            #print \"test\"\n",
    "            \n",
    "            x =  np.digitize(state[i], self.buckets[i])\n",
    "            \n",
    "            #digitize notation starts at 1 therefore we have to subtract 1 to get it inline\n",
    "            #with our array notation... (this took 4 hours to figure out)\n",
    "            x = x-1\n",
    "            \n",
    "            #print \"state[i] is:\", state[i]\n",
    "            #print \"buckets[i] Is:\", self.buckets[i]\n",
    "            #print \"bucket assignment is:\", x\n",
    "            \n",
    "            if x < 0:\n",
    "                x = 0\n",
    "            \n",
    "            quantizedState_index.append(x)\n",
    "        \n",
    "        #because the state matrix is one dimensional, digitize is fucking out and assigning 1 instead of the\n",
    "        #correct state index when we only have a 1x1 matrix (scalar)\n",
    "        #quantizedState_index[0] = 0\n",
    "        #quantizedState_index[1] = 0\n",
    "            \n",
    "        \n",
    "        #print quantizedState_index[0], quantizedState_index[1], quantizedState_index[2], quantizedState_index[3]\n",
    "        \n",
    "        #quantizedState = []\n",
    "        \n",
    "        #for i in range(4):\n",
    "\n",
    "\n",
    "            #tempvar = quantizedState_index[i]\n",
    "            \n",
    "            #quantizedState.append(buckets[i][tempvar-1])\n",
    "                          \n",
    "    \n",
    "        #returns a pointer that can be used to retrieve the specific \"bucket\" in questions value\n",
    "        return quantizedState_index\n",
    "        \n",
    "        \n",
    "    \n",
    "    def resetState(self):\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def getEnvironmentVariables(self,action):\n",
    "        \n",
    "        observation, reward, done, _ =  self.env.step(action)\n",
    "        \n",
    "        return(observation, reward, done)\n",
    "    \n",
    "    \n",
    "    def randomAction(self):\n",
    "                          \n",
    "        action = self.env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "                          \n",
    "    \n",
    "    def getAction(self, currentState, episodeCounter):\n",
    "    \n",
    "        #implementing the epsilon-greedy algorithm here\n",
    "        if(np.random.random(1)[0] < (1.0-epsilon)):\n",
    "            print \"yo mama\"\n",
    "            \n",
    "            #print 'in GREEDY/exploitation mode'\n",
    "            \n",
    "            #best_action_index = np.argmax(q_table)\n",
    " \n",
    "        \n",
    "            #print best_action_index\n",
    "        \n",
    "        \n",
    "            #action = best_action_index\n",
    "            \n",
    "            #def get_best_action(self, s):\n",
    "            #return np.argmax(self.Q[s[0], s[1], s[2], s[3]])\n",
    "            \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            action = self.env.action_space.sample()\n",
    "            #print 'in Exploration mode'\n",
    "            \n",
    "        return action\n",
    "                          \n",
    "    def initializeQ(self):\n",
    "        \n",
    "        for i in range(self.obs_length):\n",
    "            \n",
    "\n",
    "            t = self.buckets[i].shape            \n",
    "            self.Q.append(np.zeros((t[0],self.n)))\n",
    "                          \n",
    "    \n",
    "    def getMaxQ(self, S_dash):\n",
    "            \n",
    "        MaxQVector = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        print \"S_dash is:\", S_dash[0],S_dash[1],S_dash[2], S_dash[3]\n",
    "        \n",
    "                          \n",
    "        for k in range(self.obs_length):\n",
    "            \n",
    "           \n",
    "            #print \"S_dash is:\",S_dash\n",
    "            \n",
    "            #print \"the type of s_dash is:\",type(S_dash)\n",
    "            \n",
    "            \n",
    "            #print \"k is:\", k\n",
    "            #print \"the kth element of s dash is\", S_dash[k]\n",
    "            localQ = self.Q[k]\n",
    "            \n",
    "            print localQ\n",
    "            \n",
    "            tempVar = S_dash[k]\n",
    "            \n",
    "            #print \"S_dash for this bin is:\", tempVar\n",
    "            \n",
    "            MaxQ = max(localQ[tempVar,:])\n",
    "            \n",
    "            #print \"tempq is\", tempQ\n",
    "            \n",
    "            #MaxQ = max(localQ[(S_dash[k]),:])\n",
    "            \n",
    "            #print \"maxQ is: \", MaxQ\n",
    "            #this is all wrong....\n",
    "            #we want to get the max Q value (not the max S.....)\n",
    "            # the S just gives the index of which row to go to in the Q table\n",
    "            \n",
    "            #MaxQ =  #max(S_dash[k,:])\n",
    "            MaxQVector.append(MaxQ)\n",
    "            \n",
    "            #MaxQVector = 1\n",
    "            \n",
    "            #print MaxQVector\n",
    "                          \n",
    "        return MaxQVector\n",
    "                          \n",
    "            #maxQ = np.argmax(self.Q[k])\n",
    "            #max_value = max(my_list)\n",
    "            #max_index = my_list.index(max_value)\n",
    "        \n",
    "    def updateQ(self, S_initial, S_dash, action, reward, maxQVector):\n",
    "        print \"test\"\n",
    "        \n",
    "        #action is just an int.\n",
    "        \n",
    "        for k in range(self.obs_length):\n",
    "            \n",
    "            print \"k is:\", k\n",
    "            \n",
    "            localQ = self.Q[k]\n",
    "            \n",
    "            Old_q = localQ[(S_initial[k]),action]\n",
    "            \n",
    "            print type(k)\n",
    "            print type(S_dash[k])\n",
    "            print type(action)\n",
    "            print type(maxQVector[k])\n",
    "            \n",
    "            #self.Q[k,int(S_dash[k]),action] = Old_q + self.alpha*(reward+self.gamma*(int(maxQVector[k]))-Old_q)\n",
    "            self.Q[k][int(S_dash[k])][action] = Old_q + self.alpha*(reward+self.gamma*(int(maxQVector[k]))-Old_q)\n",
    "            \n",
    "            #first one equals matrix (3rd) dimension\n",
    "            #second one equals row dimension\n",
    "            #third one equals column dimension\n",
    "            #print d[0,1,0]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        for k in range(self.numberOfEpisodes):\n",
    "            \n",
    "            #initializing our Q matrix\n",
    "            self.initializeQ()\n",
    "                          \n",
    "            #first we take want to quantize the observation outputs from the environment\n",
    "            S_initial = self.quantizeState(self.resetState()) #selecting a random initial state\n",
    "            \n",
    "            #f = 1 \n",
    "            \n",
    "            #print self.buckets[0]\n",
    "            #print self.buckets[1]\n",
    "            #print self.buckets[2]\n",
    "            #print self.buckets[3]\n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                print \"nth episode\", counter\n",
    "                \n",
    "                #env.render()\n",
    "                \n",
    "                #S_initial_temp = S_initial\n",
    "                \n",
    "                #action = #take some action defined using epsilon greedy maybe?\n",
    "                \n",
    "                #take a random action\n",
    "                action = self.randomAction()\n",
    "                \n",
    "                print \"action is:\", action\n",
    "                               \n",
    "                #observation, reward, done\n",
    "                \n",
    "                print \"getting a new observation\"\n",
    "                \n",
    "                S_dash, reward, done = self.getEnvironmentVariables(action)\n",
    "                S_dash = self.quantizeState(S_dash)\n",
    "                \n",
    "                #print type(S_dash)\n",
    "                \n",
    "                \n",
    "                MaxQVector = self.getMaxQ(S_dash)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                print MaxQVector[0], MaxQVector[1], MaxQVector[2], MaxQVector[3]\n",
    "                \n",
    "                self.updateQ(S_initial,S_dash, action, reward, MaxQVector)\n",
    "                \n",
    "                S_initial = S_dash\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "                if done:\n",
    "                    print \"completed training\"\n",
    "                    break;\n",
    "                    \n",
    "                \n",
    "\n",
    "                \n",
    "                #break()\n",
    "        \n",
    "                \n",
    "                #print \"running....\"\n",
    "                          \n",
    "                #getAction(self, currentState, episodeCounter)\n",
    "                #action = getAction(S_initial)\n",
    "                \n",
    "                #implement tabular Q here\n",
    "                    \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                   \n",
    "                \n",
    "                    \n",
    "                \n",
    "                \n",
    "                #updating openAi gym with our new action determined from the epsilon-greedy algorithm above \n",
    "                \n",
    "                #S, reward, done, _ =  getState(action)  # _ = delete info, previously the 'info' was here\n",
    "                \n",
    "\n",
    "        \n",
    " \n",
    "    \n",
    "#tester = QTabular()        \n",
    "                          \n",
    "#QTabular.run()   \n",
    "\n",
    "test = QTabular()\n",
    "test.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
