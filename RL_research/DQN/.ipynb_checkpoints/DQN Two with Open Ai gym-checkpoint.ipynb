{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Two with Open Ai gym\n",
    "\n",
    "having another crack at this after finally understanding some of the latest frameworks out there (as of writing this notebook March 27th 2018, hint, its PyTorch)\n",
    "\n",
    "mostly inspired by this keras example:\n",
    "\n",
    "https://keon.io/deep-q-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "EPISODES = 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-27 23:43:05,378] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self): #constructor\n",
    "        super(LinearNet, self).__init__() #calling the super -> inheriting all the cool shit from the Net class\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def linear_forward(self, x):\n",
    "        print x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LinearNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "class DQAgent(nn.Module):\n",
    "    def __init__(self, state_space_size, action_space_size):\n",
    "    #def __init__(self, state_space_size, action_space_size, alpha = 0.1, gamma = 0.99, epsilon_start = 1.0, epsilon_end = 0.01, epsilon_decay = 0.01, memory_length = 2000, learning_rate = 0.001 ): #constructor\n",
    "        super(DQAgent, self).__init__() #calling the super -> inheriting all the cool shit from the Net class\n",
    "        \n",
    "        self.state_size = state_space_size\n",
    "        self.action_size = action_space_size\n",
    "        \n",
    "        self.memory = deque(maxlen=2000) # a dequue is fast python list...\n",
    "        \n",
    "        #this is self.alpha\n",
    "        self.learning_rate = 0.001 # learning rate, 0.1 is the optimal paramters taken from Ferdinands Tabular Grid Search\n",
    "        \n",
    "        self.gamma = 0.99 # discount factor, used to control the \"weight\" that is given to future reward\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.1 # how much we want to explore...see multi armed bandit workbook\n",
    "        \n",
    "        self.epsilon_decay = 0.995 # the rate at which epsilon decays with epsililon greedy...\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "    \n",
    "    #creating our replay memory (essentially x specified amount of images\n",
    "    #we feed into the classifier - to stop it from \"forgetting\")\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #a cleaner version of my act class...\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.linear_forward(state) #self.model.predict(state)\n",
    "        return np.argmax(act_values[0]) \n",
    "    \n",
    "    \n",
    "    #this is the heart of DQN\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "             # if done, make our target reward\n",
    "            target = reward\n",
    "            print target\n",
    "            \n",
    "            #we need to ensure that our Q network is not in a terminal state, otherwise it does not converge.\n",
    "            if not done:\n",
    "                \n",
    "                #implementing the bellman equation here wtf?\n",
    "                # predict the future discounted reward\n",
    "\n",
    "                target = reward + self.gamma*(np.amax(net.linear_forward(next_state)[0]))\n",
    "            \n",
    "            # what we are doing here with DQN is\n",
    "            \n",
    "            # if we received a positive reward, what we want to do is\n",
    "            # map our current state\n",
    "            # to the future action\n",
    "            # that produced the reward!\n",
    "            \n",
    "            #forward pass (evaluating the neural network as a function , uh, forward)\n",
    "            \n",
    "            #original code : target_f = self.model.predict(state)\n",
    "            \n",
    "            target_f = net.linear_forward(state)\n",
    "            \n",
    "            #not sure what this is doing\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            \n",
    "            # fit the model here\n",
    "            \n",
    "            \n",
    "            inputs = state\n",
    "            labels = target_f\n",
    "            \n",
    "            #wrapping our input & labels in variable objects so that they can have gradients...\n",
    "            \n",
    "            inputs, labels = Variable(inputs), Variable(labels) #add .cuda() here to load these onto the gpu eg inputs.cuda()\n",
    "            \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # backward + optimize\n",
    "            \n",
    "            #loss = criterion(outputs, labels)\n",
    "            \n",
    "            # wills loss\n",
    "            #loss = self.criterion(Q, y)\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = self.net(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-27 23:44:34,499] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: 14, e: 1.0\n",
      "episode: 1/1000, score: 24, e: 1.0\n",
      "1.0\n",
      "[[ 0.0511121   0.35563903 -0.17356947 -0.8625986 ]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-69f87b7e5586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-c8d24cc55c24>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# predict the future discounted reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# what we are doing here with DQN is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f63afab2665f>\u001b[0m in \u001b[0;36mlinear_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Paul/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Paul/anaconda2/lib/python2.7/site-packages/torch/nn/modules/linear.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Paul/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m     \"\"\"\n\u001b[0;32m--> 833\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQAgent(state_size, action_size)\n",
    "    # agent.load(\"./save/cartpole-dqn.h5\")\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolutional architecture to try after we get the linear one working...\n",
    "\n",
    "    def build_conv_model(self)\n",
    "        super(Net, self).__init__() #calling the super -> inheriting all the cool shit from the Net class\n",
    "        self.conv1 = nn.Conv2d(4, 128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(128, 16)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def conv_forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQAgent(nn.Module):\n",
    "    def __init__(self, state_space_size, action_space_size):\n",
    "    #def __init__(self, state_space_size, action_space_size, alpha = 0.1, gamma = 0.99, epsilon_start = 1.0, epsilon_end = 0.01, epsilon_decay = 0.01, memory_length = 2000, learning_rate = 0.001 ): #constructor\n",
    "        super(DQAgent, self).__init__() #calling the super -> inheriting all the cool shit from the Net class\n",
    "        \n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        \n",
    "        self.memory = deque(maxlen=2000) # a dequue is fast python list...\n",
    "        \n",
    "        \n",
    "        self.alpha = 0.1 # learning rate, 0.1 is the optimal paramters taken from Ferdinands Tabular Grid Search\n",
    "        self.gamma = 0.99 # discount factor, used to control the \"weight\" that is given to future reward\n",
    "        \n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.1 # how much we want to explore...see multi armed bandit workbook\n",
    "        \n",
    "        self.epsilon_decay = 0.01 # the rate at which epsilon decays with epsililon greedy...\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "        \n",
    "        #defining our model architecture\n",
    "        \n",
    "        #self.fc1 = nn.Linear(4, 128)\n",
    "        #self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "        \n",
    "        #self.net = Net()\n",
    "        #print(net)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   # def linear_forward(self, x):\n",
    "      #  x = F.relu(self.model.fc1(x))\n",
    "      #  x = F.relu(self.model.fc2(x))\n",
    "       # return x\n",
    "    \n",
    "    #creating our replay memory (essentially x specified amount of images\n",
    "    #we feed into the classifier - to stop it from \"forgetting\")\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    #a cleaner version of my act class...\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.linear_forward(state) #self.model.predict(state)\n",
    "        return np.argmax(act_values[0]) \n",
    "    \n",
    "    \n",
    "    #this is the heart of DQN\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "             # if done, make our target reward\n",
    "            target = reward\n",
    "            print target\n",
    "            \n",
    "            if not done:\n",
    "                \n",
    "                #implementing the bellman equation here wtf?\n",
    "                # predict the future discounted reward\n",
    "\n",
    "                target = reward + self.gamma*(np.amax(self.model.predict(next_state)[0]))\n",
    "            \n",
    "            # what we are doing here with DQN is\n",
    "            \n",
    "            # if we received a positive reward, what we want to do is\n",
    "            # map our current state\n",
    "            # to the future action\n",
    "            # that produced the reward!\n",
    "            \n",
    "            #forward pass (evaluating the neural network as a function , uh, forward)\n",
    "            \n",
    "            #original code : target_f = self.model.predict(state)\n",
    "            \n",
    "            target_f = self.linear_forward(state)\n",
    "            \n",
    "            #not sure what this is doing\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            \n",
    "            # fit the model here\n",
    "            \n",
    "            \n",
    "            inputs = state\n",
    "            labels = target_f\n",
    "            \n",
    "            #wrapping our input & labels in variable objects so that they can have gradients...\n",
    "            \n",
    "            inputs, labels = Variable(inputs), Variable(labels) #add .cuda() here to load these onto the gpu eg inputs.cuda()\n",
    "            \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            #loss = criterion(outputs, labels)\n",
    "            \n",
    "            # wills loss\n",
    "            #loss = self.criterion(Q, y)\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
