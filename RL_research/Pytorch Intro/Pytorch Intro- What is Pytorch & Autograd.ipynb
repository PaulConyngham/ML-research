{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Intro\n",
    "\n",
    "purpose of this notebook is to walk through the pytorch intro tutorials located here:\n",
    "\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111803cd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00  2.0000e+00  0.0000e+00\n",
      " 2.0000e+00  5.2241e+22  6.4462e-10\n",
      " 1.0682e-05  5.3930e-05  6.4535e-10\n",
      " 3.2798e-09  5.2944e+22  2.7450e-06\n",
      " 4.3247e-05  1.6821e-04  0.0000e+00\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct an uninitialized 5x3 matrix\n",
    "\n",
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0411  0.8415  0.1255\n",
      " 0.7737  0.8568  0.4041\n",
      " 0.4328  0.9454  0.2446\n",
      " 0.3803  0.3818  0.6631\n",
      " 0.2154  0.1732  0.5112\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a randomly initialized matrix\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "#tensor size\n",
    "\n",
    "print(x.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4349  1.0159  0.8791\n",
      " 0.9203  1.1791  0.5712\n",
      " 1.3673  1.0019  0.4322\n",
      " 1.3511  0.3992  1.0272\n",
      " 0.4649  0.6296  0.5653\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tensor addition syntax 1\n",
    "\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4349  1.0159  0.8791\n",
      " 0.9203  1.1791  0.5712\n",
      " 1.3673  1.0019  0.4322\n",
      " 1.3511  0.3992  1.0272\n",
      " 0.4649  0.6296  0.5653\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tensor addition syntax 2\n",
    "\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4349  1.0159  0.8791\n",
      " 0.9203  1.1791  0.5712\n",
      " 1.3673  1.0019  0.4322\n",
      " 1.3511  0.3992  1.0272\n",
      " 0.4649  0.6296  0.5653\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#saving the output of an addition to an another tensor...\n",
    "\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.4349  1.0159  0.8791\n",
      " 0.9203  1.1791  0.5712\n",
      " 1.3673  1.0019  0.4322\n",
      " 1.3511  0.3992  1.0272\n",
      " 0.4649  0.6296  0.5653\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"in place\" addition\n",
    "\n",
    "# not sure what this is...?\n",
    "\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.8415\n",
      " 0.8568\n",
      " 0.9454\n",
      " 0.3818\n",
      " 0.1732\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can use numpy indexing:\n",
    "\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# if you want to resize the tensor... you can use .view\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converting a tensor to a numpy array\n",
    "\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "# manually adding a scalar...\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all tensors on CPU support converting to Numpy & back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not available\n"
     ]
    }
   ],
   "source": [
    "# tensors can be moved onto the GPU using the .cuda method...\n",
    "\n",
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y\n",
    "else:\n",
    "    print(\"not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## computing gradients with the autograd.variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a variable\n",
    "\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do an operation...\n",
    "\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x1110f0950>\n"
     ]
    }
   ],
   "source": [
    "# y was created as the result of an operation, so apparently, this gives it a .grad function...\n",
    "\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do some more operations on our variable y\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is apparently enough to do backprop? haha\n",
    "\n",
    "# also apparently out.backward() is equivalent to doing out.backward(torch.Tensor([1.0])), whatever that means?\n",
    "\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print gradients d(out)/dx\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "#actually no way... it propagated the error back to x!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 586.2022\n",
      "-835.8894\n",
      " 623.2039\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apparently you can do many crazy things with autograd... ?\n",
    "\n",
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  102.4000\n",
      " 1024.0000\n",
      "    0.1024\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## stepping through Siraj's example here:\n",
    "\n",
    "based off the video: https://www.youtube.com/watch?v=nbJ-2G2GXL0\n",
    "\n",
    "code located here: https://github.com/llSourcell/pytorch_in_5_minutes/blob/master/demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26776602.0\n",
      "1 21948224.0\n",
      "2 21937342.0\n",
      "3 23310238.0\n",
      "4 23591656.0\n",
      "5 21099372.0\n",
      "6 16137400.0\n",
      "7 10619088.0\n",
      "8 6342823.0\n",
      "9 3672775.5\n",
      "10 2203835.5\n",
      "11 1426060.625\n",
      "12 1007904.375\n",
      "13 768706.875\n",
      "14 619983.1875\n",
      "15 518579.4375\n",
      "16 443748.4375\n",
      "17 385224.15625\n",
      "18 337521.34375\n",
      "19 297731.5\n",
      "20 263979.46875\n",
      "21 234988.203125\n",
      "22 209924.265625\n",
      "23 188118.453125\n",
      "24 169089.78125\n",
      "25 152380.65625\n",
      "26 137657.84375\n",
      "27 124625.132812\n",
      "28 113053.5625\n",
      "29 102750.523438\n",
      "30 93557.015625\n",
      "31 85327.9375\n",
      "32 77950.5625\n",
      "33 71321.8671875\n",
      "34 65356.4648438\n",
      "35 59977.5507812\n",
      "36 55116.34375\n",
      "37 50717.3554688\n",
      "38 46721.9765625\n",
      "39 43088.3789062\n",
      "40 39779.8046875\n",
      "41 36763.5703125\n",
      "42 34008.4179688\n",
      "43 31488.9882812\n",
      "44 29183.4472656\n",
      "45 27068.1621094\n",
      "46 25127.0820312\n",
      "47 23344.2011719\n",
      "48 21703.6523438\n",
      "49 20194.3847656\n",
      "50 18803.2460938\n",
      "51 17520.0898438\n",
      "52 16334.3710938\n",
      "53 15238.828125\n",
      "54 14225.7900391\n",
      "55 13287.9345703\n",
      "56 12418.875\n",
      "57 11613.4863281\n",
      "58 10866.3476562\n",
      "59 10172.5673828\n",
      "60 9527.66699219\n",
      "61 8928.09472656\n",
      "62 8370.25292969\n",
      "63 7850.99560547\n",
      "64 7367.22460938\n",
      "65 6917.48339844\n",
      "66 6497.92626953\n",
      "67 6106.45263672\n",
      "68 5740.98730469\n",
      "69 5399.31640625\n",
      "70 5079.97949219\n",
      "71 4781.40771484\n",
      "72 4502.14160156\n",
      "73 4240.45214844\n",
      "74 3995.4375\n",
      "75 3765.9909668\n",
      "76 3550.85986328\n",
      "77 3348.9543457\n",
      "78 3159.53881836\n",
      "79 2981.71826172\n",
      "80 2814.80541992\n",
      "81 2657.94580078\n",
      "82 2510.59228516\n",
      "83 2372.10522461\n",
      "84 2241.84863281\n",
      "85 2119.23217773\n",
      "86 2003.85742188\n",
      "87 1895.31213379\n",
      "88 1793.04748535\n",
      "89 1696.67614746\n",
      "90 1605.97460938\n",
      "91 1520.42114258\n",
      "92 1439.68017578\n",
      "93 1363.5456543\n",
      "94 1291.73278809\n",
      "95 1223.96313477\n",
      "96 1159.98779297\n",
      "97 1099.5657959\n",
      "98 1042.5012207\n",
      "99 988.618713379\n",
      "100 937.676391602\n",
      "101 889.516723633\n",
      "102 843.98059082\n",
      "103 800.934753418\n",
      "104 760.207214355\n",
      "105 721.688049316\n",
      "106 685.222961426\n",
      "107 650.715332031\n",
      "108 618.071166992\n",
      "109 587.125305176\n",
      "110 557.821472168\n",
      "111 530.06072998\n",
      "112 503.762512207\n",
      "113 478.867126465\n",
      "114 455.249694824\n",
      "115 432.85949707\n",
      "116 411.633331299\n",
      "117 391.49407959\n",
      "118 372.401855469\n",
      "119 354.283843994\n",
      "120 337.080291748\n",
      "121 320.758331299\n",
      "122 305.26361084\n",
      "123 290.552124023\n",
      "124 276.584777832\n",
      "125 263.311279297\n",
      "126 250.716659546\n",
      "127 238.743118286\n",
      "128 227.376876831\n",
      "129 216.566833496\n",
      "130 206.291503906\n",
      "131 196.525802612\n",
      "132 187.243759155\n",
      "133 178.420120239\n",
      "134 170.025161743\n",
      "135 162.041549683\n",
      "136 154.445922852\n",
      "137 147.225738525\n",
      "138 140.357849121\n",
      "139 133.816802979\n",
      "140 127.592552185\n",
      "141 121.669212341\n",
      "142 116.031272888\n",
      "143 110.66456604\n",
      "144 105.555854797\n",
      "145 100.690612793\n",
      "146 96.0609893799\n",
      "147 91.6481933594\n",
      "148 87.4448623657\n",
      "149 83.4446792603\n",
      "150 79.6295928955\n",
      "151 76.0015945435\n",
      "152 72.54246521\n",
      "153 69.244758606\n",
      "154 66.1052398682\n",
      "155 63.1124382019\n",
      "156 60.2575874329\n",
      "157 57.5387763977\n",
      "158 54.9485015869\n",
      "159 52.4757232666\n",
      "160 50.1225204468\n",
      "161 47.8746109009\n",
      "162 45.7327651978\n",
      "163 43.6892433167\n",
      "164 41.7398109436\n",
      "165 39.8788909912\n",
      "166 38.1058006287\n",
      "167 36.4104156494\n",
      "168 34.7945556641\n",
      "169 33.2525787354\n",
      "170 31.7802143097\n",
      "171 30.3756885529\n",
      "172 29.0348415375\n",
      "173 27.7538070679\n",
      "174 26.5313892365\n",
      "175 25.3638896942\n",
      "176 24.2487430573\n",
      "177 23.1844978333\n",
      "178 22.1684646606\n",
      "179 21.198135376\n",
      "180 20.2703819275\n",
      "181 19.3848133087\n",
      "182 18.5385837555\n",
      "183 17.7304172516\n",
      "184 16.9583263397\n",
      "185 16.2207355499\n",
      "186 15.5159339905\n",
      "187 14.8429031372\n",
      "188 14.1990308762\n",
      "189 13.5843219757\n",
      "190 12.9966278076\n",
      "191 12.4347887039\n",
      "192 11.8975067139\n",
      "193 11.3843317032\n",
      "194 10.8940372467\n",
      "195 10.4249095917\n",
      "196 9.97641181946\n",
      "197 9.54818820953\n",
      "198 9.13787746429\n",
      "199 8.74598121643\n",
      "200 8.37128829956\n",
      "201 8.0128660202\n",
      "202 7.67049646378\n",
      "203 7.34290742874\n",
      "204 7.02920198441\n",
      "205 6.72944879532\n",
      "206 6.44265317917\n",
      "207 6.16834354401\n",
      "208 5.90580844879\n",
      "209 5.65493345261\n",
      "210 5.41464567184\n",
      "211 5.1851849556\n",
      "212 4.9651517868\n",
      "213 4.75491094589\n",
      "214 4.55355548859\n",
      "215 4.36110258102\n",
      "216 4.17671442032\n",
      "217 4.00014305115\n",
      "218 3.83146452904\n",
      "219 3.66992282867\n",
      "220 3.51529455185\n",
      "221 3.36724925041\n",
      "222 3.22559714317\n",
      "223 3.08987665176\n",
      "224 2.96014332771\n",
      "225 2.83605694771\n",
      "226 2.71709060669\n",
      "227 2.60315704346\n",
      "228 2.4942650795\n",
      "229 2.38969492912\n",
      "230 2.2898106575\n",
      "231 2.19430708885\n",
      "232 2.10263538361\n",
      "233 2.01478290558\n",
      "234 1.93075978756\n",
      "235 1.8502753973\n",
      "236 1.77323782444\n",
      "237 1.69939172268\n",
      "238 1.6287368536\n",
      "239 1.56105601788\n",
      "240 1.49632322788\n",
      "241 1.43416833878\n",
      "242 1.37469649315\n",
      "243 1.31767463684\n",
      "244 1.2630084753\n",
      "245 1.21077716351\n",
      "246 1.16077780724\n",
      "247 1.11275184155\n",
      "248 1.06683206558\n",
      "249 1.02270925045\n",
      "250 0.980476498604\n",
      "251 0.940055608749\n",
      "252 0.901281833649\n",
      "253 0.864086866379\n",
      "254 0.828615128994\n",
      "255 0.794581830502\n",
      "256 0.761889755726\n",
      "257 0.730538010597\n",
      "258 0.700502753258\n",
      "259 0.67173653841\n",
      "260 0.644188225269\n",
      "261 0.617684721947\n",
      "262 0.59243106842\n",
      "263 0.568144917488\n",
      "264 0.544898152351\n",
      "265 0.522613704205\n",
      "266 0.501275539398\n",
      "267 0.480785429478\n",
      "268 0.461130321026\n",
      "269 0.442300051451\n",
      "270 0.424247920513\n",
      "271 0.406989485025\n",
      "272 0.390413880348\n",
      "273 0.374504774809\n",
      "274 0.35926297307\n",
      "275 0.344626516104\n",
      "276 0.330628782511\n",
      "277 0.317192077637\n",
      "278 0.304295122623\n",
      "279 0.291899949312\n",
      "280 0.280055612326\n",
      "281 0.268685728312\n",
      "282 0.257776647806\n",
      "283 0.247379854321\n",
      "284 0.237335264683\n",
      "285 0.227730423212\n",
      "286 0.218490496278\n",
      "287 0.209617182612\n",
      "288 0.201185643673\n",
      "289 0.19301071763\n",
      "290 0.185226947069\n",
      "291 0.17775310576\n",
      "292 0.17058108747\n",
      "293 0.163684815168\n",
      "294 0.157093629241\n",
      "295 0.150761425495\n",
      "296 0.144704714417\n",
      "297 0.138865187764\n",
      "298 0.13327896595\n",
      "299 0.127913177013\n",
      "300 0.122785098851\n",
      "301 0.117831803858\n",
      "302 0.11312687397\n",
      "303 0.108546994627\n",
      "304 0.104209527373\n",
      "305 0.100028574467\n",
      "306 0.0959922522306\n",
      "307 0.0921562314034\n",
      "308 0.0884771794081\n",
      "309 0.084944434464\n",
      "310 0.0815433710814\n",
      "311 0.0782893821597\n",
      "312 0.0751279294491\n",
      "313 0.0721455141902\n",
      "314 0.0692346394062\n",
      "315 0.0664945542812\n",
      "316 0.0638385340571\n",
      "317 0.0613001286983\n",
      "318 0.0588494352996\n",
      "319 0.0565214604139\n",
      "320 0.054249368608\n",
      "321 0.0521038137376\n",
      "322 0.0500326752663\n",
      "323 0.0480544641614\n",
      "324 0.0461337938905\n",
      "325 0.0442970581353\n",
      "326 0.0425395816565\n",
      "327 0.0408531352878\n",
      "328 0.0392275825143\n",
      "329 0.0376797169447\n",
      "330 0.0361856147647\n",
      "331 0.0347520150244\n",
      "332 0.033377289772\n",
      "333 0.0320595242083\n",
      "334 0.0308028608561\n",
      "335 0.0295845177025\n",
      "336 0.0284174457192\n",
      "337 0.0272933784872\n",
      "338 0.0262223519385\n",
      "339 0.0251903161407\n",
      "340 0.0241956505924\n",
      "341 0.0232487656176\n",
      "342 0.0223287772387\n",
      "343 0.0214616078883\n",
      "344 0.0206109564751\n",
      "345 0.0198119916022\n",
      "346 0.0190386082977\n",
      "347 0.0182950999588\n",
      "348 0.0175840929151\n",
      "349 0.0168930310756\n",
      "350 0.0162318646908\n",
      "351 0.0156047111377\n",
      "352 0.0149936266243\n",
      "353 0.0144056752324\n",
      "354 0.0138517720625\n",
      "355 0.0133250532672\n",
      "356 0.0128070320934\n",
      "357 0.0123125696555\n",
      "358 0.0118456948549\n",
      "359 0.0113943200558\n",
      "360 0.0109539404511\n",
      "361 0.0105329575017\n",
      "362 0.0101313544437\n",
      "363 0.00974340550601\n",
      "364 0.00936550181359\n",
      "365 0.00901712663472\n",
      "366 0.00867448467761\n",
      "367 0.00834927428514\n",
      "368 0.00803001318127\n",
      "369 0.00772604579106\n",
      "370 0.00743548758328\n",
      "371 0.00716131180525\n",
      "372 0.00689844461158\n",
      "373 0.00664265919477\n",
      "374 0.00639627128839\n",
      "375 0.00615325989202\n",
      "376 0.00592411169782\n",
      "377 0.0057133003138\n",
      "378 0.00549887912348\n",
      "379 0.00529698841274\n",
      "380 0.00510758720338\n",
      "381 0.00491913454607\n",
      "382 0.00474120303988\n",
      "383 0.00457552401349\n",
      "384 0.00441042566672\n",
      "385 0.00424716994166\n",
      "386 0.0040971506387\n",
      "387 0.00395135395229\n",
      "388 0.00381176103838\n",
      "389 0.00367612740956\n",
      "390 0.00354604236782\n",
      "391 0.00342178670689\n",
      "392 0.00330118346028\n",
      "393 0.00318667455576\n",
      "394 0.00307774404064\n",
      "395 0.00297237164341\n",
      "396 0.0028695918154\n",
      "397 0.00277348607779\n",
      "398 0.00267776683904\n",
      "399 0.00258784135804\n",
      "400 0.00249988003634\n",
      "401 0.00241455761716\n",
      "402 0.00233633909374\n",
      "403 0.00225909077562\n",
      "404 0.00218381546438\n",
      "405 0.00210902630351\n",
      "406 0.00204186979681\n",
      "407 0.00197688979097\n",
      "408 0.00191276392434\n",
      "409 0.00185272924136\n",
      "410 0.00179170304909\n",
      "411 0.00173488305882\n",
      "412 0.00168134178966\n",
      "413 0.00162929866929\n",
      "414 0.00157677219249\n",
      "415 0.00152709998656\n",
      "416 0.00147809146438\n",
      "417 0.00143299775664\n",
      "418 0.00138712744229\n",
      "419 0.00134485401213\n",
      "420 0.00130447593983\n",
      "421 0.00126591091976\n",
      "422 0.00122807326261\n",
      "423 0.00119087810162\n",
      "424 0.00115509156603\n",
      "425 0.00112303590868\n",
      "426 0.00108903110959\n",
      "427 0.00105760444421\n",
      "428 0.00102703808807\n",
      "429 0.000996525981463\n",
      "430 0.000967929547187\n",
      "431 0.000940515776165\n",
      "432 0.000913215975743\n",
      "433 0.000889321614522\n",
      "434 0.000865248963237\n",
      "435 0.000840869150124\n",
      "436 0.000817237305455\n",
      "437 0.000794796680566\n",
      "438 0.000774806539994\n",
      "439 0.000753204862121\n",
      "440 0.000734173867386\n",
      "441 0.000714685767889\n",
      "442 0.000695148482919\n",
      "443 0.000677136238664\n",
      "444 0.000658575736452\n",
      "445 0.000641306804027\n",
      "446 0.000626063672826\n",
      "447 0.000609787995927\n",
      "448 0.000594365468714\n",
      "449 0.000578425417189\n",
      "450 0.000565106864087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451 0.000550157856196\n",
      "452 0.000535813742317\n",
      "453 0.00052302289987\n",
      "454 0.000509206729475\n",
      "455 0.000496660650242\n",
      "456 0.000484028540086\n",
      "457 0.000472314655781\n",
      "458 0.000461605493911\n",
      "459 0.000451286061434\n",
      "460 0.000440312811406\n",
      "461 0.000428983563324\n",
      "462 0.000419259158662\n",
      "463 0.000410815322539\n",
      "464 0.000400721270125\n",
      "465 0.000390926899854\n",
      "466 0.000382439146051\n",
      "467 0.000373218354071\n",
      "468 0.000365353538655\n",
      "469 0.000357971322956\n",
      "470 0.000350604212144\n",
      "471 0.000341815670254\n",
      "472 0.000335044867825\n",
      "473 0.000328157533659\n",
      "474 0.000320628460031\n",
      "475 0.0003136768064\n",
      "476 0.000307299080305\n",
      "477 0.000299989798805\n",
      "478 0.00029420608189\n",
      "479 0.000288831419311\n",
      "480 0.000283049943391\n",
      "481 0.000277026556432\n",
      "482 0.000270528544206\n",
      "483 0.000265860027866\n",
      "484 0.0002605637128\n",
      "485 0.000255017483141\n",
      "486 0.00025089745759\n",
      "487 0.000245669594733\n",
      "488 0.000239930857788\n",
      "489 0.000235977640841\n",
      "490 0.000231697908021\n",
      "491 0.000226728356211\n",
      "492 0.000222518588998\n",
      "493 0.000218507106183\n",
      "494 0.000215204505366\n",
      "495 0.000210230529774\n",
      "496 0.000206320051802\n",
      "497 0.000202425319003\n",
      "498 0.000198982277652\n",
      "499 0.00019505986711\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/two_layer_net_autograd.py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Variables; these\n",
    "  # are exactly the same operations we used to compute the forward pass using\n",
    "  # Tensors, but we do not need to keep references to intermediate values since\n",
    "  # we are not implementing the backward pass by hand.\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss using operations on Variables.\n",
    "  # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "  # (1,); loss.data[0] is a scalar value holding the loss.\n",
    "\n",
    "  #pow 2 , means to the power of 2\n",
    "\n",
    "  # sum() , means sigma, or the sum of all squared error\n",
    "\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.data[0])\n",
    "  \n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "  # After this call w1.grad and w2.grad will be Variables holding the gradient\n",
    "  # of the loss with respect to w1 and w2 respectively.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent; w1.data and w2.data are Tensors,\n",
    "  # w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are\n",
    "  # Tensors.\n",
    "  w1.data -= learning_rate * w1.grad.data\n",
    "  w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "  # Manually zero the gradients \n",
    "  w1.grad.data.zero_()\n",
    "  w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-4.8052e-02 -4.0543e-02  4.4600e-01  ...  -1.7186e+00  1.9580e-01  2.6152e-01\n",
      "-7.3047e-01 -3.8490e-01  1.7806e+00  ...  -2.1728e+00  3.4729e-01 -1.2514e-01\n",
      "-4.9727e-01 -7.2018e-01 -1.4319e+00  ...   8.4825e-01 -8.2667e-02 -8.0257e-04\n",
      "                ...                   ⋱                   ...                \n",
      "-1.1457e+00  8.7169e-01 -1.9630e+00  ...   7.2256e-02 -2.4460e-01 -5.8085e-01\n",
      "-1.2080e+00 -8.3331e-01  8.0788e-01  ...  -2.3028e-01 -8.0669e-02  1.8830e+00\n",
      " 1.3251e-01 -8.6802e-02  3.2665e-01  ...   2.4901e+00  1.1503e+00  2.8159e-01\n",
      "[torch.FloatTensor of size 64x1000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploring Computation Graphs and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "<AddBackward1 object at 0x1153ce450>\n"
     ]
    }
   ],
   "source": [
    "# Variables wrap tensor objects\n",
    "x = autograd.Variable(torch.Tensor([1., 2., 3]), requires_grad=True)\n",
    "# You can access the data with the .data attribute\n",
    "print(x.data)\n",
    "\n",
    "# You can also do all the same operations you did with tensors with Variables.\n",
    "y = autograd.Variable(torch.Tensor([4., 5., 6]), requires_grad=True)\n",
    "z = x + y\n",
    "print(z.data)\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)\n",
    "\n",
    "#print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so, variables objects know what created them... but how does that help us create a gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<SumBackward0 object at 0x1153ba9d0>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, what is the derivative of this sum with respect to the first component of x? In math, we want\n",
    "\n",
    "∂s/∂x0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, s knows that it was created as a sum of the tensor z. z knows that it was the sum x + y. So\n",
    "\n",
    "s=x0+y0⏞z0+x1+y1⏞z1+x2+y2⏞z2\n",
    "\n",
    "And so s contains enough information to determine that the derivative we want is 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3\n",
      " 3\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      " 3\n",
      " 3\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(s.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dont know why it accumulates, but ok\n",
    "\n",
    "#Lets have Pytorch compute the gradient, and see that we were right:\n",
    "#(note if you run this block multiple times, the gradient will increment. \n",
    "#That is because Pytorch accumulates the gradient into the .grad property,\n",
    "#since for many models this is very convenient.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what is going here is apparently extremely important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward1 object at 0x1153bae50>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 2))\n",
    "y = torch.randn((2, 2))\n",
    "z = x + y  # These are Tensor types, and backprop would not be possible\n",
    "\n",
    "var_x = autograd.Variable(x, requires_grad=True)\n",
    "var_y = autograd.Variable(y, requires_grad=True)\n",
    "# var_z contains enough information to compute gradients, as we saw above\n",
    "var_z = var_x + var_y\n",
    "print(var_z.grad_fn)\n",
    "\n",
    "var_z_data = var_z.data  # Get the wrapped Tensor object out of var_z...\n",
    "# Re-wrap the tensor in a new variable\n",
    "new_var_z = autograd.Variable(var_z_data)\n",
    "\n",
    "# ... does new_var_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print(new_var_z.grad_fn)\n",
    "# And how could it?  We yanked the tensor out of var_z (that is\n",
    "# what var_z.data is).  This tensor doesn't know anything about\n",
    "# how it was computed.  We pass it into new_var_z, and this is all the\n",
    "# information new_var_z gets.  If var_z_data doesn't know how it was\n",
    "# computed, theres no way new_var_z will.\n",
    "# In essence, we have broken the variable away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want the error from your loss function to backpropagate to a component of your network, you MUST NOT break the Variable chain from that component to your loss Variable. If you do, the loss will have no idea your component exists, and its parameters can’t be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
